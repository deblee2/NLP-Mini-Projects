{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "1669063 이송_영문 자연언어처리 POS 태깅-기계학습 기반.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5ZRVgJ64FXm"
      },
      "source": [
        "## ★ 예비프로젝트 ★ [선택 #3] 영문 POS 태깅 코딩 실습 (기계학습기반) 튜토리얼 형식의 보고서\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SoUisrCn4rIr"
      },
      "source": [
        "**1669063 이송**\n",
        "\n",
        "- 제시된 블로그 글(Machine Learning 알고리즘을 활용한 POS Tagging :https://nlpforhackers.io/training-pos-tagger/)을 참고하여 튜토리얼 형식의 보고서를 작성했습니다. \n",
        "- 기본 틀은 블로그 글을 따라가되, 필요하다고 생각하는 내용을 추가하여 작성하였습니다.\n",
        "- 추가된 부분으로는 코드의 결과값에 대한 설명, 코드 내의 주석으로 단 설명, 태거를 더 쉽게 이해하기 위한 POS 태그 리스트가 있습니다.\n",
        "- POS 태그를 설명하기 위하여 참고한 블로그는 https://excelsior-cjh.tistory.com/m/71 입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nl1_BXoqYz8o"
      },
      "source": [
        "# 나만의 Part-Of-Speech(POS, 품사) 태깅 훈련을 위한 가이드\n",
        "\n",
        "POS 태깅은 어느 NLP 분석에서든 주요 구성 요소 중 하나이다. POS 태깅의 작업은 단어에 적절한 품사를 레이블링 하는 것을 의미한다.\n",
        "\n",
        "# Penn Treebank Tags\n",
        "영어 POS 태깅에서 가장 인기있는 태그 세트는 Penn Treebank Tags이다. 대부분의 이미 훈련된 영어 태거들은 Penn Treebank Tags로 훈련되었다. 예로는 NLTK default tagger와 Stanford CoreNLP tagger가 있다.\n",
        "\n",
        "# POS 태깅이 뭘까?\n",
        "아래는 간단한 예시이다. nltk를 사용하여 토크나이즈와 POS 태깅을 수행하는 과정을 보자."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8VaYeII3aGF4"
      },
      "source": [
        "import nltk\n",
        "#nltk.download()"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "INRhRxbcapTx",
        "outputId": "14760305-0c10-48c6-a4d8-dc06f6ddcf62"
      },
      "source": [
        "#nltk의 word_tokenize와 pos_tag 클래스를 위하여 다운로드함\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tKwGaF0SYz8p",
        "outputId": "8324c2fc-eda0-4da8-a9a1-d420bddac997"
      },
      "source": [
        "#nltk에서 word_tokenize와 pos_tag 모듈 불러옴\n",
        "from nltk import word_tokenize, pos_tag\n",
        " \n",
        "#토큰화와 POS 태깅을 한다\n",
        "print(pos_tag(word_tokenize(\"I'm learning NLP\")))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('I', 'PRP'), (\"'m\", 'VBP'), ('learning', 'VBG'), ('NLP', 'NNP')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0AJRkCblYFb"
      },
      "source": [
        "# 결과값 출력 및 설명\n",
        "```\n",
        "[('I', 'PRP'), (\"'m\", 'VBP'), ('learning', 'VBG'), ('NLP', 'NNP')]`\n",
        "```\n",
        "- 'I'는 인칭 대명사(personal pronoun)\n",
        "-\"'m\"은 비3인칭 단수 현재 동사(verb, non-3rd person singular present)\n",
        "-'learning'은 동명사 혹은 현재 분사(verb, gerund or present participle)\n",
        "-'NLP'는 단수 고유 명사(proper noun, singular)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAmRS5IWpFL3"
      },
      "source": [
        "# (자체 추가한 부분) POS 태그 리스트 \n",
        "아래의 리스트를 참고하여 태그를 이해해보자. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMpJE0-dois7"
      },
      "source": [
        "![이미지](https://image.slidesharecdn.com/largescalenlpusingpythonsnltkonazure-160507093517/95/large-scale-nlp-using-pythons-nltk-on-azure-18-638.jpg?cb=1462613961)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rt143NcpuveP"
      },
      "source": [
        "\n",
        "이미지 출처: \n",
        "https://www.google.com/url?sa=i&url=https%3A%2F%2Fwww.slideshare.net%2Fcloudbeatsch%2Flarge-scale-nlp-using-pythons-nltk-on-azure&psig=AOvVaw1xyHc5MMk6QjMn9kGiRslX&ust=1623679817003000&source=images&cd=vfe&ved=0CAIQjRxqFwoTCMiP3enklPECFQAAAAAdAAAAABAK"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_uQdic75Yz8q"
      },
      "source": [
        "#NLTK에서 POS 태깅 도구\n",
        "NLTK에서 자신만의 POS 태거를 구축할 수 있는 간단한 도구들이 있다. NLT 문서 Chapter 5 , section 4: “Automatic Tagging”를 참고하라.\n",
        "다음과 같은 간단한 태거를 만들 수 있다.\n",
        "\n",
        "- DefaultTagger: 모든 것을 동일한 태그로 태그한다.\n",
        "- RegexpTagger: 정규식 집합에 따라 태그를 적용한다.\n",
        "- UnigramTagger: 알려진 단어에 대해 가장 빈번한 태그를 선택한다.\n",
        "- BigramTagger, TriagramTagger: UnigramTagger와 유사하게 작동하지만 일부 컨텍스트를 고려한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZOP285VYz8q"
      },
      "source": [
        "#POS 태거 훈련을 위하여 말뭉치 선택\n",
        "POS 태거를 구축하기 위한 리소스는 매우 부족한데, 이유는 단순히 대량의 텍스트에 주석을 다는 것은 매우 지루한 작업이기 때문이다. 우리가 사용할 수 있고 우리가 선호하는 태스 세트를 사용하는 하나의 리소스는 NLTK 안에 있다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TKxQ9ZKDciBj",
        "outputId": "1c4e1e34-4a9f-4825-ebf5-c802d51e3176"
      },
      "source": [
        "#말뭉치를 사용하기 위해서 다운로드\n",
        "nltk.download('treebank')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/treebank.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VSpkGda-Yz8r",
        "outputId": "a8079aab-f4e9-4d89-ed07-8d27275989b2"
      },
      "source": [
        "#import nltk\n",
        "\n",
        "# nltk에서 토큰화와 품사 태깅이 된 말뭉치를 받아옴\n",
        "tagged_sentences = nltk.corpus.treebank.tagged_sents()\n",
        "\n",
        "# 말뭉치에서 첫 번째 문장의 각 단어와 그 단어의 태그들을 출력함\n",
        "print(tagged_sentences[0])\n",
        "# 말뭉치 문장의 개수\n",
        "print(\"Tagged sentences: \", len(tagged_sentences))\n",
        "# 말뭉치 단어의 개수\n",
        "print(\"Tagged words:\", len(nltk.corpus.treebank.tagged_words()))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ('61', 'CD'), ('years', 'NNS'), ('old', 'JJ'), (',', ','), ('will', 'MD'), ('join', 'VB'), ('the', 'DT'), ('board', 'NN'), ('as', 'IN'), ('a', 'DT'), ('nonexecutive', 'JJ'), ('director', 'NN'), ('Nov.', 'NNP'), ('29', 'CD'), ('.', '.')]\n",
            "Tagged sentences:  3914\n",
            "Tagged words: 100676\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZWbqPqsmQjj"
      },
      "source": [
        "# 결과값 설명\n",
        "\n",
        "\n",
        "```\n",
        "[('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ('61', 'CD'), ('years', 'NNS'), ('old', 'JJ'), (',', ','), ('will', 'MD'), ('join', 'VB'), ('the', 'DT'), ('board', 'NN'), ('as', 'IN'), ('a', 'DT'), ('nonexecutive', 'JJ'), ('director', 'NN'), ('Nov.', 'NNP'), ('29', 'CD'), ('.', '.')]\n",
        "```\n",
        "- 'Pierre'는 단수 고유 명사\n",
        "- 'Vinken'은 단수 고유 명사\n",
        "- '61'은 기수\n",
        "- 'years'는 복수 명사\n",
        "- 'old'는 형용사\n",
        "- 'will'은 법조동사\n",
        "- 'join'은 기본형 동사\n",
        "- 'the'는 한정사\n",
        "- 'board'는 단수 혹은 불가산 명사\n",
        "- 'as'는 전치사나 종속 접속사\n",
        "- 'a'는 한정사\n",
        "- 'nonexecutive'는 형용사\n",
        "- 'director'는 단수 혹은 불가산 명사\n",
        "- 'Nov.'는 고유명사\n",
        "- '29'는 기수\n",
        "\n",
        "\n",
        "```\n",
        "Tagged sentences:  3914\n",
        "```\n",
        "- 말뭉치 안 문장 개수\n",
        "\n",
        "\n",
        "```\n",
        "Tagged words: 100676\n",
        "```\n",
        "- 말뭉치 안 단어 개수"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxpmrMMjYz8r"
      },
      "source": [
        "#scikit-learn을 사용하여 자체 POS 태거를 훈련하기\n",
        "분류기 훈련을 시작하기 전에, 어떤 피처를 사용할 것인지 먼저 결정해야 한다. 가장 명백한 선택지는 단어 자체와, 이전 단어, 그리고 이후 단어이다. 더 발전시켜서 생각해보면, '-ed'로 끝나는 과거형 동사의 경우, 두 글자 접미사를 사용할 수 있다. '-ing'으로 끝나는 현재 분사를 인식할 때는 세 글자 접미사를 사용할 수 있다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JxfQPPSTYz8s",
        "outputId": "0636ec33-b967-4147-9daf-87eb27b4b74e"
      },
      "source": [
        "# 피처들을 함수로 표현함. 함수에 문장과 인덱스를 입력하면 피처와 피처의 True/False 여부를 출력해줌.\n",
        "def features(sentence, index):\n",
        "    \"\"\" sentence: [w1, w2, ...], index: the index of the word \"\"\"\n",
        "    return {\n",
        "        #출력하는 단어. 입력한 sentence의 index에 위치하는 단어임.\n",
        "        'word': sentence[index],\n",
        "        #맨 첫 단어와 마지막 단어\n",
        "        'is_first': index == 0, #첫 단어\n",
        "        'is_last': index == len(sentence) - 1, # 마지막 단어를 출력함. 파이썬은 0부터 시작하기 때문에 마지막 글자를 뽑으려면 -1을 해야 함.\n",
        "        # 대문자, 소문자\n",
        "        'is_capitalized': sentence[index][0].upper() == sentence[index][0], #.upper()를 사용하여 본 단어의 첫 글자를 대문자화된 상태와 본 단어를 비교하였을 때 같으면 대문자라고 함.\n",
        "        'is_all_caps': sentence[index].upper() == sentence[index], #단어 전체가 대문자화되었는지 확인함. 위의 피처와 비교하여 [0]이 없음을 알 수 있음.\n",
        "        'is_all_lower': sentence[index].lower() == sentence[index], #.lower()를 사용하여 본 단어가 소문자화된 상태와 본 단어를 비교하였을 때 같으면 대문자라고 함.\n",
        "        # 접두사\n",
        "        'prefix-1': sentence[index][0], #접두사 첫 글자만 출력함\n",
        "        'prefix-2': sentence[index][:2], #접두사 두 자리를 출력함\n",
        "        'prefix-3': sentence[index][:3], #접두사 세 자리를 출력함\n",
        "        # 접미사\n",
        "        'suffix-1': sentence[index][-1], # 접미사 한 자리를 출력함\n",
        "        'suffix-2': sentence[index][-2:], #접미사 두 자리를 출력함(끝에서 두 자리)\n",
        "        'suffix-3': sentence[index][-3:], #접미사 세 자리를 출력함\n",
        "        # 이전 단어와 이후 단어\n",
        "        'prev_word': '' if index == 0 else sentence[index - 1], #이전 단어: 만약 단어가 첫 번째 단어라면 이전 단어는 비어있고, 그렇지 않다면 단어의 그 전 인덱스에 위치하는 단어를 출력함\n",
        "        'next_word': '' if index == len(sentence) - 1 else sentence[index + 1], #이후 단어: 만약 단어가 마지막 단어라면 이후 단어는 비어있고, 그렇지 않다면 단어의 그 다음 인덱스에 위치하는 단어를 출력함 \n",
        "        #하이픈(-) 여부 확인\n",
        "        'has_hyphen': '-' in sentence[index], #-가 단어에 있는지 확인함\n",
        "        # 숫자 여부 확인\n",
        "        'is_numeric': sentence[index].isdigit(), #.isdigit()으로 숫자인지 확인함\n",
        "        # 단어의 첫 글자 제외하고 내부에 대문자 있는지 확인\n",
        "        'capitals_inside': sentence[index][1:].lower() != sentence[index][1:] #첫 단어를 제외하고(그러므로 [0:]가 아닌 [1:]이라고 표현함) 문장 안이 모두 소문자(.lower)가 아니라면(!=) capitals_inside임\n",
        "    }\n",
        "\n",
        "# pprint 모듈: 임의의 파이썬 데이터 구조를 인터프리터의 입력으로 사용할 수 있는 형태로 '예쁘게 인쇄'할 수 있는 기능을 제공 (출처: https://docs.python.org/ko/3/library/pprint.html)\n",
        "import pprint \n",
        "pprint.pprint(features(['This', 'is', 'a', 'sentence'], 2))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'capitals_inside': False,\n",
            " 'has_hyphen': False,\n",
            " 'is_all_caps': False,\n",
            " 'is_all_lower': True,\n",
            " 'is_capitalized': False,\n",
            " 'is_first': False,\n",
            " 'is_last': False,\n",
            " 'is_numeric': False,\n",
            " 'next_word': 'sentence',\n",
            " 'prefix-1': 'a',\n",
            " 'prefix-2': 'a',\n",
            " 'prefix-3': 'a',\n",
            " 'prev_word': 'is',\n",
            " 'suffix-1': 'a',\n",
            " 'suffix-2': 'a',\n",
            " 'suffix-3': 'a',\n",
            " 'word': 'a'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QhzUrdtotWG"
      },
      "source": [
        "# 결과값 설명: \n",
        "- ['This', 'is', 'a', 'sentence']에서 인덱스 2인 'a'에 대한 결과값이다.\n",
        "\n",
        "```\n",
        "'capitals_inside': False, \n",
        "- 단어의 첫 글자를 제외하고 내부에 대문자가 없음\n",
        "\n",
        "'has_hyphen': False, \n",
        "- 단어에 '-'이 없음\n",
        "\n",
        "'is_all_caps': False, \n",
        "- 모두 대문자가 아님\n",
        "\n",
        "'is_all_lower': True, \n",
        "- 모두 소문자가 맞음\n",
        "\n",
        "'is_capitalized': False, \n",
        "- 단어의 첫 글자들이 대문자가 아님\n",
        "\n",
        "'is_first': False, \n",
        "- sentence에서 첫 단어가 아님\n",
        "\n",
        "'is_last': False, \n",
        "- sentence에서 마지막 단어가 아님\n",
        "\n",
        "'is_numeric': False, \n",
        "- 숫자가 아님\n",
        "\n",
        "'next_word': 'sentence', \n",
        "- 'a' 다음 인덱스(3)에 위치한 단어는 'sentence'임\n",
        "\n",
        "'prefix-1': 'a', \n",
        "- 접두사(첫 글자)는 'a'임\n",
        "\n",
        "'prefix-2': 'a', \n",
        "- 접두사(앞에서 두 글자)까지 출력해도 'a' 밖에 없으므로 'a'만 출력함\n",
        "\n",
        "'prefix-3': 'a', \n",
        "- 접두사(앞에서 세 글자)까지 출력해도 'a' 밖에 없으므로 'a'만 출력함\n",
        "\n",
        "'prev_word': 'is', \n",
        "- 'a' 전 인덱스(1)에 위치한 단어는 'is'임\n",
        "\n",
        "'suffix-1': 'a', \n",
        "- 접미사(맨 끝 글자)는 'a'임\n",
        "\n",
        "'suffix-2': 'a', \n",
        "- 접미사(맨 끝에서 두 글자)까지 출력해도 'a' 밖에 없으므로 'a'만 출력함\n",
        "\n",
        "'suffix-3': 'a', \n",
        "- 접미사(맨 끝에서 세 글자)까지 출력해도 'a' 밖에 없으므로 'a'만 출력함\n",
        "\n",
        "'word': 'a'} \n",
        "- 입력된 문장에서 인덱스 2에 위치하는 단어임\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ney50GOYz8t"
      },
      "source": [
        "태그된 말뭉치에서 태그를 제거하고 분류기에 공급하는 작은 도우미 함수를 작성한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLPSWKVtYz8u"
      },
      "source": [
        "# 태그 제거하는 함수. 태그된 문장을 입력값으로 넣음.\n",
        "def untag(tagged_sentence):\n",
        "    return [w for w, t in tagged_sentence]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Erry2JFGYz8u"
      },
      "source": [
        "훈련 데이터셋 만들기: 분류기는 한 단어의 피처를 허용하는데, 말뭉치는 문장들로 구성되어 있어 변환을 수행한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0lqlJ3_AYz8u",
        "outputId": "0cedaf0f-1133-4b18-a51c-630c53dcf96c"
      },
      "source": [
        "# 훈련과 테스팅을 위해 데이터셋을 나눔\n",
        "\n",
        "# 말뭉치 안 문장 개수* 0.75를 구함\n",
        "cutoff = int(.75 * len(tagged_sentences))\n",
        "# 훈련용 문장들을 전체 코퍼스의 75%로 구성함\n",
        "training_sentences = tagged_sentences[:cutoff]\n",
        "# 테스트용 문장들을 나머지 25%로 구성함\n",
        "test_sentences = tagged_sentences[cutoff:]\n",
        "\n",
        "# 훈련용 문장들과 테스트용 문장들의 길이를 보면 75%, 25% 나누어진 걸 알 수 있음\n",
        "print(len(training_sentences))   # 2935\n",
        "print(len(test_sentences))         # 979\n",
        " \n",
        "# 말뭉치를 넣으면 데이터셋으로 변환되는 함수\n",
        "def transform_to_dataset(tagged_sentences):\n",
        "    X, y = [], []\n",
        " \n",
        "    for tagged in tagged_sentences: #말뭉치 안 문장 하나씩\n",
        "        for index in range(len(tagged)): #문장 안 단어의 인덱스 하나씩\n",
        "            X.append(features(untag(tagged), index)) #단어에서 기존의 태그를 떼고, 인덱스와 함께 features 함수에 넣어 피처들을 추출한 후, X 리스트에 추가\n",
        "            y.append(tagged[index][1]) #단어의 POS 태그를 y 리스트에 추가\n",
        " \n",
        "    return X, y\n",
        " \n",
        "X, y = transform_to_dataset(training_sentences) #training_sentences 말뭉치를 넣어 단어 별로 피처들과 POS 태그를 출력함"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2935\n",
            "979\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vqORIBFHulbP",
        "outputId": "062829c5-92f4-4c2b-83b7-aa0346dd474d"
      },
      "source": [
        "# 각 단어별로 추출한 피처들이 출력됐는지 확인하기 위하여 수행함. \n",
        "# 말뭉치의 첫번째 문장 순서대로 'Pierre', 'Vinken', ',' , '61', 'years' 각각의 피처들이 출력됨\n",
        "X[:5]"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'capitals_inside': False,\n",
              "  'has_hyphen': False,\n",
              "  'is_all_caps': False,\n",
              "  'is_all_lower': False,\n",
              "  'is_capitalized': True,\n",
              "  'is_first': True,\n",
              "  'is_last': False,\n",
              "  'is_numeric': False,\n",
              "  'next_word': 'Vinken',\n",
              "  'prefix-1': 'P',\n",
              "  'prefix-2': 'Pi',\n",
              "  'prefix-3': 'Pie',\n",
              "  'prev_word': '',\n",
              "  'suffix-1': 'e',\n",
              "  'suffix-2': 're',\n",
              "  'suffix-3': 'rre',\n",
              "  'word': 'Pierre'},\n",
              " {'capitals_inside': False,\n",
              "  'has_hyphen': False,\n",
              "  'is_all_caps': False,\n",
              "  'is_all_lower': False,\n",
              "  'is_capitalized': True,\n",
              "  'is_first': False,\n",
              "  'is_last': False,\n",
              "  'is_numeric': False,\n",
              "  'next_word': ',',\n",
              "  'prefix-1': 'V',\n",
              "  'prefix-2': 'Vi',\n",
              "  'prefix-3': 'Vin',\n",
              "  'prev_word': 'Pierre',\n",
              "  'suffix-1': 'n',\n",
              "  'suffix-2': 'en',\n",
              "  'suffix-3': 'ken',\n",
              "  'word': 'Vinken'},\n",
              " {'capitals_inside': False,\n",
              "  'has_hyphen': False,\n",
              "  'is_all_caps': True,\n",
              "  'is_all_lower': True,\n",
              "  'is_capitalized': True,\n",
              "  'is_first': False,\n",
              "  'is_last': False,\n",
              "  'is_numeric': False,\n",
              "  'next_word': '61',\n",
              "  'prefix-1': ',',\n",
              "  'prefix-2': ',',\n",
              "  'prefix-3': ',',\n",
              "  'prev_word': 'Vinken',\n",
              "  'suffix-1': ',',\n",
              "  'suffix-2': ',',\n",
              "  'suffix-3': ',',\n",
              "  'word': ','},\n",
              " {'capitals_inside': False,\n",
              "  'has_hyphen': False,\n",
              "  'is_all_caps': True,\n",
              "  'is_all_lower': True,\n",
              "  'is_capitalized': True,\n",
              "  'is_first': False,\n",
              "  'is_last': False,\n",
              "  'is_numeric': True,\n",
              "  'next_word': 'years',\n",
              "  'prefix-1': '6',\n",
              "  'prefix-2': '61',\n",
              "  'prefix-3': '61',\n",
              "  'prev_word': ',',\n",
              "  'suffix-1': '1',\n",
              "  'suffix-2': '61',\n",
              "  'suffix-3': '61',\n",
              "  'word': '61'},\n",
              " {'capitals_inside': False,\n",
              "  'has_hyphen': False,\n",
              "  'is_all_caps': False,\n",
              "  'is_all_lower': True,\n",
              "  'is_capitalized': False,\n",
              "  'is_first': False,\n",
              "  'is_last': False,\n",
              "  'is_numeric': False,\n",
              "  'next_word': 'old',\n",
              "  'prefix-1': 'y',\n",
              "  'prefix-2': 'ye',\n",
              "  'prefix-3': 'yea',\n",
              "  'prev_word': '61',\n",
              "  'suffix-1': 's',\n",
              "  'suffix-2': 'rs',\n",
              "  'suffix-3': 'ars',\n",
              "  'word': 'years'}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zjiofTOavHKt",
        "outputId": "eb2f7130-a633-4c06-878b-3207bdbc9bad"
      },
      "source": [
        "# 각 단어별 POS 태그가 출력됨\n",
        "y[:5]"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['NNP', 'NNP', ',', 'CD', 'NNS']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0MoCO29Yz8v"
      },
      "source": [
        "\n",
        "분류기 훈련할 준비가 되었다. 분류기 훈련은 결정트리 기계학습으로 이루어진다. \n",
        "scikit-learn 모듈의 DecisionTreeClassifier로 실행한다.\n",
        "\n",
        "결정트리(Decision Tree)는 데이터들의 속성으로부터 패턴을 찾아서 분류를 수행하는 지도학습 기계학습 모델이다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KmzA0-ZdYz8v",
        "outputId": "8752a884-3265-4a33-ef36-38b5f039517c"
      },
      "source": [
        "#결정트리 클래스를 불러옴\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "#DictVectorizer 클래스를 불러옴. DictVectorizer는 data vectorizerion을 의미하며 데이터를 실수 형태로 전환하는 것임\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "#파이프라인을 불러옴. 파이프라인 클래스는 연속된 변환을 순차적으로 처리할 수 있는 기능을 제공하는 유용한 wrapper 도구임. (출처: https://skasha.tistory.com/80)\n",
        "from sklearn.pipeline import Pipeline \n",
        " \n",
        "clf = Pipeline([\n",
        "    ('vectorizer', DictVectorizer(sparse=False)), #sparse=False로 지정하면 밀집 벡터를 출력할 수 있음\n",
        "    ('classifier', DecisionTreeClassifier(criterion='entropy')) \n",
        "    #기본적으로는 지니 불순도가 사용되지만, criterion 매개변수를 entropy로 지정하면 entropy불순도를 사용할 수 있음. \n",
        "    #정확히 잘 예측했다면 entropy는 0이 됨. 지니와 비교했을 때, 지니가 속도가 더 빠르지만 일반적으로 한쪽 가지로 고립시키는 경향이 있음.\n",
        "    #하지만 엔트로피는 조금 더 균형잡힌 트리를 만들 수 있음.\n",
        "])\n",
        "\n",
        "clf.fit(X[:10000], y[:10000])   # 코드를 여러 번 돌린다면 시간이 오래 걸리므로 만 개의 샘플만 사용하는 것을 추천함(해당 블로그에서)\n",
        " \n",
        "print('Training completed')\n",
        "\n",
        "# 테스트 데이터를 함수에 넣어 테스트 데이터의 피처와 태그를 추출함\n",
        "X_test, y_test = transform_to_dataset(test_sentences)\n",
        "\n",
        "# 파이프라인이 제공하는 score 메소드로 평가\n",
        "print(\"Accuracy:\", clf.score(X_test, y_test))\n",
        " \n",
        "# Accuracy: 0.895026514542825 # 성능이 꽤 훌륭하게 나왔음"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training completed\n",
            "Accuracy: 0.895026514542825\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_fwaZ_sHYz8w"
      },
      "source": [
        "# 성능을 확인했으니 분류기를 아래와 같이 사용할 수 있다"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wnds_xEgYz8w",
        "outputId": "6af80d15-4a9e-4088-9c2f-53dda1dea855"
      },
      "source": [
        "# POS 태거 함수\n",
        "def pos_tag(sentence):\n",
        "    # 훈련한 것을 바탕으로 입력값 결과 예측\n",
        "    tags = clf.predict([features(sentence, index) for index in range(len(sentence))])\n",
        "    return zip(sentence, tags) #zip()은 동일한 개수로 이루어진 자료형을 묶어주는 역할을 하는 함수임\n",
        "\n",
        "# 토크나이즈를 한 후, 위에서 만든 pos_tag 함수로 태깅을 하고 이를 리스트화하여 출력함\n",
        "print(list(pos_tag(word_tokenize('This is my friend, John.'))))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('This', 'DT'), ('is', 'VBZ'), ('my', 'NN'), ('friend', 'NN'), (',', ','), ('John', 'NNP'), ('.', '.')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfiJ1UdZ1bp4"
      },
      "source": [
        "# 결과값 출력\n",
        "\n",
        "\n",
        "```\n",
        "[('This', 'DT'), ('is', 'VBZ'), ('my', 'NN'), ('friend', 'NN'), (',', ','), ('John', 'NNP'), ('.', '.')]\n",
        "```\n",
        "- 'This'는 한정사(Determiner)\n",
        "- 'is'는 3인칭 단수 현재 동사(verb, 3rd person singular present)\n",
        "- 'my'는 단수 혹은 불가산 명사(Noun, singular or mass)\n",
        "- 'friend'는 단수 혹은 불가산 명사(Noun, singular or mass)\n",
        "- 'John'은 단수 고유 명사(Proper noun, singular)\n",
        "\n",
        "**결과값에서 사실 'my'는 단수 혹은 불가산 명사(NN, Noun, singular or mass)보다는 한정사(DT, Determiner)가 더 적절하다. \n",
        "분류기의 성능이 완벽하지는 않음을 볼 수 있다.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtAGB1PX5C7S"
      },
      "source": [
        "# 결론\n",
        "\n",
        "이렇게 간단한 방법으로 나만의 POS 태거를 만들어볼 수 있다. 부담 가지 말고 설명과 코드의 주석을 읽어보며 천천히 따라해본다면 어느새 POS 태거를 만들 수 있을 것이다. POS 태거를 만들었다면, 심화 학습으로 POS 태거의 성능을 더 향상시킬 수 있는 방법에 대해서 고민해보고 코드를 수정해보자."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sKOQyKMB5nQd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}