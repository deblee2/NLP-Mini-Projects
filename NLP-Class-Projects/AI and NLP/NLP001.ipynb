{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Text Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "sentence = \"The quick brown fox jumps over the lazy dog\"\n",
    "print('quick' in sentence) #리스트 구조, x in y(list)=>true or false, 리스트방에 알파벳을 하나씩 하나씩 입력하는 구조\n",
    "print(sentence.index('fox'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"The quick brown fox jumps over the lazy dog\"\n",
    "t = sentence.split() #split(구분자, deliminator)\n",
    "##sentence = \"The quick brown fox jumps over the lazy dog .\"\n",
    "##text processing 순서\n",
    "##1. regular expression 정규식\n",
    "## 적당하게 문자열을 처리해요\n",
    "## dog. ==> dog .(한 칸 띄우기) 마침표를 단어에서 분리하는 작업\n",
    "## 2. split 함수\n",
    "## 단어화, tokenize\n",
    "\n",
    "## 대체방식\n",
    "## NLTK와 같은 자연어처리 라이브러리를 사용\n",
    "\n",
    "print(len(t))\n",
    "print(t)\n",
    "t.index('lazy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "brown\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'nworb'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(sentence.split()[2])\n",
    "sentence.split()[2][::-1] #처음부터 끝까지 뒤집기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "녕안\n"
     ]
    }
   ],
   "source": [
    "r = \"안녕\" #ㅇ + ㅏ + ㄴ ㄴ + ㅕ + ㅇ\n",
    "print(r[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The:dog\n"
     ]
    }
   ],
   "source": [
    "words = sentence.split()\n",
    "first_word = words[0]\n",
    "last_word = words[len(words)-1]\n",
    "concat_word = first_word + ':' + last_word\n",
    "print(concat_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'brown', 'jumps', 'the', 'dog']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[words[i] for i in range(len(words)) if i%2 == 0]  #리스트로 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dog'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence[-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'god yzal eht revo spmuj xof nworb kciuq ehT'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ehT kciuq nworb xof spmuj revo eht yzal god\n"
     ]
    }
   ],
   "source": [
    "print(' '.join([word[::-1] for word in words]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'reading', 'NLP', 'Fundamentals']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "\n",
    "words = word_tokenize(\"I am reading NLP Fundamentals\")\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'PRP'),\n",
       " ('am', 'VBP'),\n",
       " ('reading', 'VBG'),\n",
       " ('NLP', 'NNP'),\n",
       " ('Fundamentals', 'NNS')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "stop_words = stopwords.words('English') #English, Italian, French, German\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'learning', 'Python', '.', 'It', 'is', 'one', 'of', 'the', 'most', 'popular', 'programming', 'languages']\n",
      "I learning Python . It one popular programming languages\n"
     ]
    }
   ],
   "source": [
    "sentence = \"I am learning Python. It is one of the most popular programming languages\"\n",
    "sentence_words = word_tokenize(sentence)\n",
    "print(sentence_words)\n",
    "\n",
    "sentence_no_stops = ' '.join([word for word in sentence_words if word not in stop_words])\n",
    "print(sentence_no_stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I visited United States from United Kingdom on 22-10-2018\n"
     ]
    }
   ],
   "source": [
    "sentence = \"I visited US from UK on 22-10-18\"\n",
    "normalized_sentence = sentence.replace(\"US\", \"United States\").replace(\"UK\", \"United Kingdom\").replace(\"-18\", \"-2018\")\n",
    "print(normalized_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 연습문제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음 문장들을 tokenize 하고 몇 개의 단어로 구성되었는지 세시오.\n",
    "\n",
    "s = 'Her mother had grown up in Puerto Rico before moving to Indiana and then Florida, and while she had sought out good public schools for her children, she didn’t push for education beyond that. But Ms. Lathion had high grades and a college counselor suggested that she apply to QuestBridge, a nonprofit that matches low-income students with colleges offering full-tuition scholarships.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Her', 'mother', 'had', 'grown', 'up', 'in', 'Puerto', 'Rico', 'before', 'moving', 'to', 'Indiana', 'and', 'then', 'Florida', ',', 'and', 'while', 'she', 'had', 'sought', 'out', 'good', 'public', 'schools', 'for', 'her', 'children', ',', 'she', 'didn', '’', 't', 'push', 'for', 'education', 'beyond', 'that', '.', 'But', 'Ms.', 'Lathion', 'had', 'high', 'grades', 'and', 'a', 'college', 'counselor', 'suggested', 'that', 'she', 'apply', 'to', 'QuestBridge', ',', 'a', 'nonprofit', 'that', 'matches', 'low-income', 'students', 'with', 'colleges', 'offering', 'full-tuition', 'scholarships', '.']\n",
      "68\n"
     ]
    }
   ],
   "source": [
    "s = 'Her mother had grown up in Puerto Rico before moving to Indiana and then Florida, and while she had sought out good public schools for her children, she didn’t push for education beyond that. But Ms. Lathion had high grades and a college counselor suggested that she apply to QuestBridge, a nonprofit that matches low-income students with colleges offering full-tuition scholarships.'\n",
    "\n",
    "s_words = word_tokenize(s)\n",
    "print(s_words)\n",
    "print(len(s_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# autocorrect module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting autocorrect\n",
      "  Downloading https://files.pythonhosted.org/packages/a9/b0/a1d628fa192e8ebf124b4cebc2a42b4e3aa65b8052fdf4888e04fadf3e8d/autocorrect-1.1.0.tar.gz (1.8MB)\n",
      "Building wheels for collected packages: autocorrect\n",
      "  Building wheel for autocorrect (setup.py): started\n",
      "  Building wheel for autocorrect (setup.py): finished with status 'done'\n",
      "  Created wheel for autocorrect: filename=autocorrect-1.1.0-cp36-none-any.whl size=1810773 sha256=0da5a6ec3e91757ba1c307973da3c4b6f505a3f7fd6e9535150ccc035b9ec03e\n",
      "  Stored in directory: C:\\Users\\USER\\AppData\\Local\\pip\\Cache\\wheels\\78\\7f\\b1\\527522820ae623df6a2dbe14f778d23adaea4bebe43f7ebcfe\n",
      "Successfully built autocorrect\n",
      "Installing collected packages: autocorrect\n",
      "Successfully installed autocorrect-1.1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 19.3.1; however, version 20.0.2 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install autocorrect\n",
    "from autocorrect import spell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "autocorrect.spell is deprecated,             use autocorrect.Speller instead\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Natural'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spell('Natureal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ntural', 'Luanguage', 'Processin', 'deals', 'with', 'the', 'art', 'of', 'extracting', 'insightes', 'from', 'Natural', 'Languaes']\n",
      "autocorrect.spell is deprecated,             use autocorrect.Speller instead\n",
      "autocorrect.spell is deprecated,             use autocorrect.Speller instead\n",
      "autocorrect.spell is deprecated,             use autocorrect.Speller instead\n",
      "autocorrect.spell is deprecated,             use autocorrect.Speller instead\n",
      "autocorrect.spell is deprecated,             use autocorrect.Speller instead\n",
      "autocorrect.spell is deprecated,             use autocorrect.Speller instead\n",
      "autocorrect.spell is deprecated,             use autocorrect.Speller instead\n",
      "autocorrect.spell is deprecated,             use autocorrect.Speller instead\n",
      "autocorrect.spell is deprecated,             use autocorrect.Speller instead\n",
      "autocorrect.spell is deprecated,             use autocorrect.Speller instead\n",
      "autocorrect.spell is deprecated,             use autocorrect.Speller instead\n",
      "autocorrect.spell is deprecated,             use autocorrect.Speller instead\n",
      "autocorrect.spell is deprecated,             use autocorrect.Speller instead\n",
      "Natural Language Procession deals with the art of extracting insights from Natural Languages\n"
     ]
    }
   ],
   "source": [
    "sentence = word_tokenize(\"Ntural Luanguage Processin deals with the art of extracting insightes from Natural Languaes\")\n",
    "print(sentence)\n",
    "\n",
    "sentence_corrected = ' '.join([spell(word) for word in sentence])\n",
    "print(sentence_corrected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 연습문제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음 단어에서 문제가 되는 단어들을 찾아 내시오.\n",
    "\n",
    "\n",
    "t = 'hardly introspctive triffle your now servent to cloths incorrigable their'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: autocorrect in c:\\users\\user\\anaconda3\\lib\\site-packages (1.1.0)\n",
      "autocorrect.spell is deprecated,             use autocorrect.Speller instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.0.2; however, version 20.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\user\\anaconda3\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "autocorrect.spell is deprecated,             use autocorrect.Speller instead\n",
      "introspctive ==> introspective\n",
      "autocorrect.spell is deprecated,             use autocorrect.Speller instead\n",
      "triffle ==> trifle\n",
      "autocorrect.spell is deprecated,             use autocorrect.Speller instead\n",
      "autocorrect.spell is deprecated,             use autocorrect.Speller instead\n",
      "autocorrect.spell is deprecated,             use autocorrect.Speller instead\n",
      "servent ==> servant\n",
      "autocorrect.spell is deprecated,             use autocorrect.Speller instead\n",
      "autocorrect.spell is deprecated,             use autocorrect.Speller instead\n",
      "autocorrect.spell is deprecated,             use autocorrect.Speller instead\n",
      "incorrigable ==> incorrigible\n",
      "autocorrect.spell is deprecated,             use autocorrect.Speller instead\n"
     ]
    }
   ],
   "source": [
    "!pip install autocorrect\n",
    "from autocorrect import spell\n",
    "\n",
    "t = 'hardly introspctive triffle your now servent to cloths incorrigable their'\n",
    "t_words = word_tokenize(t)\n",
    "\n",
    "#sentence_corrected = ' '.join([spell(word) for word in t])\n",
    "for word in t_words:\n",
    "    t = spell(word)\n",
    "    if t == word:\n",
    "        pass\n",
    "    else: \n",
    "        print(word, \"==>\", t)\n",
    "#print(sentence_corrected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# porter stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'product'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer = nltk.stem.PorterStemmer()\n",
    "stemmer.stem(\"production\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'come'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem(\"coming\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fire'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem(\"firing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'battl'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem(\"battling\") #battle\n",
    "#stemmer 자르기\n",
    "#ing, -ion, ation\n",
    "# 단어의 기본형을 알파벳을 뒤에서부터 잘라내서 맞춰주는 작업\n",
    "# unknown word: 한 번도 본 적이 없는 단어"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 연습문제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음 문장에서 단어들을 stemmer로 바꾸어 보시오.\n",
    "\n",
    "t = 'One student sat at a vacation home on the coast of Maine'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One ==> one\n",
      "student ==> student\n",
      "sat ==> sat\n",
      "at ==> at\n",
      "a ==> a\n",
      "vacation ==> vacat\n",
      "home ==> home\n",
      "on ==> on\n",
      "the ==> the\n",
      "coast ==> coast\n",
      "of ==> of\n",
      "Maine ==> main\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "\n",
    "t = 'One student sat at a vacation home on the coast of Maine'\n",
    "t_words = word_tokenize(t)\n",
    "\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "for ss in t_words:\n",
    "    print(ss, \"==>\", stemmer.stem(ss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'product'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "lemmatizer.lemmatize('products')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'production'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('production')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'coming'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('coming')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 연습문제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음 문장들에서 몇 개의 lemma가 사용되었는지 세고 token과의 차이가 어느 정도 나는지 확인하시오.\n",
    "\n",
    "\n",
    "s = 'Her mother had grown up in Puerto Rico before moving to Indiana and then Florida, and while she had sought out good public schools for her children, she didn’t push for education beyond that. But Ms. Lathion had high grades and a college counselor suggested that she apply to QuestBridge, a nonprofit that matches low-income students with colleges offering full-tuition scholarships.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "product\n",
      "Her Her\n",
      "mother mother\n",
      "had had\n",
      "grown grown\n",
      "up up\n",
      "in in\n",
      "Puerto Puerto\n",
      "Rico Rico\n",
      "before before\n",
      "moving moving\n",
      "to to\n",
      "Indiana Indiana\n",
      "and and\n",
      "then then\n",
      "Florida Florida\n",
      ", ,\n",
      "and and\n",
      "while while\n",
      "she she\n",
      "had had\n",
      "sought sought\n",
      "out out\n",
      "good good\n",
      "public public\n",
      "schools school\n",
      "for for\n",
      "her her\n",
      "children child\n",
      ", ,\n",
      "she she\n",
      "didn didn\n",
      "’ ’\n",
      "t t\n",
      "push push\n",
      "for for\n",
      "education education\n",
      "beyond beyond\n",
      "that that\n",
      ". .\n",
      "But But\n",
      "Ms. Ms.\n",
      "Lathion Lathion\n",
      "had had\n",
      "high high\n",
      "grades grade\n",
      "and and\n",
      "a a\n",
      "college college\n",
      "counselor counselor\n",
      "suggested suggested\n",
      "that that\n",
      "she she\n",
      "apply apply\n",
      "to to\n",
      "QuestBridge QuestBridge\n",
      ", ,\n",
      "a a\n",
      "nonprofit nonprofit\n",
      "that that\n",
      "matches match\n",
      "low-income low-income\n",
      "students student\n",
      "with with\n",
      "colleges college\n",
      "offering offering\n",
      "full-tuition full-tuition\n",
      "scholarships scholarship\n",
      ". .\n",
      "lemma:  7\n",
      "token:  68\n",
      "difference between token and lemma:  61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "nltk.download('wordnet')\n",
    "\n",
    "s = 'Her mother had grown up in Puerto Rico before moving to Indiana and then Florida, and while she had sought out good public schools for her children, she didn’t push for education beyond that. But Ms. Lathion had high grades and a college counselor suggested that she apply to QuestBridge, a nonprofit that matches low-income students with colleges offering full-tuition scholarships.'\n",
    "s_word = word_tokenize(s)\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(lemmatizer.lemmatize('products'))\n",
    "\n",
    "cnt=0\n",
    "\n",
    "for w in s_word:\n",
    "    r = lemmatizer.lemmatize(w)\n",
    "    print(w, lemmatizer.lemmatize(w))\n",
    "    if r == w:\n",
    "        pass\n",
    "    else:\n",
    "        cnt += 1\n",
    "    \n",
    "print(\"lemma: \", cnt)\n",
    "print(\"token: \", len(s_word))\n",
    "print(\"difference between token and lemma: \", len(s_word)-cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sentence tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['We are reading a book.',\n",
       " 'Do you know who is the publisher?',\n",
       " 'It is Packt.',\n",
       " 'Packt is based out of Birmingham.']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "sent_tokenize(\"We are reading a book. Do you know who is the publisher? It is Packt. Packt is based out of Birmingham.\")\n",
    "# Mr. Mrs.\n",
    "# . \" , ! \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 연습문제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음 문장들을 한 문장씩 나누고 몇 개의 문장으로 구성되었는지 세시오.\n",
    "\n",
    "s = 'Her mother had grown up in Puerto Rico before moving to Indiana and then Florida, and while she had sought out good public schools for her children, she didn’t push for education beyond that. But Ms. Lathion had high grades and a college counselor suggested that she apply to QuestBridge, a nonprofit that matches low-income students with colleges offering full-tuition scholarships.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Her mother had grown up in Puerto Rico before moving to Indiana and then Florida, and while she had sought out good public schools for her children, she didn’t push for education beyond that.', 'But Ms. Lathion had high grades and a college counselor suggested that she apply to QuestBridge, a nonprofit that matches low-income students with colleges offering full-tuition scholarships.']\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "st = 'Her mother had grown up in Puerto Rico before moving to Indiana and then Florida, and while she had sought out good public schools for her children, she didn’t push for education beyond that. But Ms. Lathion had high grades and a college counselor suggested that she apply to QuestBridge, a nonprofit that matches low-income students with colleges offering full-tuition scholarships.'\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "tt = sent_tokenize(st)\n",
    "\n",
    "print(tt)\n",
    "print(len(tt))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word sense disambiguation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('savings_bank.n.02')\n",
      "Synset('bank.v.07')\n"
     ]
    }
   ],
   "source": [
    "from nltk.wsd import lesk\n",
    "from nltk import word_tokenize\n",
    "\n",
    "sentence1 = \"Keep your savings in the bank\" #bank 1. 은행 2. 강둑\n",
    "print(lesk(word_tokenize(sentence1), 'bank'))\n",
    "\n",
    "sentence2 = \"It's so risky to drive over the banks of the road\"\n",
    "print(lesk(word_tokenize(sentence2), 'bank'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
