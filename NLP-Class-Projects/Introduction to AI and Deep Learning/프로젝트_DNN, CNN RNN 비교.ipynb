{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pima-indians-diabetes.csv 데이터를 활용하여 MLP, CNN, RNN을 만들었습니다. \n",
    "\n",
    "책에서 데이터의 코드 accuracy 결과를 보면(11과) 약 72%로 나오고 있어 이보다 더 높은 수치를 내는 것을 목표로 하였습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Accuracy: 0.7446*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "#데이터 적용\n",
    "my_data = \"C:\\\\Users\\\\user\\\\Desktop\\\\dataset\\\\pima-indians-diabetes.csv\"\n",
    "dataset = np.loadtxt(my_data, delimiter=\",\")\n",
    "X = dataset[:, 0:8]\n",
    "Y = dataset[:, 8]\n",
    "\n",
    "#학습셋과 테스트셋의 구분\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLP에서는 은닉층을 추가하여 총 3개의 은닉층이 존재하고, 인풋 노드 수를 50으로 늘렸습니다\n",
    "\n",
    "Dropout layer은 제거하는 것이 더 accuracy가 높게 나왔습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#모델의 설정\n",
    "model = Sequential()\n",
    "model.add(Dense(40, input_dim=8, activation='relu')) #1차원 데이터이므로 dense 전에 Flatten을 할 필요가 없음\n",
    "model.add(Dense(20, activation='relu')) \n",
    "model.add(Dense(10, activation='relu')) \n",
    "model.add(Dense(1, activation='sigmoid')) #0과 1로 이루어진 binary 데이터이므로 sigmoid를 사용함\n",
    "\n",
    "#모델 컴파일\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) #이항 분류이므로 binary crossentropy를 사용함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "에포크를 늘리고 batch_size를 추가하였습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_55\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_124 (Dense)            (None, 40)                360       \n",
      "_________________________________________________________________\n",
      "dense_125 (Dense)            (None, 20)                820       \n",
      "_________________________________________________________________\n",
      "dense_126 (Dense)            (None, 10)                210       \n",
      "_________________________________________________________________\n",
      "dense_127 (Dense)            (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 1,401\n",
      "Trainable params: 1,401\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 537 samples\n",
      "Epoch 1/100\n",
      "537/537 [==============================] - 1s 1ms/sample - loss: 2.1801 - accuracy: 0.5196\n",
      "Epoch 2/100\n",
      "537/537 [==============================] - 0s 191us/sample - loss: 0.7293 - accuracy: 0.5996\n",
      "Epoch 3/100\n",
      "537/537 [==============================] - 0s 197us/sample - loss: 0.6785 - accuracy: 0.6406\n",
      "Epoch 4/100\n",
      "537/537 [==============================] - 0s 250us/sample - loss: 0.6680 - accuracy: 0.6462\n",
      "Epoch 5/100\n",
      "537/537 [==============================] - 0s 409us/sample - loss: 0.6120 - accuracy: 0.6704\n",
      "Epoch 6/100\n",
      "537/537 [==============================] - 0s 310us/sample - loss: 0.6148 - accuracy: 0.6890\n",
      "Epoch 7/100\n",
      "537/537 [==============================] - 0s 189us/sample - loss: 0.6276 - accuracy: 0.6480\n",
      "Epoch 8/100\n",
      "537/537 [==============================] - 0s 190us/sample - loss: 0.6189 - accuracy: 0.6760\n",
      "Epoch 9/100\n",
      "537/537 [==============================] - 0s 200us/sample - loss: 0.5869 - accuracy: 0.6797\n",
      "Epoch 10/100\n",
      "537/537 [==============================] - 0s 218us/sample - loss: 0.5824 - accuracy: 0.7002\n",
      "Epoch 11/100\n",
      "537/537 [==============================] - 0s 226us/sample - loss: 0.5912 - accuracy: 0.6853\n",
      "Epoch 12/100\n",
      "537/537 [==============================] - 0s 195us/sample - loss: 0.5710 - accuracy: 0.7114\n",
      "Epoch 13/100\n",
      "537/537 [==============================] - 0s 251us/sample - loss: 0.5907 - accuracy: 0.6890\n",
      "Epoch 14/100\n",
      "537/537 [==============================] - 0s 230us/sample - loss: 0.5986 - accuracy: 0.6872\n",
      "Epoch 15/100\n",
      "537/537 [==============================] - 0s 227us/sample - loss: 0.5842 - accuracy: 0.6667\n",
      "Epoch 16/100\n",
      "537/537 [==============================] - 0s 225us/sample - loss: 0.5639 - accuracy: 0.7188\n",
      "Epoch 17/100\n",
      "537/537 [==============================] - 0s 224us/sample - loss: 0.5807 - accuracy: 0.7207\n",
      "Epoch 18/100\n",
      "537/537 [==============================] - 0s 224us/sample - loss: 0.5736 - accuracy: 0.7095\n",
      "Epoch 19/100\n",
      "537/537 [==============================] - 0s 219us/sample - loss: 0.5744 - accuracy: 0.7039\n",
      "Epoch 20/100\n",
      "537/537 [==============================] - 0s 215us/sample - loss: 0.5675 - accuracy: 0.6965\n",
      "Epoch 21/100\n",
      "537/537 [==============================] - 0s 243us/sample - loss: 0.5433 - accuracy: 0.7281\n",
      "Epoch 22/100\n",
      "537/537 [==============================] - 0s 262us/sample - loss: 0.5807 - accuracy: 0.6890\n",
      "Epoch 23/100\n",
      "537/537 [==============================] - 0s 241us/sample - loss: 0.5682 - accuracy: 0.7114\n",
      "Epoch 24/100\n",
      "537/537 [==============================] - 0s 214us/sample - loss: 0.5812 - accuracy: 0.7058\n",
      "Epoch 25/100\n",
      "537/537 [==============================] - 0s 334us/sample - loss: 0.5641 - accuracy: 0.6946\n",
      "Epoch 26/100\n",
      "537/537 [==============================] - 0s 348us/sample - loss: 0.5841 - accuracy: 0.6965\n",
      "Epoch 27/100\n",
      "537/537 [==============================] - 0s 309us/sample - loss: 0.5496 - accuracy: 0.7076\n",
      "Epoch 28/100\n",
      "537/537 [==============================] - 0s 231us/sample - loss: 0.5710 - accuracy: 0.7039\n",
      "Epoch 29/100\n",
      "537/537 [==============================] - 0s 235us/sample - loss: 0.5798 - accuracy: 0.7020\n",
      "Epoch 30/100\n",
      "537/537 [==============================] - 0s 232us/sample - loss: 0.5769 - accuracy: 0.7039\n",
      "Epoch 31/100\n",
      "537/537 [==============================] - 0s 220us/sample - loss: 0.5502 - accuracy: 0.7076\n",
      "Epoch 32/100\n",
      "537/537 [==============================] - 0s 233us/sample - loss: 0.5434 - accuracy: 0.7337\n",
      "Epoch 33/100\n",
      "537/537 [==============================] - 0s 213us/sample - loss: 0.5530 - accuracy: 0.7188\n",
      "Epoch 34/100\n",
      "537/537 [==============================] - 0s 233us/sample - loss: 0.5412 - accuracy: 0.7318\n",
      "Epoch 35/100\n",
      "537/537 [==============================] - 0s 216us/sample - loss: 0.5407 - accuracy: 0.7244\n",
      "Epoch 36/100\n",
      "537/537 [==============================] - 0s 218us/sample - loss: 0.5429 - accuracy: 0.7337\n",
      "Epoch 37/100\n",
      "537/537 [==============================] - 0s 259us/sample - loss: 0.5453 - accuracy: 0.7132\n",
      "Epoch 38/100\n",
      "537/537 [==============================] - 0s 238us/sample - loss: 0.5338 - accuracy: 0.7337\n",
      "Epoch 39/100\n",
      "537/537 [==============================] - 0s 287us/sample - loss: 0.5326 - accuracy: 0.7393\n",
      "Epoch 40/100\n",
      "537/537 [==============================] - 0s 253us/sample - loss: 0.5340 - accuracy: 0.7542\n",
      "Epoch 41/100\n",
      "537/537 [==============================] - 0s 218us/sample - loss: 0.5391 - accuracy: 0.7393\n",
      "Epoch 42/100\n",
      "537/537 [==============================] - 0s 244us/sample - loss: 0.5426 - accuracy: 0.7244\n",
      "Epoch 43/100\n",
      "537/537 [==============================] - 0s 257us/sample - loss: 0.5550 - accuracy: 0.7188\n",
      "Epoch 44/100\n",
      "537/537 [==============================] - 0s 205us/sample - loss: 0.5298 - accuracy: 0.7374\n",
      "Epoch 45/100\n",
      "537/537 [==============================] - 0s 255us/sample - loss: 0.5495 - accuracy: 0.7188\n",
      "Epoch 46/100\n",
      "537/537 [==============================] - 0s 244us/sample - loss: 0.5321 - accuracy: 0.7244\n",
      "Epoch 47/100\n",
      "537/537 [==============================] - 0s 231us/sample - loss: 0.5308 - accuracy: 0.7207\n",
      "Epoch 48/100\n",
      "537/537 [==============================] - 0s 203us/sample - loss: 0.5324 - accuracy: 0.7318\n",
      "Epoch 49/100\n",
      "537/537 [==============================] - 0s 194us/sample - loss: 0.5632 - accuracy: 0.7169\n",
      "Epoch 50/100\n",
      "537/537 [==============================] - 0s 194us/sample - loss: 0.5255 - accuracy: 0.7393\n",
      "Epoch 51/100\n",
      "537/537 [==============================] - 0s 225us/sample - loss: 0.5411 - accuracy: 0.7356\n",
      "Epoch 52/100\n",
      "537/537 [==============================] - 0s 340us/sample - loss: 0.5155 - accuracy: 0.7486\n",
      "Epoch 53/100\n",
      "537/537 [==============================] - 0s 284us/sample - loss: 0.5174 - accuracy: 0.7523\n",
      "Epoch 54/100\n",
      "537/537 [==============================] - 0s 263us/sample - loss: 0.5429 - accuracy: 0.7356\n",
      "Epoch 55/100\n",
      "537/537 [==============================] - 0s 249us/sample - loss: 0.5317 - accuracy: 0.7356\n",
      "Epoch 56/100\n",
      "537/537 [==============================] - 0s 189us/sample - loss: 0.5227 - accuracy: 0.7449\n",
      "Epoch 57/100\n",
      "537/537 [==============================] - 0s 197us/sample - loss: 0.5224 - accuracy: 0.7523\n",
      "Epoch 58/100\n",
      "537/537 [==============================] - ETA: 0s - loss: 0.5339 - accuracy: 0.74 - 0s 227us/sample - loss: 0.5324 - accuracy: 0.7412\n",
      "Epoch 59/100\n",
      "537/537 [==============================] - 0s 374us/sample - loss: 0.5116 - accuracy: 0.7598\n",
      "Epoch 60/100\n",
      "537/537 [==============================] - 0s 244us/sample - loss: 0.5090 - accuracy: 0.7430\n",
      "Epoch 61/100\n",
      "537/537 [==============================] - 0s 261us/sample - loss: 0.5117 - accuracy: 0.7412\n",
      "Epoch 62/100\n",
      "537/537 [==============================] - 0s 238us/sample - loss: 0.5172 - accuracy: 0.7412\n",
      "Epoch 63/100\n",
      "537/537 [==============================] - 0s 207us/sample - loss: 0.5194 - accuracy: 0.7374\n",
      "Epoch 64/100\n",
      "537/537 [==============================] - 0s 209us/sample - loss: 0.5072 - accuracy: 0.7412\n",
      "Epoch 65/100\n",
      "537/537 [==============================] - 0s 213us/sample - loss: 0.5160 - accuracy: 0.7300\n",
      "Epoch 66/100\n",
      "537/537 [==============================] - 0s 238us/sample - loss: 0.5234 - accuracy: 0.7449\n",
      "Epoch 67/100\n",
      "537/537 [==============================] - 0s 245us/sample - loss: 0.5188 - accuracy: 0.7374\n",
      "Epoch 68/100\n",
      "537/537 [==============================] - 0s 190us/sample - loss: 0.4988 - accuracy: 0.7467\n",
      "Epoch 69/100\n",
      "537/537 [==============================] - 0s 192us/sample - loss: 0.5078 - accuracy: 0.7467\n",
      "Epoch 70/100\n",
      "537/537 [==============================] - 0s 183us/sample - loss: 0.5083 - accuracy: 0.7542\n",
      "Epoch 71/100\n",
      "537/537 [==============================] - 0s 227us/sample - loss: 0.5020 - accuracy: 0.7486\n",
      "Epoch 72/100\n",
      "537/537 [==============================] - 0s 217us/sample - loss: 0.5049 - accuracy: 0.7281\n",
      "Epoch 73/100\n",
      "537/537 [==============================] - 0s 256us/sample - loss: 0.5141 - accuracy: 0.7523\n",
      "Epoch 74/100\n",
      "537/537 [==============================] - 0s 188us/sample - loss: 0.5182 - accuracy: 0.7356\n",
      "Epoch 75/100\n",
      "537/537 [==============================] - 0s 255us/sample - loss: 0.4983 - accuracy: 0.7505\n",
      "Epoch 76/100\n",
      "537/537 [==============================] - 0s 182us/sample - loss: 0.4964 - accuracy: 0.7430\n",
      "Epoch 77/100\n",
      "537/537 [==============================] - 0s 186us/sample - loss: 0.5067 - accuracy: 0.7616\n",
      "Epoch 78/100\n",
      "537/537 [==============================] - 0s 180us/sample - loss: 0.4901 - accuracy: 0.7598\n",
      "Epoch 79/100\n",
      "537/537 [==============================] - 0s 180us/sample - loss: 0.5014 - accuracy: 0.7505\n",
      "Epoch 80/100\n",
      "537/537 [==============================] - 0s 278us/sample - loss: 0.4995 - accuracy: 0.7467\n",
      "Epoch 81/100\n",
      "537/537 [==============================] - 0s 285us/sample - loss: 0.4857 - accuracy: 0.7747\n",
      "Epoch 82/100\n",
      "537/537 [==============================] - 0s 266us/sample - loss: 0.4962 - accuracy: 0.7523\n",
      "Epoch 83/100\n",
      "537/537 [==============================] - 0s 200us/sample - loss: 0.4982 - accuracy: 0.7505\n",
      "Epoch 84/100\n",
      "537/537 [==============================] - 0s 180us/sample - loss: 0.4941 - accuracy: 0.7579\n",
      "Epoch 85/100\n",
      "537/537 [==============================] - 0s 187us/sample - loss: 0.4902 - accuracy: 0.7542\n",
      "Epoch 86/100\n",
      "537/537 [==============================] - 0s 214us/sample - loss: 0.4969 - accuracy: 0.7430\n",
      "Epoch 87/100\n",
      "537/537 [==============================] - 0s 188us/sample - loss: 0.5031 - accuracy: 0.7598\n",
      "Epoch 88/100\n",
      "537/537 [==============================] - 0s 186us/sample - loss: 0.5097 - accuracy: 0.7542\n",
      "Epoch 89/100\n",
      "537/537 [==============================] - 0s 183us/sample - loss: 0.5228 - accuracy: 0.7430\n",
      "Epoch 90/100\n",
      "537/537 [==============================] - 0s 178us/sample - loss: 0.4985 - accuracy: 0.7803\n",
      "Epoch 91/100\n",
      "537/537 [==============================] - 0s 197us/sample - loss: 0.5028 - accuracy: 0.7523\n",
      "Epoch 92/100\n",
      "537/537 [==============================] - 0s 212us/sample - loss: 0.5033 - accuracy: 0.7356\n",
      "Epoch 93/100\n",
      "537/537 [==============================] - 0s 194us/sample - loss: 0.5186 - accuracy: 0.7318\n",
      "Epoch 94/100\n",
      "537/537 [==============================] - 0s 191us/sample - loss: 0.4790 - accuracy: 0.7691\n",
      "Epoch 95/100\n",
      "537/537 [==============================] - 0s 186us/sample - loss: 0.5025 - accuracy: 0.7561\n",
      "Epoch 96/100\n",
      "537/537 [==============================] - 0s 183us/sample - loss: 0.5102 - accuracy: 0.7486\n",
      "Epoch 97/100\n",
      "537/537 [==============================] - 0s 177us/sample - loss: 0.4882 - accuracy: 0.7747\n",
      "Epoch 98/100\n",
      "537/537 [==============================] - 0s 189us/sample - loss: 0.4929 - accuracy: 0.7691\n",
      "Epoch 99/100\n",
      "537/537 [==============================] - 0s 179us/sample - loss: 0.4850 - accuracy: 0.7672\n",
      "Epoch 100/100\n",
      "537/537 [==============================] - 0s 179us/sample - loss: 0.4914 - accuracy: 0.7598\n",
      "231/231 [==============================] - 1s 2ms/sample - loss: 0.5706 - accuracy: 0.7446\n",
      "\n",
      " Accuracy: 0.7446\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n",
    "\n",
    "#모델 실행\n",
    "model.fit(X_train, Y_train, epochs=100, batch_size=10)\n",
    "\n",
    "#결과출력\n",
    "print(\"\\n Accuracy: %.4f\" % model.evaluate(X_test, Y_test)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Accuracy: 0.7359*\n",
    "\n",
    "수업시간에는 mnist 데이터로 Conv2D를 활용하여 cnn을 만드는 방법을 배웠지만, 제가 사용한 indian 데이터는 1차원적 숫자 데이터이므로 접근을 달리 해야 된다고 생각했습니다. \n",
    "\n",
    "https://cs.stackexchange.com/questions/82027/can-cnn-be-used-for-numeric-data\n",
    "해당 링크의 글을 참고하였을 때 이미지가 아닌 숫자 데이터를 다루기 때문에 Conv2D보다는 Conv1D가 적합하다고 판단하여 Conv1D을 적용하였습니다.\n",
    "\n",
    "여러 시도를 해보았지만 accuracy가 최대 0.7359로 그리 높지 않은 것을 보아 Conv1D 모델을 indian 데이터에 적합한 모델이라고 판단할 수는 없을 것 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accuracy 0.7359\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv1D, MaxPooling1D\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "#seed값 설정\n",
    "seed=0\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(3)\n",
    "\n",
    "#데이터 적용\n",
    "my_data = \"C:\\\\Users\\\\user\\\\Desktop\\\\dataset\\\\pima-indians-diabetes.csv\"\n",
    "dataset = np.loadtxt(my_data, delimiter=\",\")\n",
    "X = dataset[:, 0:8]\n",
    "Y = dataset[:, 8]\n",
    "\n",
    "#학습셋과 테스트셋의 구분\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conv1D 모델은 처음 만들어보는 것이라 우여곡절이 있었습니다. \n",
    "\n",
    "특히 csv의 숫자 데이터셋의 차원을 reshaping하는 것이 mnist 이미지 데이터와 달라 고민했습니다.\n",
    "\n",
    "Conv2D는 X_train.reshape(X_train.shape[0], 28, 28, 1)과 같은 차원으로 표현되는 반면, \n",
    "\n",
    "Conv1D는 X_train.reshape((X_train.shape[0], X_train.shape[1], 1)과 같은 차원으로 표현하여 차이가 있었습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터를 reshaping\n",
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1)) \n",
    "#데이터의 class가 0 or 1이므로 따로 원핫 인코딩을 적용할 필요가 없음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conv1D는 은닉층을 생성할 때 input node를 따로 추가하지 않고, kernel_size를 1차원으로, input_shape를 2차원으로 추가하였습니다.\n",
    "\n",
    "기존의 모델에 비해 은닉층을 하나 더 추가하였고, 필터의 크기를 크게 할수록 accuracy가 높게 나왔습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#모델 설정\n",
    "model = Sequential()\n",
    "model.add(Conv1D(400, kernel_size=3, input_shape=(X_train.shape[1], 1), activation='relu'))\n",
    "model.add(Conv1D(800, kernel_size=3, activation='relu'))\n",
    "model.add(Conv1D(800, kernel_size=1, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(200, activation='relu'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(1, activation='sigmoid')) \n",
    "#이항 분류이므로 활성화함수로 sigmoid를 사용함\n",
    "#마지막에는 값이 1개가 도출되어야 하기에 노드를 1로 설정함\n",
    "          \n",
    "#모델 컴파일\n",
    "model.compile(loss='binary_crossentropy', #이항분류이므로 binary crossentropy 사용\n",
    "             optimizer='adam',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "epoch를 더 크게 할수록 accuracy가 높게 나왔습니다. \n",
    "그렇지만 epoch가 100을 넘은 이후로는 accuracy가 비슷하게 나와 100으로 설정했습니다. \n",
    "\n",
    "batch_size도 추가했습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_22\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_38 (Conv1D)           (None, 6, 400)            1600      \n",
      "_________________________________________________________________\n",
      "conv1d_39 (Conv1D)           (None, 4, 800)            960800    \n",
      "_________________________________________________________________\n",
      "conv1d_40 (Conv1D)           (None, 4, 800)            640800    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling (None, 2, 800)            0         \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 2, 800)            0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_45 (Dense)             (None, 200)               320200    \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_46 (Dense)             (None, 1)                 201       \n",
      "=================================================================\n",
      "Total params: 1,923,601\n",
      "Trainable params: 1,923,601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 537 samples, validate on 231 samples\n",
      "Epoch 1/100\n",
      "537/537 [==============================] - 3s 6ms/sample - loss: 3.2182 - accuracy: 0.5140 - val_loss: 0.7329 - val_accuracy: 0.6797\n",
      "Epoch 2/100\n",
      "537/537 [==============================] - 2s 4ms/sample - loss: 0.7609 - accuracy: 0.6034 - val_loss: 0.6390 - val_accuracy: 0.6797\n",
      "Epoch 3/100\n",
      "537/537 [==============================] - 2s 4ms/sample - loss: 0.6866 - accuracy: 0.6499 - val_loss: 0.6727 - val_accuracy: 0.5714\n",
      "Epoch 4/100\n",
      "537/537 [==============================] - 2s 4ms/sample - loss: 0.6732 - accuracy: 0.6387 - val_loss: 0.6152 - val_accuracy: 0.6537\n",
      "Epoch 5/100\n",
      "537/537 [==============================] - 2s 4ms/sample - loss: 0.6359 - accuracy: 0.6536 - val_loss: 0.6235 - val_accuracy: 0.6364\n",
      "Epoch 6/100\n",
      "537/537 [==============================] - 3s 5ms/sample - loss: 0.6510 - accuracy: 0.6462 - val_loss: 0.6064 - val_accuracy: 0.6926\n",
      "Epoch 7/100\n",
      "537/537 [==============================] - 3s 5ms/sample - loss: 0.6335 - accuracy: 0.6574 - val_loss: 0.6770 - val_accuracy: 0.5801\n",
      "Epoch 8/100\n",
      "537/537 [==============================] - 3s 5ms/sample - loss: 0.6466 - accuracy: 0.6667 - val_loss: 0.6292 - val_accuracy: 0.6537\n",
      "Epoch 9/100\n",
      "537/537 [==============================] - 3s 5ms/sample - loss: 0.6099 - accuracy: 0.6760 - val_loss: 0.6067 - val_accuracy: 0.6926\n",
      "Epoch 10/100\n",
      "537/537 [==============================] - 3s 5ms/sample - loss: 0.5999 - accuracy: 0.6853 - val_loss: 0.6299 - val_accuracy: 0.6580\n",
      "Epoch 11/100\n",
      "537/537 [==============================] - 2s 4ms/sample - loss: 0.6192 - accuracy: 0.6946 - val_loss: 0.7120 - val_accuracy: 0.5368\n",
      "Epoch 12/100\n",
      "537/537 [==============================] - 2s 5ms/sample - loss: 0.6203 - accuracy: 0.6518 - val_loss: 0.6091 - val_accuracy: 0.6840\n",
      "Epoch 13/100\n",
      "537/537 [==============================] - 2s 4ms/sample - loss: 0.5944 - accuracy: 0.6927 - val_loss: 0.6563 - val_accuracy: 0.6320\n",
      "Epoch 14/100\n",
      "537/537 [==============================] - 2s 4ms/sample - loss: 0.6107 - accuracy: 0.6723 - val_loss: 0.6032 - val_accuracy: 0.7186\n",
      "Epoch 15/100\n",
      "537/537 [==============================] - 2s 4ms/sample - loss: 0.5920 - accuracy: 0.6834 - val_loss: 0.6193 - val_accuracy: 0.6926\n",
      "Epoch 16/100\n",
      "537/537 [==============================] - 2s 4ms/sample - loss: 0.5870 - accuracy: 0.6853 - val_loss: 0.6217 - val_accuracy: 0.6970\n",
      "Epoch 17/100\n",
      "537/537 [==============================] - 2s 4ms/sample - loss: 0.5842 - accuracy: 0.6816 - val_loss: 0.6271 - val_accuracy: 0.7013\n",
      "Epoch 18/100\n",
      "537/537 [==============================] - 2s 4ms/sample - loss: 0.5907 - accuracy: 0.7020 - val_loss: 0.6181 - val_accuracy: 0.6926\n",
      "Epoch 19/100\n",
      "537/537 [==============================] - 2s 4ms/sample - loss: 0.5999 - accuracy: 0.6778 - val_loss: 0.6184 - val_accuracy: 0.6883\n",
      "Epoch 20/100\n",
      "537/537 [==============================] - 2s 4ms/sample - loss: 0.5963 - accuracy: 0.6723 - val_loss: 0.6170 - val_accuracy: 0.7056\n",
      "Epoch 21/100\n",
      "537/537 [==============================] - 2s 4ms/sample - loss: 0.5684 - accuracy: 0.7114 - val_loss: 0.6647 - val_accuracy: 0.6450\n",
      "Epoch 22/100\n",
      "537/537 [==============================] - 2s 4ms/sample - loss: 0.6023 - accuracy: 0.6872 - val_loss: 0.6085 - val_accuracy: 0.7186\n",
      "Epoch 23/100\n",
      "537/537 [==============================] - 2s 4ms/sample - loss: 0.5855 - accuracy: 0.6704 - val_loss: 0.7295 - val_accuracy: 0.5887\n",
      "Epoch 24/100\n",
      "537/537 [==============================] - 2s 4ms/sample - loss: 0.5891 - accuracy: 0.7076 - val_loss: 0.6562 - val_accuracy: 0.6797\n",
      "Epoch 25/100\n",
      "537/537 [==============================] - 2s 4ms/sample - loss: 0.5757 - accuracy: 0.6890 - val_loss: 0.6510 - val_accuracy: 0.6970\n",
      "Epoch 26/100\n",
      "537/537 [==============================] - 2s 4ms/sample - loss: 0.5783 - accuracy: 0.6853 - val_loss: 0.6318 - val_accuracy: 0.6970\n",
      "Epoch 27/100\n",
      "537/537 [==============================] - 2s 4ms/sample - loss: 0.5753 - accuracy: 0.6946 - val_loss: 0.6249 - val_accuracy: 0.7013\n",
      "Epoch 28/100\n",
      "537/537 [==============================] - 2s 4ms/sample - loss: 0.5682 - accuracy: 0.6983 - val_loss: 0.6170 - val_accuracy: 0.7100\n",
      "Epoch 29/100\n",
      "537/537 [==============================] - 2s 4ms/sample - loss: 0.5747 - accuracy: 0.6983 - val_loss: 0.7274 - val_accuracy: 0.6883\n",
      "Epoch 30/100\n",
      "537/537 [==============================] - 2s 4ms/sample - loss: 0.5742 - accuracy: 0.6927 - val_loss: 0.6253 - val_accuracy: 0.6926\n",
      "Epoch 31/100\n",
      "537/537 [==============================] - 2s 4ms/sample - loss: 0.5830 - accuracy: 0.6778 - val_loss: 0.6084 - val_accuracy: 0.6883\n",
      "Epoch 32/100\n",
      "537/537 [==============================] - 2s 4ms/sample - loss: 0.5523 - accuracy: 0.7132 - val_loss: 0.6036 - val_accuracy: 0.6883\n",
      "Epoch 33/100\n",
      "537/537 [==============================] - 2s 4ms/sample - loss: 0.5680 - accuracy: 0.7151 - val_loss: 0.6185 - val_accuracy: 0.6797\n",
      "Epoch 34/100\n",
      "537/537 [==============================] - 2s 4ms/sample - loss: 0.5593 - accuracy: 0.6983 - val_loss: 0.6200 - val_accuracy: 0.6840\n",
      "Epoch 35/100\n",
      "537/537 [==============================] - 2s 4ms/sample - loss: 0.5625 - accuracy: 0.7058 - val_loss: 0.6404 - val_accuracy: 0.6623\n",
      "Epoch 36/100\n",
      "537/537 [==============================] - 2s 4ms/sample - loss: 0.5524 - accuracy: 0.6965 - val_loss: 0.6247 - val_accuracy: 0.6797\n",
      "Epoch 37/100\n",
      "537/537 [==============================] - 2s 4ms/sample - loss: 0.5552 - accuracy: 0.7132 - val_loss: 0.6466 - val_accuracy: 0.7056\n",
      "Epoch 38/100\n",
      "537/537 [==============================] - 2s 4ms/sample - loss: 0.5444 - accuracy: 0.7337 - val_loss: 0.6725 - val_accuracy: 0.6883\n",
      "Epoch 39/100\n",
      "537/537 [==============================] - 2s 4ms/sample - loss: 0.5424 - accuracy: 0.7169 - val_loss: 0.6521 - val_accuracy: 0.6580\n",
      "Epoch 40/100\n",
      "537/537 [==============================] - 2s 4ms/sample - loss: 0.5286 - accuracy: 0.7076 - val_loss: 0.6374 - val_accuracy: 0.7186\n",
      "Epoch 41/100\n",
      "537/537 [==============================] - 2s 4ms/sample - loss: 0.5405 - accuracy: 0.7356 - val_loss: 0.6223 - val_accuracy: 0.7056\n",
      "Epoch 42/100\n",
      "537/537 [==============================] - 2s 4ms/sample - loss: 0.5626 - accuracy: 0.6946 - val_loss: 0.6798 - val_accuracy: 0.7013\n",
      "Epoch 43/100\n",
      "537/537 [==============================] - 2s 5ms/sample - loss: 0.5536 - accuracy: 0.7020 - val_loss: 0.6103 - val_accuracy: 0.7143\n",
      "Epoch 44/100\n",
      "537/537 [==============================] - 2s 5ms/sample - loss: 0.5314 - accuracy: 0.7169 - val_loss: 0.6445 - val_accuracy: 0.6667\n",
      "Epoch 45/100\n",
      "537/537 [==============================] - 2s 5ms/sample - loss: 0.5371 - accuracy: 0.7263 - val_loss: 0.5811 - val_accuracy: 0.7316\n",
      "Epoch 46/100\n",
      "537/537 [==============================] - 3s 5ms/sample - loss: 0.5480 - accuracy: 0.7225 - val_loss: 0.6265 - val_accuracy: 0.6797\n",
      "Epoch 47/100\n",
      "537/537 [==============================] - 3s 5ms/sample - loss: 0.5170 - accuracy: 0.7300 - val_loss: 0.7442 - val_accuracy: 0.6147\n",
      "Epoch 48/100\n",
      "537/537 [==============================] - 3s 5ms/sample - loss: 0.5343 - accuracy: 0.7151 - val_loss: 0.6047 - val_accuracy: 0.7273\n",
      "Epoch 49/100\n",
      "537/537 [==============================] - 3s 5ms/sample - loss: 0.5408 - accuracy: 0.7076 - val_loss: 0.6454 - val_accuracy: 0.6970\n",
      "Epoch 50/100\n",
      "537/537 [==============================] - 3s 5ms/sample - loss: 0.5302 - accuracy: 0.7169 - val_loss: 0.6217 - val_accuracy: 0.7229\n",
      "Epoch 51/100\n",
      "537/537 [==============================] - 3s 5ms/sample - loss: 0.5206 - accuracy: 0.7449 - val_loss: 0.6301 - val_accuracy: 0.7273\n",
      "Epoch 52/100\n",
      "537/537 [==============================] - 3s 5ms/sample - loss: 0.5050 - accuracy: 0.7263 - val_loss: 0.6503 - val_accuracy: 0.6623\n",
      "Epoch 53/100\n",
      "537/537 [==============================] - 3s 6ms/sample - loss: 0.5200 - accuracy: 0.7337 - val_loss: 0.6281 - val_accuracy: 0.7229\n",
      "Epoch 54/100\n",
      "537/537 [==============================] - 3s 6ms/sample - loss: 0.5271 - accuracy: 0.7151 - val_loss: 0.7852 - val_accuracy: 0.5931\n",
      "Epoch 55/100\n",
      "537/537 [==============================] - 3s 6ms/sample - loss: 0.5179 - accuracy: 0.7225 - val_loss: 0.6946 - val_accuracy: 0.7100\n",
      "Epoch 56/100\n",
      "537/537 [==============================] - 3s 5ms/sample - loss: 0.5196 - accuracy: 0.7114 - val_loss: 0.6940 - val_accuracy: 0.6883\n",
      "Epoch 57/100\n",
      "537/537 [==============================] - 3s 6ms/sample - loss: 0.5143 - accuracy: 0.7523 - val_loss: 0.6185 - val_accuracy: 0.7186\n",
      "Epoch 58/100\n",
      "537/537 [==============================] - 3s 6ms/sample - loss: 0.5089 - accuracy: 0.7225 - val_loss: 0.6648 - val_accuracy: 0.6883\n",
      "Epoch 59/100\n",
      "537/537 [==============================] - 3s 6ms/sample - loss: 0.5031 - accuracy: 0.7579 - val_loss: 0.7420 - val_accuracy: 0.6580\n",
      "Epoch 60/100\n",
      "537/537 [==============================] - 3s 5ms/sample - loss: 0.4868 - accuracy: 0.7672 - val_loss: 0.6617 - val_accuracy: 0.7100\n",
      "Epoch 61/100\n",
      "537/537 [==============================] - 3s 6ms/sample - loss: 0.5000 - accuracy: 0.7169 - val_loss: 0.6703 - val_accuracy: 0.6667\n",
      "Epoch 62/100\n",
      "537/537 [==============================] - 3s 6ms/sample - loss: 0.5118 - accuracy: 0.7412 - val_loss: 0.6632 - val_accuracy: 0.6797\n",
      "Epoch 63/100\n",
      "537/537 [==============================] - 3s 6ms/sample - loss: 0.5147 - accuracy: 0.7076 - val_loss: 0.6240 - val_accuracy: 0.7446\n",
      "Epoch 64/100\n",
      "537/537 [==============================] - 3s 5ms/sample - loss: 0.4973 - accuracy: 0.7449 - val_loss: 0.6840 - val_accuracy: 0.7013\n",
      "Epoch 65/100\n",
      "537/537 [==============================] - 3s 6ms/sample - loss: 0.5047 - accuracy: 0.7374 - val_loss: 0.7006 - val_accuracy: 0.7143\n",
      "Epoch 66/100\n",
      "537/537 [==============================] - 3s 6ms/sample - loss: 0.5390 - accuracy: 0.7188 - val_loss: 0.7002 - val_accuracy: 0.7273\n",
      "Epoch 67/100\n",
      "537/537 [==============================] - 3s 6ms/sample - loss: 0.5135 - accuracy: 0.7263 - val_loss: 0.6353 - val_accuracy: 0.7143\n",
      "Epoch 68/100\n",
      "537/537 [==============================] - 3s 6ms/sample - loss: 0.4928 - accuracy: 0.7486 - val_loss: 0.7443 - val_accuracy: 0.6840\n",
      "Epoch 69/100\n",
      "537/537 [==============================] - 3s 5ms/sample - loss: 0.5166 - accuracy: 0.7225 - val_loss: 0.6705 - val_accuracy: 0.7100\n",
      "Epoch 70/100\n",
      "537/537 [==============================] - 3s 6ms/sample - loss: 0.4896 - accuracy: 0.7449 - val_loss: 0.6451 - val_accuracy: 0.7359\n",
      "Epoch 71/100\n",
      "537/537 [==============================] - 3s 6ms/sample - loss: 0.4878 - accuracy: 0.7505 - val_loss: 0.6902 - val_accuracy: 0.7143\n",
      "Epoch 72/100\n",
      "537/537 [==============================] - 4s 7ms/sample - loss: 0.4960 - accuracy: 0.7654 - val_loss: 0.6623 - val_accuracy: 0.6926\n",
      "Epoch 73/100\n",
      "537/537 [==============================] - 4s 8ms/sample - loss: 0.4896 - accuracy: 0.7393 - val_loss: 0.6802 - val_accuracy: 0.7186\n",
      "Epoch 74/100\n",
      "537/537 [==============================] - 4s 7ms/sample - loss: 0.4926 - accuracy: 0.7505 - val_loss: 0.6612 - val_accuracy: 0.7403\n",
      "Epoch 75/100\n",
      "537/537 [==============================] - 4s 7ms/sample - loss: 0.4723 - accuracy: 0.7467 - val_loss: 0.6818 - val_accuracy: 0.7186\n",
      "Epoch 76/100\n",
      "537/537 [==============================] - 3s 6ms/sample - loss: 0.4794 - accuracy: 0.7374 - val_loss: 0.7080 - val_accuracy: 0.7359\n",
      "Epoch 77/100\n",
      "537/537 [==============================] - 3s 5ms/sample - loss: 0.4962 - accuracy: 0.7523 - val_loss: 0.7189 - val_accuracy: 0.7273\n",
      "Epoch 78/100\n",
      "537/537 [==============================] - 3s 5ms/sample - loss: 0.4914 - accuracy: 0.7635 - val_loss: 0.7154 - val_accuracy: 0.7186\n",
      "Epoch 79/100\n",
      "537/537 [==============================] - 3s 5ms/sample - loss: 0.4817 - accuracy: 0.7356 - val_loss: 0.7139 - val_accuracy: 0.6797\n",
      "Epoch 80/100\n",
      "537/537 [==============================] - 3s 5ms/sample - loss: 0.5042 - accuracy: 0.7635 - val_loss: 0.6420 - val_accuracy: 0.7013\n",
      "Epoch 81/100\n",
      "537/537 [==============================] - 2s 5ms/sample - loss: 0.4614 - accuracy: 0.7709 - val_loss: 0.6402 - val_accuracy: 0.7359\n",
      "Epoch 82/100\n",
      "537/537 [==============================] - 2s 5ms/sample - loss: 0.4695 - accuracy: 0.7672 - val_loss: 0.6738 - val_accuracy: 0.7273\n",
      "Epoch 83/100\n",
      "537/537 [==============================] - 2s 4ms/sample - loss: 0.4815 - accuracy: 0.7709 - val_loss: 0.6260 - val_accuracy: 0.7403\n",
      "Epoch 84/100\n",
      "537/537 [==============================] - 2s 4ms/sample - loss: 0.4770 - accuracy: 0.7691 - val_loss: 0.6368 - val_accuracy: 0.7056\n",
      "Epoch 85/100\n",
      "537/537 [==============================] - 2s 5ms/sample - loss: 0.4630 - accuracy: 0.7747 - val_loss: 0.6992 - val_accuracy: 0.7100\n",
      "Epoch 86/100\n",
      "537/537 [==============================] - 2s 4ms/sample - loss: 0.5113 - accuracy: 0.7467 - val_loss: 0.6899 - val_accuracy: 0.7013\n",
      "Epoch 87/100\n",
      "537/537 [==============================] - 2s 4ms/sample - loss: 0.4861 - accuracy: 0.7561 - val_loss: 0.6528 - val_accuracy: 0.7186\n",
      "Epoch 88/100\n",
      "537/537 [==============================] - 2s 4ms/sample - loss: 0.4788 - accuracy: 0.7542 - val_loss: 0.6565 - val_accuracy: 0.7446\n",
      "Epoch 89/100\n",
      "537/537 [==============================] - 2s 4ms/sample - loss: 0.4892 - accuracy: 0.7169 - val_loss: 0.7024 - val_accuracy: 0.7056\n",
      "Epoch 90/100\n",
      "537/537 [==============================] - 2s 4ms/sample - loss: 0.4898 - accuracy: 0.7691 - val_loss: 0.6528 - val_accuracy: 0.7056\n",
      "Epoch 91/100\n",
      "537/537 [==============================] - 2s 4ms/sample - loss: 0.4704 - accuracy: 0.7616 - val_loss: 0.6537 - val_accuracy: 0.7403\n",
      "Epoch 92/100\n",
      "537/537 [==============================] - 2s 4ms/sample - loss: 0.4627 - accuracy: 0.7654 - val_loss: 0.7698 - val_accuracy: 0.6753\n",
      "Epoch 93/100\n",
      "537/537 [==============================] - 2s 4ms/sample - loss: 0.4857 - accuracy: 0.7505 - val_loss: 0.6699 - val_accuracy: 0.7273\n",
      "Epoch 94/100\n",
      "537/537 [==============================] - 2s 4ms/sample - loss: 0.4480 - accuracy: 0.7747 - val_loss: 0.7307 - val_accuracy: 0.6883\n",
      "Epoch 95/100\n",
      "537/537 [==============================] - 2s 4ms/sample - loss: 0.4622 - accuracy: 0.7672 - val_loss: 0.7257 - val_accuracy: 0.7273\n",
      "Epoch 96/100\n",
      "537/537 [==============================] - 2s 4ms/sample - loss: 0.4818 - accuracy: 0.7579 - val_loss: 0.7076 - val_accuracy: 0.7100\n",
      "Epoch 97/100\n",
      "537/537 [==============================] - 2s 4ms/sample - loss: 0.4595 - accuracy: 0.7728 - val_loss: 0.6556 - val_accuracy: 0.7403\n",
      "Epoch 98/100\n",
      "537/537 [==============================] - 2s 4ms/sample - loss: 0.4732 - accuracy: 0.7709 - val_loss: 0.7097 - val_accuracy: 0.7403\n",
      "Epoch 99/100\n",
      "537/537 [==============================] - 2s 4ms/sample - loss: 0.4544 - accuracy: 0.7654 - val_loss: 0.7049 - val_accuracy: 0.7316\n",
      "Epoch 100/100\n",
      "537/537 [==============================] - 2s 4ms/sample - loss: 0.4565 - accuracy: 0.7747 - val_loss: 0.7359 - val_accuracy: 0.7359\n",
      "231/231 [==============================] - 0s 225us/sample - loss: 0.7359 - accuracy: 0.7359\n",
      "231/231 [==============================] - 0s 225us/sample - loss: 0.7359 - accuracy: 0.7359\n",
      "\n",
      " Accuracy: 0.7359\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n",
    "\n",
    "#모델 실행\n",
    "model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=100, batch_size=10)\n",
    "\n",
    "#모델 평가\n",
    "model.evaluate(X_test, Y_test)\n",
    "print(\"\\n Accuracy: %.4f\" % model.evaluate(X_test, Y_test)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Accurcy: 0.7489*\n",
    "\n",
    "MLP, CNN, RNN 중에서 제일 accuracy가 높게 나왔습니다.\n",
    "\n",
    "RNN은 주로 sequential data에 많이 쓰인다고 알고 있는데, 숫자형 데이터를 다룰 때 이미지형 아키텍쳐인 CNN보다는 텍스트용 sequential 데이터용인 RNN 아키텍쳐가 더 적절하다는 것을 알 수 있었습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "#seed값 설정\n",
    "seed=0\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(3)\n",
    "\n",
    "#데이터 적용\n",
    "my_data = \"C:\\\\Users\\\\user\\\\Desktop\\\\dataset\\\\pima-indians-diabetes.csv\"\n",
    "dataset = np.loadtxt(my_data, delimiter=\",\")\n",
    "X = dataset[:, 0:8]\n",
    "Y = dataset[:, 8]\n",
    "\n",
    "#학습셋과 테스트셋의 구분\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN과 마찬가지로 데이터 reshaping에 있어서 mnist 데이터와 차원을 다르게 하였습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터를 reshaping \n",
    "#mnist 데이터와 다르다!\n",
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "X_test = X_test.reshape((X_test.shape[0], X_train.shape[1], 1)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM에서는 노드의 개수를 전반적으로 늘리고, dropout의 퍼센트를 줄였다가 점차 커지도록 하였습니다.\n",
    "\n",
    "데이터가 binary하므로 마지막 dense 레이어에서 노드를 1개로 설정하고, sigmoid를 사용하였습니다.\n",
    "또한 sparse categorical crossentropy 대신 binary_crossentropy를 사용하였습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#모델 설정\n",
    "model = Sequential()\n",
    "model.add(LSTM(400, input_shape=(X_train.shape[1], 1), activation='relu', return_sequences=True))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(LSTM(200, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(40, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(1, activation='sigmoid')) #출력층에서는 노드가 하나이므로 1을 넣고, 이항 분류이므로 sigmoid와 binary crossentropy 사용\n",
    "model.compile(loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "epoch를 100으로 늘리고 batch_size를 추가했습니다. epoch를 200으로 설정했을 때 accuracy가 100과 같아 100으로 설정하였습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_40\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_12 (LSTM)               (None, 8, 400)            643200    \n",
      "_________________________________________________________________\n",
      "dropout_48 (Dropout)         (None, 8, 400)            0         \n",
      "_________________________________________________________________\n",
      "lstm_13 (LSTM)               (None, 200)               480800    \n",
      "_________________________________________________________________\n",
      "dropout_49 (Dropout)         (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_82 (Dense)             (None, 40)                8040      \n",
      "_________________________________________________________________\n",
      "dropout_50 (Dropout)         (None, 40)                0         \n",
      "_________________________________________________________________\n",
      "dense_83 (Dense)             (None, 1)                 41        \n",
      "=================================================================\n",
      "Total params: 1,132,081\n",
      "Trainable params: 1,132,081\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 537 samples, validate on 231 samples\n",
      "Epoch 1/100\n",
      "537/537 [==============================] - 9s 17ms/sample - loss: 1.0099 - accuracy: 0.6052 - val_loss: 0.5992 - val_accuracy: 0.6840\n",
      "Epoch 2/100\n",
      "537/537 [==============================] - 5s 9ms/sample - loss: 0.6555 - accuracy: 0.6480 - val_loss: 0.6552 - val_accuracy: 0.6926\n",
      "Epoch 3/100\n",
      "537/537 [==============================] - 5s 9ms/sample - loss: 0.6429 - accuracy: 0.6536 - val_loss: 0.5714 - val_accuracy: 0.6883\n",
      "Epoch 4/100\n",
      "537/537 [==============================] - 4s 8ms/sample - loss: 0.6088 - accuracy: 0.6723 - val_loss: 0.5247 - val_accuracy: 0.7532\n",
      "Epoch 5/100\n",
      "537/537 [==============================] - 4s 8ms/sample - loss: 0.6246 - accuracy: 0.6778 - val_loss: 0.5279 - val_accuracy: 0.7229\n",
      "Epoch 6/100\n",
      "537/537 [==============================] - 4s 8ms/sample - loss: 0.6001 - accuracy: 0.7263 - val_loss: 0.5155 - val_accuracy: 0.7489\n",
      "Epoch 7/100\n",
      "537/537 [==============================] - 4s 8ms/sample - loss: 0.5826 - accuracy: 0.7281 - val_loss: 0.5008 - val_accuracy: 0.7316\n",
      "Epoch 8/100\n",
      "537/537 [==============================] - 4s 8ms/sample - loss: 0.5637 - accuracy: 0.7225 - val_loss: 0.4945 - val_accuracy: 0.7359\n",
      "Epoch 9/100\n",
      "537/537 [==============================] - 4s 8ms/sample - loss: 0.5658 - accuracy: 0.7151 - val_loss: 0.6642 - val_accuracy: 0.6450\n",
      "Epoch 10/100\n",
      "537/537 [==============================] - 5s 9ms/sample - loss: 0.5520 - accuracy: 0.7430 - val_loss: 0.5010 - val_accuracy: 0.7273\n",
      "Epoch 11/100\n",
      "537/537 [==============================] - 4s 7ms/sample - loss: 0.5573 - accuracy: 0.7318 - val_loss: 0.5302 - val_accuracy: 0.7576\n",
      "Epoch 12/100\n",
      "537/537 [==============================] - 4s 7ms/sample - loss: 0.5477 - accuracy: 0.7188 - val_loss: 0.5135 - val_accuracy: 0.7056\n",
      "Epoch 13/100\n",
      "537/537 [==============================] - 4s 8ms/sample - loss: 0.5297 - accuracy: 0.7393 - val_loss: 0.5901 - val_accuracy: 0.6970\n",
      "Epoch 14/100\n",
      "537/537 [==============================] - 5s 9ms/sample - loss: 0.5536 - accuracy: 0.7523 - val_loss: 0.4893 - val_accuracy: 0.7576\n",
      "Epoch 15/100\n",
      "537/537 [==============================] - 5s 9ms/sample - loss: 0.5396 - accuracy: 0.7244 - val_loss: 0.4893 - val_accuracy: 0.7489\n",
      "Epoch 16/100\n",
      "537/537 [==============================] - 4s 7ms/sample - loss: 0.5278 - accuracy: 0.7467 - val_loss: 0.5266 - val_accuracy: 0.7576\n",
      "Epoch 17/100\n",
      "537/537 [==============================] - 4s 8ms/sample - loss: 0.5365 - accuracy: 0.7598 - val_loss: 0.5105 - val_accuracy: 0.7619\n",
      "Epoch 18/100\n",
      "537/537 [==============================] - 4s 8ms/sample - loss: 0.5366 - accuracy: 0.7505 - val_loss: 0.5347 - val_accuracy: 0.7186\n",
      "Epoch 19/100\n",
      "537/537 [==============================] - 4s 7ms/sample - loss: 0.5172 - accuracy: 0.7598 - val_loss: 0.6228 - val_accuracy: 0.7186\n",
      "Epoch 20/100\n",
      "537/537 [==============================] - 4s 7ms/sample - loss: 0.5102 - accuracy: 0.7523 - val_loss: 0.5495 - val_accuracy: 0.7273\n",
      "Epoch 21/100\n",
      "537/537 [==============================] - 4s 7ms/sample - loss: 0.5017 - accuracy: 0.7467 - val_loss: 0.4885 - val_accuracy: 0.7446\n",
      "Epoch 22/100\n",
      "537/537 [==============================] - 4s 7ms/sample - loss: 0.5082 - accuracy: 0.7728 - val_loss: 0.5081 - val_accuracy: 0.7576\n",
      "Epoch 23/100\n",
      "537/537 [==============================] - 4s 7ms/sample - loss: 0.5114 - accuracy: 0.7598 - val_loss: 0.5282 - val_accuracy: 0.7489\n",
      "Epoch 24/100\n",
      "537/537 [==============================] - 4s 7ms/sample - loss: 0.4997 - accuracy: 0.7430 - val_loss: 0.5010 - val_accuracy: 0.7316\n",
      "Epoch 25/100\n",
      "537/537 [==============================] - 4s 7ms/sample - loss: 0.5138 - accuracy: 0.7505 - val_loss: 0.4880 - val_accuracy: 0.7662\n",
      "Epoch 26/100\n",
      "537/537 [==============================] - 4s 7ms/sample - loss: 0.4813 - accuracy: 0.7709 - val_loss: 0.5933 - val_accuracy: 0.7532\n",
      "Epoch 27/100\n",
      "537/537 [==============================] - 4s 7ms/sample - loss: 0.5121 - accuracy: 0.7672 - val_loss: 0.4741 - val_accuracy: 0.7489\n",
      "Epoch 28/100\n",
      "537/537 [==============================] - 4s 7ms/sample - loss: 0.4896 - accuracy: 0.7821 - val_loss: 0.5951 - val_accuracy: 0.7186\n",
      "Epoch 29/100\n",
      "537/537 [==============================] - 4s 8ms/sample - loss: 0.5004 - accuracy: 0.7747 - val_loss: 0.8156 - val_accuracy: 0.7229\n",
      "Epoch 30/100\n",
      "537/537 [==============================] - 4s 7ms/sample - loss: 0.4863 - accuracy: 0.7654 - val_loss: 0.4927 - val_accuracy: 0.7662\n",
      "Epoch 31/100\n",
      "537/537 [==============================] - 4s 7ms/sample - loss: 0.5133 - accuracy: 0.7561 - val_loss: 0.4712 - val_accuracy: 0.7532\n",
      "Epoch 32/100\n",
      "537/537 [==============================] - 4s 7ms/sample - loss: 0.5024 - accuracy: 0.7747 - val_loss: 0.4786 - val_accuracy: 0.7489\n",
      "Epoch 33/100\n",
      "537/537 [==============================] - 4s 7ms/sample - loss: 0.4962 - accuracy: 0.7542 - val_loss: 0.4881 - val_accuracy: 0.7359\n",
      "Epoch 34/100\n",
      "537/537 [==============================] - 4s 7ms/sample - loss: 0.5463 - accuracy: 0.7747 - val_loss: 0.4922 - val_accuracy: 0.7359\n",
      "Epoch 35/100\n",
      "537/537 [==============================] - 4s 7ms/sample - loss: 0.4800 - accuracy: 0.7579 - val_loss: 0.8076 - val_accuracy: 0.7532\n",
      "Epoch 36/100\n",
      "537/537 [==============================] - 4s 7ms/sample - loss: 0.8048 - accuracy: 0.7523 - val_loss: 0.4963 - val_accuracy: 0.7403\n",
      "Epoch 37/100\n",
      "537/537 [==============================] - 4s 7ms/sample - loss: 0.5004 - accuracy: 0.7896 - val_loss: 0.5109 - val_accuracy: 0.7532\n",
      "Epoch 38/100\n",
      "537/537 [==============================] - 4s 7ms/sample - loss: 0.5184 - accuracy: 0.7821 - val_loss: 0.5319 - val_accuracy: 0.7489\n",
      "Epoch 39/100\n",
      "537/537 [==============================] - 4s 7ms/sample - loss: 0.5043 - accuracy: 0.7784 - val_loss: 0.4842 - val_accuracy: 0.7619\n",
      "Epoch 40/100\n",
      "537/537 [==============================] - 4s 7ms/sample - loss: 0.4872 - accuracy: 0.7561 - val_loss: 0.5690 - val_accuracy: 0.7489\n",
      "Epoch 41/100\n",
      "537/537 [==============================] - 4s 7ms/sample - loss: 0.4999 - accuracy: 0.7747 - val_loss: 0.4921 - val_accuracy: 0.7489\n",
      "Epoch 42/100\n",
      "537/537 [==============================] - 4s 7ms/sample - loss: 0.4722 - accuracy: 0.7691 - val_loss: 0.4938 - val_accuracy: 0.7662s: 0.4720 - accuracy: \n",
      "Epoch 43/100\n",
      "537/537 [==============================] - 3s 6ms/sample - loss: 0.4855 - accuracy: 0.7784 - val_loss: 0.4825 - val_accuracy: 0.7489\n",
      "Epoch 44/100\n",
      "537/537 [==============================] - 4s 7ms/sample - loss: 0.6017 - accuracy: 0.7784 - val_loss: 0.5063 - val_accuracy: 0.73165 - accuracy\n",
      "Epoch 45/100\n",
      "537/537 [==============================] - 4s 7ms/sample - loss: 0.4827 - accuracy: 0.7523 - val_loss: 0.5272 - val_accuracy: 0.7446\n",
      "Epoch 46/100\n",
      "537/537 [==============================] - 4s 7ms/sample - loss: 0.4892 - accuracy: 0.7672 - val_loss: 0.5850 - val_accuracy: 0.7403\n",
      "Epoch 47/100\n",
      "537/537 [==============================] - 4s 7ms/sample - loss: 0.4882 - accuracy: 0.7821 - val_loss: 1.5065 - val_accuracy: 0.7446\n",
      "Epoch 48/100\n",
      "537/537 [==============================] - 4s 7ms/sample - loss: 0.5346 - accuracy: 0.7691 - val_loss: 0.4743 - val_accuracy: 0.7619\n",
      "Epoch 49/100\n",
      "537/537 [==============================] - 4s 7ms/sample - loss: 0.5040 - accuracy: 0.7914 - val_loss: 0.4840 - val_accuracy: 0.7706\n",
      "Epoch 50/100\n",
      "537/537 [==============================] - 4s 7ms/sample - loss: 0.6153 - accuracy: 0.7914 - val_loss: 1.3008 - val_accuracy: 0.7359\n",
      "Epoch 51/100\n",
      "537/537 [==============================] - 4s 7ms/sample - loss: 0.4697 - accuracy: 0.7765 - val_loss: 0.4962 - val_accuracy: 0.7489\n",
      "Epoch 52/100\n",
      "537/537 [==============================] - 5s 9ms/sample - loss: 4.9374 - accuracy: 0.8045 - val_loss: 0.6404 - val_accuracy: 0.7403\n",
      "Epoch 53/100\n",
      "537/537 [==============================] - 5s 9ms/sample - loss: 0.5530 - accuracy: 0.7933 - val_loss: 0.5273 - val_accuracy: 0.7662\n",
      "Epoch 54/100\n",
      "537/537 [==============================] - 5s 9ms/sample - loss: 0.4710 - accuracy: 0.7579 - val_loss: 0.5154 - val_accuracy: 0.7576\n",
      "Epoch 55/100\n",
      "537/537 [==============================] - 5s 10ms/sample - loss: 0.5168 - accuracy: 0.7933 - val_loss: 0.5074 - val_accuracy: 0.7489\n",
      "Epoch 56/100\n",
      "537/537 [==============================] - 5s 10ms/sample - loss: 0.4758 - accuracy: 0.7914 - val_loss: 0.5208 - val_accuracy: 0.7619\n",
      "Epoch 57/100\n",
      "537/537 [==============================] - 5s 10ms/sample - loss: 0.4887 - accuracy: 0.7896 - val_loss: 0.4721 - val_accuracy: 0.7619\n",
      "Epoch 58/100\n",
      "537/537 [==============================] - 5s 9ms/sample - loss: 0.5878 - accuracy: 0.7654 - val_loss: 0.4665 - val_accuracy: 0.7489\n",
      "Epoch 59/100\n",
      "537/537 [==============================] - 5s 8ms/sample - loss: 0.4812 - accuracy: 0.7616 - val_loss: 0.5921 - val_accuracy: 0.7446\n",
      "Epoch 60/100\n",
      "537/537 [==============================] - 5s 9ms/sample - loss: 0.4877 - accuracy: 0.7765 - val_loss: 0.6366 - val_accuracy: 0.7403\n",
      "Epoch 61/100\n",
      "537/537 [==============================] - 5s 9ms/sample - loss: 0.4754 - accuracy: 0.7914 - val_loss: 0.5462 - val_accuracy: 0.7229\n",
      "Epoch 62/100\n",
      "537/537 [==============================] - 4s 8ms/sample - loss: 0.7308 - accuracy: 0.7821 - val_loss: 0.4733 - val_accuracy: 0.7229\n",
      "Epoch 63/100\n",
      "537/537 [==============================] - 4s 8ms/sample - loss: 0.4669 - accuracy: 0.7691 - val_loss: 0.5008 - val_accuracy: 0.7489\n",
      "Epoch 64/100\n",
      "537/537 [==============================] - 4s 8ms/sample - loss: 0.6165 - accuracy: 0.7896 - val_loss: 0.5196 - val_accuracy: 0.7489\n",
      "Epoch 65/100\n",
      "537/537 [==============================] - 4s 7ms/sample - loss: 0.4683 - accuracy: 0.7877 - val_loss: 0.7280 - val_accuracy: 0.7359\n",
      "Epoch 66/100\n",
      "537/537 [==============================] - 4s 7ms/sample - loss: 0.4920 - accuracy: 0.7858 - val_loss: 0.5104 - val_accuracy: 0.7403\n",
      "Epoch 67/100\n",
      "537/537 [==============================] - 4s 7ms/sample - loss: 0.4911 - accuracy: 0.7672 - val_loss: 0.6434 - val_accuracy: 0.7403\n",
      "Epoch 68/100\n",
      "537/537 [==============================] - 4s 7ms/sample - loss: 0.4697 - accuracy: 0.7914 - val_loss: 8.3352 - val_accuracy: 0.7316\n",
      "Epoch 69/100\n",
      "537/537 [==============================] - 4s 7ms/sample - loss: 0.5348 - accuracy: 0.7821 - val_loss: 0.6736 - val_accuracy: 0.7316\n",
      "Epoch 70/100\n",
      "537/537 [==============================] - 4s 7ms/sample - loss: 0.5474 - accuracy: 0.7728 - val_loss: 0.4627 - val_accuracy: 0.7489\n",
      "Epoch 71/100\n",
      "537/537 [==============================] - 4s 7ms/sample - loss: 0.4515 - accuracy: 0.7858 - val_loss: 0.8899 - val_accuracy: 0.7229\n",
      "Epoch 72/100\n",
      "537/537 [==============================] - 4s 7ms/sample - loss: 1.2432 - accuracy: 0.7691 - val_loss: 0.4771 - val_accuracy: 0.7532\n",
      "Epoch 73/100\n",
      "537/537 [==============================] - 4s 7ms/sample - loss: 0.4679 - accuracy: 0.7765 - val_loss: 0.6201 - val_accuracy: 0.7489\n",
      "Epoch 74/100\n",
      "537/537 [==============================] - 4s 7ms/sample - loss: 0.4568 - accuracy: 0.7691 - val_loss: 0.7388 - val_accuracy: 0.7619\n",
      "Epoch 75/100\n",
      "537/537 [==============================] - 3s 6ms/sample - loss: 0.5133 - accuracy: 0.7635 - val_loss: 0.6370 - val_accuracy: 0.7446\n",
      "Epoch 76/100\n",
      "537/537 [==============================] - 3s 6ms/sample - loss: 0.4655 - accuracy: 0.7784 - val_loss: 0.5297 - val_accuracy: 0.7576\n",
      "Epoch 77/100\n",
      "537/537 [==============================] - 3s 6ms/sample - loss: 0.4991 - accuracy: 0.7728 - val_loss: 0.4748 - val_accuracy: 0.7359\n",
      "Epoch 78/100\n",
      "537/537 [==============================] - 4s 7ms/sample - loss: 0.4944 - accuracy: 0.7821 - val_loss: 0.5179 - val_accuracy: 0.7229\n",
      "Epoch 79/100\n",
      "537/537 [==============================] - 4s 7ms/sample - loss: 0.4898 - accuracy: 0.7877 - val_loss: 0.5337 - val_accuracy: 0.7489\n",
      "Epoch 80/100\n",
      "537/537 [==============================] - 4s 7ms/sample - loss: 0.4753 - accuracy: 0.7784 - val_loss: 0.5080 - val_accuracy: 0.7576\n",
      "Epoch 81/100\n",
      "537/537 [==============================] - 4s 7ms/sample - loss: 0.6831 - accuracy: 0.7747 - val_loss: 0.4726 - val_accuracy: 0.7576\n",
      "Epoch 82/100\n",
      "537/537 [==============================] - 4s 7ms/sample - loss: 0.4928 - accuracy: 0.7821 - val_loss: 0.7335 - val_accuracy: 0.7403\n",
      "Epoch 83/100\n",
      "537/537 [==============================] - 4s 7ms/sample - loss: 0.4692 - accuracy: 0.7933 - val_loss: 0.5647 - val_accuracy: 0.7619\n",
      "Epoch 84/100\n",
      "537/537 [==============================] - 4s 7ms/sample - loss: 0.6203 - accuracy: 0.7840 - val_loss: 0.5764 - val_accuracy: 0.7662\n",
      "Epoch 85/100\n",
      "537/537 [==============================] - 4s 7ms/sample - loss: 0.4438 - accuracy: 0.7933 - val_loss: 1.0617 - val_accuracy: 0.7446\n",
      "Epoch 86/100\n",
      "537/537 [==============================] - 4s 7ms/sample - loss: 0.4784 - accuracy: 0.7896 - val_loss: 0.5171 - val_accuracy: 0.7706\n",
      "Epoch 87/100\n",
      "537/537 [==============================] - 4s 7ms/sample - loss: 0.4669 - accuracy: 0.7840 - val_loss: 1.9491 - val_accuracy: 0.7229\n",
      "Epoch 88/100\n",
      "537/537 [==============================] - 4s 7ms/sample - loss: 0.5087 - accuracy: 0.8007 - val_loss: 0.5136 - val_accuracy: 0.7532\n",
      "Epoch 89/100\n",
      "537/537 [==============================] - 4s 7ms/sample - loss: 0.5086 - accuracy: 0.7821 - val_loss: 0.5971 - val_accuracy: 0.6926\n",
      "Epoch 90/100\n",
      "537/537 [==============================] - 4s 7ms/sample - loss: 0.5827 - accuracy: 0.7765 - val_loss: 0.5519 - val_accuracy: 0.7273\n",
      "Epoch 91/100\n",
      "537/537 [==============================] - 4s 7ms/sample - loss: 0.4547 - accuracy: 0.8007 - val_loss: 0.5545 - val_accuracy: 0.7532\n",
      "Epoch 92/100\n",
      "537/537 [==============================] - 4s 7ms/sample - loss: 0.4833 - accuracy: 0.7877 - val_loss: 0.4677 - val_accuracy: 0.7489\n",
      "Epoch 93/100\n",
      "537/537 [==============================] - 4s 8ms/sample - loss: 0.4895 - accuracy: 0.7989 - val_loss: 0.5359 - val_accuracy: 0.7662\n",
      "Epoch 94/100\n",
      "537/537 [==============================] - 4s 7ms/sample - loss: 0.5137 - accuracy: 0.7784 - val_loss: 0.8463 - val_accuracy: 0.7446\n",
      "Epoch 95/100\n",
      "537/537 [==============================] - 4s 7ms/sample - loss: 0.4765 - accuracy: 0.7765 - val_loss: 0.5525 - val_accuracy: 0.7532\n",
      "Epoch 96/100\n",
      "537/537 [==============================] - 4s 7ms/sample - loss: 3.1816 - accuracy: 0.7896 - val_loss: 0.4843 - val_accuracy: 0.7619\n",
      "Epoch 97/100\n",
      "537/537 [==============================] - 4s 7ms/sample - loss: 0.4434 - accuracy: 0.7970 - val_loss: 0.4901 - val_accuracy: 0.7489\n",
      "Epoch 98/100\n",
      "537/537 [==============================] - 4s 8ms/sample - loss: 0.4514 - accuracy: 0.7989 - val_loss: 0.6362 - val_accuracy: 0.7446\n",
      "Epoch 99/100\n",
      "537/537 [==============================] - 5s 9ms/sample - loss: 0.4888 - accuracy: 0.7877 - val_loss: 0.5502 - val_accuracy: 0.7446\n",
      "Epoch 100/100\n",
      "537/537 [==============================] - 5s 9ms/sample - loss: 0.8632 - accuracy: 0.7803 - val_loss: 0.5159 - val_accuracy: 0.7489\n",
      "231/231 [==============================] - 0s 1ms/sample - loss: 0.5159 - accuracy: 0.7489\n",
      "231/231 [==============================] - 0s 1ms/sample - loss: 0.5159 - accuracy: 0.7489\n",
      "\n",
      " Accuracy: 0.7489\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n",
    "\n",
    "#모델 실행\n",
    "model.fit(X_train, Y_train, validation_data = (X_test, Y_test), epochs=100, batch_size=10)\n",
    "\n",
    "#모델 평가\n",
    "model.evaluate(X_test, Y_test)\n",
    "print(\"\\n Accuracy: %.4f\" % model.evaluate(X_test, Y_test)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
