{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "What_is_NLP_Ch14_SubwordTokenizer.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvWOCoSowbqX"
      },
      "source": [
        "## 14. 서브워드 토크나이저(Subword Tokenizer)\n",
        "* 단어 토큰화에서 더 나아가 단어를 서브워드 단위까지 나누는 서브워드 토큰화를 하는 이유\n",
        "* 서브워드 토큰화를 수행하는 서브워드 토크나이저의 알고리즘과 그 구현체\n",
        "\n",
        "### 01) 바이트페어 인코딩(Byte Pair Encoding, BPE)\n",
        "* 기계가 모르는 단어: OOV(Out of Vocabulary) 또는 UNK(Unknown Token)\n",
        "* **OOV 문제**: 모르는 단어로 인해 문제를 푸는 것이 까다로워지는 상황\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LrELfqoKF9ah"
      },
      "source": [
        "* **서브워드 분리(Subword segmentation)**: 하나의 단어는 더 작은 단위의 의미있는 여러 서브워드들의 조합으로 구성된 경우가 많기 때문에, 하나의 단어를 여러 서브워드로 분리해서 단어를 인코딩 및 임베딩하겠다는 의도를 가진 전처리 작업\n",
        "  * OOV, 희귀 단어, 신조어와 같은 문제를 완화 가능\n",
        "* **BPE(Byte Pair Encoding)**: OOV 문제를 완화하는 대표적인 서브워드 분리 알고리즘\n",
        "* **Sentencepiece**: 이 알고리즘을 실무에서 사용할 수 있도록 구현\n",
        "\n",
        "#### 1. BPE\n",
        "* 글자 대신 byte라는 표현 사용\n",
        "#### 2. 자연어처리에서의 BPE\n",
        "* 자연어 처리에서의 BPE는 서브워드 분리(Subword segmentation) 알고리즘\n",
        "* 글자(character) 단위에서 점차적으로 단어 집합(vocabulary)를 만들어 내는 Bottom up 방식의 접근\n",
        "* 훈련 데이터에 있는 단어들을 모든 글자(characters) 또는 유니코드(unicode) 단위로 단어 집합(vocabulary)를 만들고, 가장 많이 등장하는 유니그램을 하나의 유니그램으로 통합\n",
        "\n",
        "##### 1) 기존의 접근\n",
        "* 학습하지 않은 단어가 나오면 해당 단어에 대해서 제대로 대응하지 못하는 OOV 문제가 발생\n",
        "\n",
        "##### 2) BPE 알고리즘을 사용한 경우\n",
        "* 우선 딕셔너리의 모든 단어들을 글자(character) 단위로 분리\n",
        "  * 초기 구성은 글자 단위로 분리된 상태\n",
        "* 가장 빈도수가 높은 유니그램의 쌍을 하나의 유니그램으로 통합하는 과정을 총 n번 반복\n",
        "* BPE에서 'lowest'가 등장하면...\n",
        "  1. 기계는 우선 lowest를 전부 글자 단위로 분할\n",
        "  2. 위의 단어 집합을 참고로 하여 low와 est를 찾아냄. 즉, lowest를 기계는 low와 est 두 단어로 인코딩함\n",
        "\n",
        "\n",
        "##### 3) 코드 실습하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdbuK044F8qK"
      },
      "source": [
        "import re, collections\n",
        "from IPython.display import display, Markdown, Latex"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqSZvPHbwXXO"
      },
      "source": [
        "num_merges = 10 #BPE를 몇 회 수행할 것인지"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBVzEeHUJE-S"
      },
      "source": [
        "# low, lower, newest, widest일 때, \n",
        "# BPE의 입력으로 사용하는 실제 단어 집합은 아래와 같음\n",
        "\n",
        "dictionary = {'l o w </w>' : 5,\n",
        "         'l o w e r </w>' : 2,\n",
        "         'n e w e s t </w>':6,\n",
        "         'w i d e s t </w>':3\n",
        "         }"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 951
        },
        "id": "jA2aB8mPJT1f",
        "outputId": "10d83662-b8e2-4cf9-cd65-45b6eba85bf6"
      },
      "source": [
        "# 가장 빈도수가 높은 유니그램의 쌍을\n",
        "# 하나의 유니그램으로 통합하는 과정을 num_merges회 반복\n",
        "\n",
        "def get_stats(dictionary):\n",
        "    # 유니그램의 pair들의 빈도수를 카운트\n",
        "    pairs = collections.defaultdict(int)\n",
        "    for word, freq in dictionary.items():\n",
        "        symbols = word.split()\n",
        "        for i in range(len(symbols)-1):\n",
        "            pairs[symbols[i],symbols[i+1]] += freq\n",
        "    print('현재 pair들의 빈도수 :', dict(pairs))\n",
        "    return pairs\n",
        "\n",
        "def merge_dictionary(pair, v_in):\n",
        "    v_out = {}\n",
        "    bigram = re.escape(' '.join(pair))\n",
        "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
        "    for word in v_in:\n",
        "        w_out = p.sub(''.join(pair), word)\n",
        "        v_out[w_out] = v_in[word]\n",
        "    return v_out\n",
        "\n",
        "bpe_codes = {}\n",
        "bpe_codes_reverse = {}\n",
        "\n",
        "for i in range(num_merges):\n",
        "    display(Markdown(\"### Iteration {}\".format(i + 1)))\n",
        "    pairs = get_stats(dictionary)\n",
        "    best = max(pairs, key=pairs.get)\n",
        "    dictionary = merge_dictionary(best, dictionary)\n",
        "\n",
        "    bpe_codes[best] = i\n",
        "    bpe_codes_reverse[best[0] + best[1]] = best\n",
        "\n",
        "    print(\"new merge: {}\".format(best))\n",
        "    print(\"dictionary: {}\".format(dictionary))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "### Iteration 1",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "현재 pair들의 빈도수 : {('l', 'o'): 7, ('o', 'w'): 7, ('w', '</w>'): 5, ('w', 'e'): 8, ('e', 'r'): 2, ('r', '</w>'): 2, ('n', 'e'): 6, ('e', 'w'): 6, ('e', 's'): 9, ('s', 't'): 9, ('t', '</w>'): 9, ('w', 'i'): 3, ('i', 'd'): 3, ('d', 'e'): 3}\n",
            "new merge: ('e', 's')\n",
            "dictionary: {'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w es t </w>': 6, 'w i d es t </w>': 3}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "### Iteration 2",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "현재 pair들의 빈도수 : {('l', 'o'): 7, ('o', 'w'): 7, ('w', '</w>'): 5, ('w', 'e'): 2, ('e', 'r'): 2, ('r', '</w>'): 2, ('n', 'e'): 6, ('e', 'w'): 6, ('w', 'es'): 6, ('es', 't'): 9, ('t', '</w>'): 9, ('w', 'i'): 3, ('i', 'd'): 3, ('d', 'es'): 3}\n",
            "new merge: ('es', 't')\n",
            "dictionary: {'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w est </w>': 6, 'w i d est </w>': 3}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "### Iteration 3",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "현재 pair들의 빈도수 : {('l', 'o'): 7, ('o', 'w'): 7, ('w', '</w>'): 5, ('w', 'e'): 2, ('e', 'r'): 2, ('r', '</w>'): 2, ('n', 'e'): 6, ('e', 'w'): 6, ('w', 'est'): 6, ('est', '</w>'): 9, ('w', 'i'): 3, ('i', 'd'): 3, ('d', 'est'): 3}\n",
            "new merge: ('est', '</w>')\n",
            "dictionary: {'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w est</w>': 6, 'w i d est</w>': 3}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "### Iteration 4",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "현재 pair들의 빈도수 : {('l', 'o'): 7, ('o', 'w'): 7, ('w', '</w>'): 5, ('w', 'e'): 2, ('e', 'r'): 2, ('r', '</w>'): 2, ('n', 'e'): 6, ('e', 'w'): 6, ('w', 'est</w>'): 6, ('w', 'i'): 3, ('i', 'd'): 3, ('d', 'est</w>'): 3}\n",
            "new merge: ('l', 'o')\n",
            "dictionary: {'lo w </w>': 5, 'lo w e r </w>': 2, 'n e w est</w>': 6, 'w i d est</w>': 3}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "### Iteration 5",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "현재 pair들의 빈도수 : {('lo', 'w'): 7, ('w', '</w>'): 5, ('w', 'e'): 2, ('e', 'r'): 2, ('r', '</w>'): 2, ('n', 'e'): 6, ('e', 'w'): 6, ('w', 'est</w>'): 6, ('w', 'i'): 3, ('i', 'd'): 3, ('d', 'est</w>'): 3}\n",
            "new merge: ('lo', 'w')\n",
            "dictionary: {'low </w>': 5, 'low e r </w>': 2, 'n e w est</w>': 6, 'w i d est</w>': 3}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "### Iteration 6",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "현재 pair들의 빈도수 : {('low', '</w>'): 5, ('low', 'e'): 2, ('e', 'r'): 2, ('r', '</w>'): 2, ('n', 'e'): 6, ('e', 'w'): 6, ('w', 'est</w>'): 6, ('w', 'i'): 3, ('i', 'd'): 3, ('d', 'est</w>'): 3}\n",
            "new merge: ('n', 'e')\n",
            "dictionary: {'low </w>': 5, 'low e r </w>': 2, 'ne w est</w>': 6, 'w i d est</w>': 3}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "### Iteration 7",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "현재 pair들의 빈도수 : {('low', '</w>'): 5, ('low', 'e'): 2, ('e', 'r'): 2, ('r', '</w>'): 2, ('ne', 'w'): 6, ('w', 'est</w>'): 6, ('w', 'i'): 3, ('i', 'd'): 3, ('d', 'est</w>'): 3}\n",
            "new merge: ('ne', 'w')\n",
            "dictionary: {'low </w>': 5, 'low e r </w>': 2, 'new est</w>': 6, 'w i d est</w>': 3}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "### Iteration 8",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "현재 pair들의 빈도수 : {('low', '</w>'): 5, ('low', 'e'): 2, ('e', 'r'): 2, ('r', '</w>'): 2, ('new', 'est</w>'): 6, ('w', 'i'): 3, ('i', 'd'): 3, ('d', 'est</w>'): 3}\n",
            "new merge: ('new', 'est</w>')\n",
            "dictionary: {'low </w>': 5, 'low e r </w>': 2, 'newest</w>': 6, 'w i d est</w>': 3}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "### Iteration 9",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "현재 pair들의 빈도수 : {('low', '</w>'): 5, ('low', 'e'): 2, ('e', 'r'): 2, ('r', '</w>'): 2, ('w', 'i'): 3, ('i', 'd'): 3, ('d', 'est</w>'): 3}\n",
            "new merge: ('low', '</w>')\n",
            "dictionary: {'low</w>': 5, 'low e r </w>': 2, 'newest</w>': 6, 'w i d est</w>': 3}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "### Iteration 10",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "현재 pair들의 빈도수 : {('low', 'e'): 2, ('e', 'r'): 2, ('r', '</w>'): 2, ('w', 'i'): 3, ('i', 'd'): 3, ('d', 'est</w>'): 3}\n",
            "new merge: ('w', 'i')\n",
            "dictionary: {'low</w>': 5, 'low e r </w>': 2, 'newest</w>': 6, 'wi d est</w>': 3}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1prEjGp4J7j3",
        "outputId": "3c258653-6c61-45bb-d5b8-0c647ea2f074"
      },
      "source": [
        "print(bpe_codes) #merge했던 기록이 출력"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{('e', 's'): 0, ('es', 't'): 1, ('est', '</w>'): 2, ('l', 'o'): 3, ('lo', 'w'): 4, ('n', 'e'): 5, ('ne', 'w'): 6, ('new', 'est</w>'): 7, ('low', '</w>'): 8, ('w', 'i'): 9}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCxO-NNIKJx0"
      },
      "source": [
        "##### 4) OOV에 대처하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1jtVrxsKIKZ"
      },
      "source": [
        "def get_pairs(word):\n",
        "    \"\"\"Return set of symbol pairs in a word.\n",
        "    Word is represented as a tuple of symbols (symbols being variable-length strings).\n",
        "    \"\"\"\n",
        "    pairs = set()\n",
        "    prev_char = word[0]\n",
        "    for char in word[1:]:\n",
        "        pairs.add((prev_char, char))\n",
        "        prev_char = char\n",
        "    return pairs\n",
        "\n",
        "\n",
        "def encode(orig):\n",
        "    \"\"\"Encode word based on list of BPE merge operations, which are applied consecutively\"\"\"\n",
        "\n",
        "    word = tuple(orig) + ('</w>',)\n",
        "    display(Markdown(\"__word split into characters:__ <tt>{}</tt>\".format(word)))\n",
        "\n",
        "    pairs = get_pairs(word)    \n",
        "\n",
        "    if not pairs:\n",
        "        return orig\n",
        "\n",
        "    iteration = 0\n",
        "    while True:\n",
        "        iteration += 1\n",
        "        display(Markdown(\"__Iteration {}:__\".format(iteration)))\n",
        "\n",
        "        print(\"bigrams in the word: {}\".format(pairs))\n",
        "        bigram = min(pairs, key = lambda pair: bpe_codes.get(pair, float('inf')))\n",
        "        print(\"candidate for merging: {}\".format(bigram))\n",
        "        if bigram not in bpe_codes:\n",
        "            display(Markdown(\"__Candidate not in BPE merges, algorithm stops.__\"))\n",
        "            break\n",
        "        first, second = bigram\n",
        "        new_word = []\n",
        "        i = 0\n",
        "        while i < len(word):\n",
        "            try:\n",
        "                j = word.index(first, i)\n",
        "                new_word.extend(word[i:j])\n",
        "                i = j\n",
        "            except:\n",
        "                new_word.extend(word[i:])\n",
        "                break\n",
        "\n",
        "            if word[i] == first and i < len(word)-1 and word[i+1] == second:\n",
        "                new_word.append(first+second)\n",
        "                i += 2\n",
        "            else:\n",
        "                new_word.append(word[i])\n",
        "                i += 1\n",
        "        new_word = tuple(new_word)\n",
        "        word = new_word\n",
        "        print(\"word after merging: {}\".format(word))\n",
        "        if len(word) == 1:\n",
        "            break\n",
        "        else:\n",
        "            pairs = get_pairs(word)\n",
        "\n",
        "    # 특별 토큰인 </w>는 출력하지 않는다.\n",
        "    if word[-1] == '</w>':\n",
        "        word = word[:-1]\n",
        "    elif word[-1].endswith('</w>'):\n",
        "        word = word[:-1] + (word[-1].replace('</w>',''),)\n",
        "\n",
        "    return word"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "id": "xPZAdTCVKQL0",
        "outputId": "f53b1c16-2ed0-41a1-f4b7-be71329fb685"
      },
      "source": [
        "encode(\"loki\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "__word split into characters:__ <tt>('l', 'o', 'k', 'i', '</w>')</tt>",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "__Iteration 1:__",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bigrams in the word: {('i', '</w>'), ('l', 'o'), ('o', 'k'), ('k', 'i')}\n",
            "candidate for merging: ('l', 'o')\n",
            "word after merging: ('lo', 'k', 'i', '</w>')\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "__Iteration 2:__",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bigrams in the word: {('lo', 'k'), ('i', '</w>'), ('k', 'i')}\n",
            "candidate for merging: ('lo', 'k')\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "__Candidate not in BPE merges, algorithm stops.__",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('lo', 'k', 'i')"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 549
        },
        "id": "qifOIdpDKWXr",
        "outputId": "e919e5a7-437b-4259-f914-88800453b798"
      },
      "source": [
        "encode(\"lowest\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "__word split into characters:__ <tt>('l', 'o', 'w', 'e', 's', 't', '</w>')</tt>",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "__Iteration 1:__",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bigrams in the word: {('e', 's'), ('w', 'e'), ('l', 'o'), ('s', 't'), ('o', 'w'), ('t', '</w>')}\n",
            "candidate for merging: ('e', 's')\n",
            "word after merging: ('l', 'o', 'w', 'es', 't', '</w>')\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "__Iteration 2:__",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bigrams in the word: {('es', 't'), ('l', 'o'), ('w', 'es'), ('o', 'w'), ('t', '</w>')}\n",
            "candidate for merging: ('es', 't')\n",
            "word after merging: ('l', 'o', 'w', 'est', '</w>')\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "__Iteration 3:__",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bigrams in the word: {('est', '</w>'), ('l', 'o'), ('w', 'est'), ('o', 'w')}\n",
            "candidate for merging: ('est', '</w>')\n",
            "word after merging: ('l', 'o', 'w', 'est</w>')\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "__Iteration 4:__",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bigrams in the word: {('l', 'o'), ('w', 'est</w>'), ('o', 'w')}\n",
            "candidate for merging: ('l', 'o')\n",
            "word after merging: ('lo', 'w', 'est</w>')\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "__Iteration 5:__",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bigrams in the word: {('lo', 'w'), ('w', 'est</w>')}\n",
            "candidate for merging: ('lo', 'w')\n",
            "word after merging: ('low', 'est</w>')\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "__Iteration 6:__",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bigrams in the word: {('low', 'est</w>')}\n",
            "candidate for merging: ('low', 'est</w>')\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "__Candidate not in BPE merges, algorithm stops.__",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('low', 'est')"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        },
        "id": "PU2s65dmKYiP",
        "outputId": "74448d08-aa9a-450a-ac74-95e893e38dfe"
      },
      "source": [
        "encode(\"lowing\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "__word split into characters:__ <tt>('l', 'o', 'w', 'i', 'n', 'g', '</w>')</tt>",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "__Iteration 1:__",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bigrams in the word: {('g', '</w>'), ('i', 'n'), ('l', 'o'), ('o', 'w'), ('n', 'g'), ('w', 'i')}\n",
            "candidate for merging: ('l', 'o')\n",
            "word after merging: ('lo', 'w', 'i', 'n', 'g', '</w>')\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "__Iteration 2:__",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bigrams in the word: {('g', '</w>'), ('i', 'n'), ('lo', 'w'), ('n', 'g'), ('w', 'i')}\n",
            "candidate for merging: ('lo', 'w')\n",
            "word after merging: ('low', 'i', 'n', 'g', '</w>')\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "__Iteration 3:__",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bigrams in the word: {('n', 'g'), ('i', 'n'), ('g', '</w>'), ('low', 'i')}\n",
            "candidate for merging: ('n', 'g')\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "__Candidate not in BPE merges, algorithm stops.__",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('low', 'i', 'n', 'g')"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 149
        },
        "id": "vfJv8RS-Kam8",
        "outputId": "2bdf5270-a696-4319-cfea-73a6fd195b82"
      },
      "source": [
        "encode(\"highing\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "__word split into characters:__ <tt>('h', 'i', 'g', 'h', 'i', 'n', 'g', '</w>')</tt>",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "__Iteration 1:__",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bigrams in the word: {('g', '</w>'), ('i', 'n'), ('i', 'g'), ('h', 'i'), ('n', 'g'), ('g', 'h')}\n",
            "candidate for merging: ('g', '</w>')\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "__Candidate not in BPE merges, algorithm stops.__",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('h', 'i', 'g', 'h', 'i', 'n', 'g')"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rq9Vb1R4Kft4"
      },
      "source": [
        "#### 3. Wordpiece model\n",
        "* BPE의 변형 알고리즘\n",
        "* WPM은 BPE가 **빈도수**에 기반하여 가장 많이 등장한 쌍을 병합하는 것과는 달리, **병합되었을 때 코퍼스의 우도(Likelihood)를 가장 높이는 쌍을 병합**\n",
        "```\n",
        "WPM을 수행하기 이전의 문장: Jet makers feud over seat width with big orders at stake\n",
        "WPM을 수행한 결과(wordpieces): _J et _makers _fe ud _over _seat _width _with _big _orders _at _stake\n",
        "```\n",
        "* 모든 단어의 맨 앞에 _를 붙이고, 단어는 subword로 통계에 기반하여 띄어쓰기로 분리\n",
        "* 기존에 있던 띄어쓰기와 구분자 역할의 띄어쓰기는 어떻게 구별?\n",
        "  * 이 역할을 수행하는 것이 단어들 앞에 붙은 언더바 _\n",
        "* WPM이 수행된 결과로부터 다시 수행 전의 결과로 돌리는 방법\n",
        "  * 현재 있는 모든 띄어쓰기를 전부 제거하고, 언더바를 띄어쓰기로 바꾸면 됨\n",
        "* BERT 훈련에도 사용됨\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_3UqnC1LNQ4"
      },
      "source": [
        "#### 4. Unigram Language Model Tokenizer\n",
        "* 각각의 서브워드들에 대해서 **손실(loss)**를 계산\n",
        "  * 서브 단어의 손실: 해당 서브워드가 단어 집합에서 **제거**되었을 경우, **코퍼스의 우도(Likelihood)가 감소하는 정도**\n",
        "* 이렇게 측정된 서브워드들은 손실의 정도로 정렬하여, 최악의 영향을 주는 10-20%의 토큰을 제거\n",
        "  * 이를 원하는 단어 집합의 크기에 도달할 때까지 반복\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5_9Xs3JMa6D"
      },
      "source": [
        "### 02) 센텐스피스(SentencePiece)\n",
        "#### 1. 센텐스피스(SentencePiece)\n",
        "* 구글의 Sentencepiece: BPE 알고리즘과 Unigram Language Model Tokenizer를 구현한 센텐스피스를 깃허브에 공개\n",
        "* 내부 단어 분리 알고리즘을 사용하기 위해서, 데이터에 단어 토큰화를 먼저 진행한 상태여야 한다면 이 단어 분리 아록리즘을 모든 언어에 사용하는 것은 쉽지 않음\n",
        "  * 영어와 달리 한국어는 단어 토큰화부터 쉽지 않음\n",
        "  * 이런 사전 토큰화 작업(pretokenization) 없이 raw data에 바로 단어 분리 토크나이저를 사용할 수 있다면, 이 토크나이저는 어떤 단어에도 적용할 수 있는 토크나이저가 될 것\n",
        "    * Sentencepiece는 이 이점을 살려서 구현됨\n",
        "    * **사전 토큰화 작업 없이 단어 분리 토큰화를 수행하므로 언어에 종속되지 않음**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "whaIsXosKcOw",
        "outputId": "5cfe070e-5e3f-49f4-a632-efad58724358"
      },
      "source": [
        "!pip install sentencepiece"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[?25l\r\u001b[K     |▎                               | 10 kB 21.1 MB/s eta 0:00:01\r\u001b[K     |▌                               | 20 kB 26.1 MB/s eta 0:00:01\r\u001b[K     |▉                               | 30 kB 23.3 MB/s eta 0:00:01\r\u001b[K     |█                               | 40 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |█▍                              | 51 kB 5.8 MB/s eta 0:00:01\r\u001b[K     |█▋                              | 61 kB 6.1 MB/s eta 0:00:01\r\u001b[K     |██                              | 71 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██▏                             | 81 kB 6.0 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 92 kB 6.1 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 102 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███                             | 112 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███▎                            | 122 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 133 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 143 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████                            | 153 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████▎                           | 163 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████▋                           | 174 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████▉                           | 184 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 194 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 204 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 215 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████                          | 225 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 235 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 245 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 256 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████                         | 266 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 276 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 286 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 296 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████                        | 307 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 317 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 327 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 337 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 348 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 358 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 368 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 378 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 389 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 399 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 409 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 419 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 430 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 440 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 450 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 460 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 471 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 481 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 491 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 501 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 512 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 522 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 532 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 542 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 552 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 563 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 573 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 583 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 593 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 604 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 614 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 624 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 634 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 645 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 655 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 665 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 675 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 686 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 696 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 706 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 716 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 727 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 737 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 747 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 757 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 768 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 778 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 788 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 798 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 808 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 819 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 829 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 839 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 849 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 860 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 870 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 880 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 890 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 901 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 911 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 921 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 931 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 942 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 952 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 962 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 972 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 983 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 993 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 1.0 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 1.0 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.0 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 1.0 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.0 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 1.1 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.1 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 1.1 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.1 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.1 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.1 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.1 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.1 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.1 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.1 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.2 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.2 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.2 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.2 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 1.2 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.2 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.2 MB 5.3 MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9K-FVWBmNynC"
      },
      "source": [
        "#### 2. IMDB 리뷰 토큰화하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WoO494kFNwq7"
      },
      "source": [
        "import sentencepiece as spm\n",
        "import pandas as pd\n",
        "import urllib.request\n",
        "import csv"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XprJbUPfN15Q",
        "outputId": "49b52e7a-c14b-426a-a19f-652ee32b9332"
      },
      "source": [
        "# IMDB 리뷰 데이터를 다운로드하고, 이를 데이터프레임에 저장\n",
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/LawrenceDuan/IMDb-Review-Analysis/master/IMDb_Reviews.csv\", filename=\"IMDb_Reviews.csv\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('IMDb_Reviews.csv', <http.client.HTTPMessage at 0x7fc886045d10>)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZhJ0DQFmN6Oq",
        "outputId": "0b75703d-3869-471d-ea05-a1e3ba004368"
      },
      "source": [
        "train_df = pd.read_csv('IMDb_Reviews.csv')\n",
        "train_df['review']"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0        My family and I normally do not watch local mo...\n",
              "1        Believe it or not, this was at one time the wo...\n",
              "2        After some internet surfing, I found the \"Home...\n",
              "3        One of the most unheralded great works of anim...\n",
              "4        It was the Sixties, and anyone with long hair ...\n",
              "                               ...                        \n",
              "49995    the people who came up with this are SICK AND ...\n",
              "49996    The script is so so laughable... this in turn,...\n",
              "49997    \"So there's this bride, you see, and she gets ...\n",
              "49998    Your mind will not be satisfied by this nobud...\n",
              "49999    The chaser's war on everything is a weekly sho...\n",
              "Name: review, Length: 50000, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BauiLX1AN9vq",
        "outputId": "e5d3b3bd-6185-4c60-c040-40cdbc34a941"
      },
      "source": [
        "print('리뷰 개수 :',len(train_df))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "리뷰 개수 : 50000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOiJdv56OAGR"
      },
      "source": [
        "# 총 5만개의 샘플 존재\n",
        "# 데이터프레임을 txt 파일로 저장\n",
        "with open('imdb_review.txt', 'w', encoding='utf8') as f:\n",
        "    f.write('\\n'.join(train_df['review']))"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5FWzLvEOEyU"
      },
      "source": [
        "# 센텐스피스로 단어 집합과 각 단어에 고유한 정수 부여\n",
        "spm.SentencePieceTrainer.Train('--input=imdb_review.txt --model_prefix=imdb --vocab_size=5000 --model_type=bpe --max_sentence_length=9999')\n",
        "\n",
        "'''\n",
        "input : 학습시킬 파일\n",
        "model_prefix : 만들어질 모델 이름\n",
        "vocab_size : 단어 집합의 크기\n",
        "model_type : 사용할 모델 (unigram(default), bpe, char, word)\n",
        "max_sentence_length: 문장의 최대 길이\n",
        "pad_id, pad_piece: pad token id, 값\n",
        "unk_id, unk_piece: unknown token id, 값\n",
        "bos_id, bos_piece: begin of sentence token id, 값\n",
        "eos_id, eos_piece: end of sequence token id, 값\n",
        "user_defined_symbols: 사용자 정의 토큰\n",
        "'''"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "id": "5kwSB-ZnOJiR",
        "outputId": "3e63f858-648d-4228-f7ec-4a159946a3ce"
      },
      "source": [
        "# vocab 생성이 완료되면 imdb.model, imdb.vocab 파일 두 개가 생성됨\n",
        "# vocab 파일에서 학습된 서브워드들을 확인 가능\n",
        "# 단어 집합의 크기를 확인하기 위해 vocab 파일을 데이터프레임에 저장\n",
        "\n",
        "vocab_list = pd.read_csv('imdb.vocab', sep='\\t', header=None, quoting=csv.QUOTE_NONE)\n",
        "vocab_list.sample(10)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3824</th>\n",
              "      <td>Sh</td>\n",
              "      <td>-3821</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2988</th>\n",
              "      <td>▁genuine</td>\n",
              "      <td>-2985</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1143</th>\n",
              "      <td>▁ey</td>\n",
              "      <td>-1140</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3732</th>\n",
              "      <td>▁Though</td>\n",
              "      <td>-3729</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4755</th>\n",
              "      <td>▁(2</td>\n",
              "      <td>-4752</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3028</th>\n",
              "      <td>sequ</td>\n",
              "      <td>-3025</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>854</th>\n",
              "      <td>▁diffe</td>\n",
              "      <td>-851</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2491</th>\n",
              "      <td>▁Gra</td>\n",
              "      <td>-2488</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1127</th>\n",
              "      <td>▁comm</td>\n",
              "      <td>-1124</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>117</th>\n",
              "      <td>▁M</td>\n",
              "      <td>-114</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             0     1\n",
              "3824        Sh -3821\n",
              "2988  ▁genuine -2985\n",
              "1143       ▁ey -1140\n",
              "3732   ▁Though -3729\n",
              "4755       ▁(2 -4752\n",
              "3028      sequ -3025\n",
              "854     ▁diffe  -851\n",
              "2491      ▁Gra -2488\n",
              "1127     ▁comm -1124\n",
              "117         ▁M  -114"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TfK4HXBlOlWA",
        "outputId": "e5c4ce8a-e20a-4217-cc04-0e2e64859839"
      },
      "source": [
        "len(vocab_list)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5000"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wVkCE0FFOmpP",
        "outputId": "158c3e7e-1dfc-492a-fcdf-e0018aabb431"
      },
      "source": [
        "# model 파일을 로드하여 단어 시퀀스를 정수 시퀀스로 바꾸는 인코딩 작업이나 반대로 변환하는 디코딩 작업 가능\n",
        "\n",
        "sp = spm.SentencePieceProcessor()\n",
        "vocab_file = \"imdb.model\"\n",
        "sp.load(vocab_file)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tJdyL02wOyaE",
        "outputId": "2ae36c5a-3375-43c1-b321-70ba1f73e4f9"
      },
      "source": [
        "'''\n",
        "encode_as_pieces : 문장을 입력하면 서브 워드 시퀀스로 변환합니다.\n",
        "encode_as_ids : 문장을 입력하면 정수 시퀀스로 변환합니다.\n",
        "'''\n",
        "\n",
        "lines = [\n",
        "  \"I didn't at all think of it this way.\",\n",
        "  \"I have waited a long time for someone to film\"\n",
        "]\n",
        "\n",
        "for line in lines:\n",
        "  print(line)\n",
        "  print(sp.encode_as_pieces(line))\n",
        "  print(sp.encode_as_ids(line))\n",
        "  print()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I didn't at all think of it this way.\n",
            "['▁I', '▁didn', \"'\", 't', '▁at', '▁all', '▁think', '▁of', '▁it', '▁this', '▁way', '.']\n",
            "[41, 623, 4950, 4926, 138, 169, 378, 30, 58, 73, 413, 4945]\n",
            "\n",
            "I have waited a long time for someone to film\n",
            "['▁I', '▁have', '▁wa', 'ited', '▁a', '▁long', '▁time', '▁for', '▁someone', '▁to', '▁film']\n",
            "[41, 141, 1364, 1120, 4, 666, 285, 92, 1078, 33, 91]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ojie2jzoPFzH",
        "outputId": "d1e16acc-d1ff-41d9-e3be-2acdada2e6e1"
      },
      "source": [
        "sp.GetPieceSize() #단어 집합의 크기 확인"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5000"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "g60tMST1PIn2",
        "outputId": "31ae6d2b-762a-47b7-f67d-027531a31290"
      },
      "source": [
        "sp.IdToPiece(430) #정수로부터 맵핑되는 서브 워드로 변환"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'▁character'"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O4lvk3GAPOsN",
        "outputId": "cbc1fddb-dd5f-4848-fc6b-a69fa0b4f6e6"
      },
      "source": [
        "sp.PieceToId('_character') #서브워드로부터 맵핑되는 정수로 변환"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "o9Qxyb85PWfk",
        "outputId": "65e5f811-284a-40c2-c10e-0d6c012dcc9c"
      },
      "source": [
        "sp.DecodeIds([41, 141, 1364, 1120, 4, 666, 285, 92, 1078, 33, 91]) #정수 시퀀스로부터 문장으로 변환"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'I have waited a long time for someone to film'"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "LFMExgiLPbge",
        "outputId": "6a642372-abfb-4737-cc25-e3ffa867daf6"
      },
      "source": [
        "sp.DecodePieces(['▁I', '▁have', '▁wa', 'ited', '▁a', '▁long', '▁time', '▁for', '▁someone', '▁to', '▁film']) \n",
        "#서브워드 시퀀스로부터 문장으로 변환"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'I have waited a long time for someone to film'"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lGzd_Ee3PhOr",
        "outputId": "b1d445b7-d5ae-4a65-9472-50971a5953ec"
      },
      "source": [
        "print(sp.encode('I have waited a long time for someone to film', out_type=str))\n",
        "print(sp.encode('I have waited a long time for someone to film', out_type=int))\n",
        "#문장으로부터 인자값에 따라서 정수 시퀀스 또는 서브워드 시퀀스로 변환 가능"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['▁I', '▁have', '▁wa', 'ited', '▁a', '▁long', '▁time', '▁for', '▁someone', '▁to', '▁film']\n",
            "[41, 141, 1364, 1120, 4, 666, 285, 92, 1078, 33, 91]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dXZ37JAYPtMp"
      },
      "source": [
        "#### 3. 네이버 영화 리뷰 토큰화하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zfl2xlNyPr6e"
      },
      "source": [
        "import pandas as pd\n",
        "import sentencepiece as spm\n",
        "import urllib.request\n",
        "import csv"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "YF7q1zM1PyrM",
        "outputId": "3b2baf5f-02a4-4f3c-cd92-4aa8a9c625ba"
      },
      "source": [
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings.txt\", filename=\"ratings.txt\")\n",
        "naver_df = pd.read_table('ratings.txt')\n",
        "naver_df[:5]"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>document</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>8112052</td>\n",
              "      <td>어릴때보고 지금다시봐도 재밌어요ㅋㅋ</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>8132799</td>\n",
              "      <td>디자인을 배우는 학생으로, 외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4655635</td>\n",
              "      <td>폴리스스토리 시리즈는 1부터 뉴까지 버릴께 하나도 없음.. 최고.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9251303</td>\n",
              "      <td>와.. 연기가 진짜 개쩔구나.. 지루할거라고 생각했는데 몰입해서 봤다.. 그래 이런...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>10067386</td>\n",
              "      <td>안개 자욱한 밤하늘에 떠 있는 초승달 같은 영화.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         id                                           document  label\n",
              "0   8112052                                어릴때보고 지금다시봐도 재밌어요ㅋㅋ      1\n",
              "1   8132799  디자인을 배우는 학생으로, 외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산...      1\n",
              "2   4655635               폴리스스토리 시리즈는 1부터 뉴까지 버릴께 하나도 없음.. 최고.      1\n",
              "3   9251303  와.. 연기가 진짜 개쩔구나.. 지루할거라고 생각했는데 몰입해서 봤다.. 그래 이런...      1\n",
              "4  10067386                        안개 자욱한 밤하늘에 떠 있는 초승달 같은 영화.      1"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ukXItYnBP1YR",
        "outputId": "a1787828-20e2-4eba-b999-3f7c787c4ea4"
      },
      "source": [
        "print('리뷰 개수 :',len(naver_df)) "
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "리뷰 개수 : 200000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yaovP8KgP24U",
        "outputId": "3d0b8375-31d3-4d49-c98b-6dfdb9e7809f"
      },
      "source": [
        "print(naver_df.isnull().values.any())"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wTUP-0GCP4WH",
        "outputId": "03dfa22b-4dbf-48d9-d72d-3184a34d048e"
      },
      "source": [
        "naver_df = naver_df.dropna(how = 'any') # Null 값이 존재하는 행 제거\n",
        "print(naver_df.isnull().values.any()) # Null 값이 존재하는지 확인"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91-tbM9JP7fU",
        "outputId": "5fc4e830-e191-49c1-da85-82686a710848"
      },
      "source": [
        "print('리뷰 개수 :',len(naver_df))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "리뷰 개수 : 199992\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7L6xyXbP9Le"
      },
      "source": [
        "# 199992개의 샘플을 naver_review.txt 파일에 저장한 후에\n",
        "# 센텐스피스를 통해 단어 집합을 생성\n",
        "with open('naver_review.txt', 'w', encoding='utf8') as f:\n",
        "    f.write('\\n'.join(naver_df['document']))"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASO-tuOkP_wB"
      },
      "source": [
        "spm.SentencePieceTrainer.Train('--input=naver_review.txt --model_prefix=naver --vocab_size=5000 --model_type=bpe --max_sentence_length=9999')"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "id": "d-FllSjNQBvm",
        "outputId": "caac8e8b-2027-4ddb-9fd8-45f8f0a37a50"
      },
      "source": [
        "# vocab 생성이 완료되면 naver.model, naver.vocab 파일 두 개가 생성됨\n",
        "# vocab에서 학습된 subwords를 확인 가능\n",
        "\n",
        "vocab_list = pd.read_csv('naver.vocab', sep='\\t', header=None, quoting=csv.QUOTE_NONE)\n",
        "vocab_list[:10]"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>&lt;unk&gt;</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>&lt;s&gt;</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>&lt;/s&gt;</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>..</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>영화</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>▁영화</td>\n",
              "      <td>-2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>▁이</td>\n",
              "      <td>-3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>▁아</td>\n",
              "      <td>-4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>...</td>\n",
              "      <td>-5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>▁그</td>\n",
              "      <td>-6</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       0  1\n",
              "0  <unk>  0\n",
              "1    <s>  0\n",
              "2   </s>  0\n",
              "3     ..  0\n",
              "4     영화 -1\n",
              "5    ▁영화 -2\n",
              "6     ▁이 -3\n",
              "7     ▁아 -4\n",
              "8    ... -5\n",
              "9     ▁그 -6"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "id": "yEmM7Tm-QPLB",
        "outputId": "c3e9d142-5025-4565-dff4-9e04d6501643"
      },
      "source": [
        "vocab_list.sample(10)\n",
        "\n",
        "# Vocab에는 unknown,\n",
        "# 문장의 시작, 문장의 끝을 의미하는 special token이 0, 1, 2에 사용됨"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3517</th>\n",
              "      <td>경</td>\n",
              "      <td>-3514</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1657</th>\n",
              "      <td>유의</td>\n",
              "      <td>-1654</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1581</th>\n",
              "      <td>!!!!!!!!</td>\n",
              "      <td>-1578</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1254</th>\n",
              "      <td>♥♥</td>\n",
              "      <td>-1251</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1414</th>\n",
              "      <td>▁삼류</td>\n",
              "      <td>-1411</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3942</th>\n",
              "      <td>슈</td>\n",
              "      <td>-3939</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>562</th>\n",
              "      <td>▁독</td>\n",
              "      <td>-559</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3635</th>\n",
              "      <td>희</td>\n",
              "      <td>-3632</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4707</th>\n",
              "      <td>퀼</td>\n",
              "      <td>-4704</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3688</th>\n",
              "      <td>범</td>\n",
              "      <td>-3685</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             0     1\n",
              "3517         경 -3514\n",
              "1657        유의 -1654\n",
              "1581  !!!!!!!! -1578\n",
              "1254        ♥♥ -1251\n",
              "1414       ▁삼류 -1411\n",
              "3942         슈 -3939\n",
              "562         ▁독  -559\n",
              "3635         희 -3632\n",
              "4707         퀼 -4704\n",
              "3688         범 -3685"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "seNCTeOVQPm2",
        "outputId": "29f91382-59ae-4920-d62b-331b99e03157"
      },
      "source": [
        "len(vocab_list)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5000"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jCyeSVlPQREe",
        "outputId": "785d523e-20d7-4c72-b19e-dd89fb079622"
      },
      "source": [
        "sp = spm.SentencePieceProcessor()\n",
        "vocab_file = \"naver.model\"\n",
        "sp.load(vocab_file)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xN_IYQuIQeqD",
        "outputId": "83de52b5-9ae0-405c-8017-c1fc6d0c4e01"
      },
      "source": [
        "lines = [\n",
        "  \"뭐 이딴 것도 영화냐.\",\n",
        "  \"진짜 최고의 영화입니다 ㅋㅋ\",\n",
        "]\n",
        "for line in lines:\n",
        "  print(line)\n",
        "  print(sp.encode_as_pieces(line))\n",
        "  print(sp.encode_as_ids(line))\n",
        "  print()"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "뭐 이딴 것도 영화냐.\n",
            "['▁뭐', '▁이딴', '▁것도', '▁영화냐', '.']\n",
            "[132, 966, 1296, 2590, 3276]\n",
            "\n",
            "진짜 최고의 영화입니다 ㅋㅋ\n",
            "['▁진짜', '▁최고의', '▁영화입니다', '▁ᄏᄏ']\n",
            "[54, 200, 821, 85]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x3vz5WEjQgsG",
        "outputId": "8497ec19-da02-4b1b-bfb5-b3358aff5330"
      },
      "source": [
        "sp.GetPieceSize()"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5000"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "J_vYbRjPQiDA",
        "outputId": "d11a32fa-1edd-4731-a05d-3672b0c611bb"
      },
      "source": [
        "sp.IdToPiece(4)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'영화'"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1fWBSwQjQkVe",
        "outputId": "2aa47a1f-a69b-4c43-ea1e-1d6de4cbbbd7"
      },
      "source": [
        "sp.PieceToId('영화')"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "RMQgqecmQlQ5",
        "outputId": "eaf6e73a-a75c-4c79-d9eb-f650bb07fa64"
      },
      "source": [
        "sp.DecodeIds([54, 200, 821, 85])"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'진짜 최고의 영화입니다 ᄏᄏ'"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "zkXav-eHQmdh",
        "outputId": "7bc398b7-7aa2-46da-ef39-2bead6467647"
      },
      "source": [
        "sp.DecodePieces(['▁진짜', '▁최고의', '▁영화입니다', '▁ᄏᄏ'])"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'진짜 최고의 영화입니다 ᄏᄏ'"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CpeWqjIYQoDY",
        "outputId": "0ec2c8c8-83bd-4214-f1ee-754b642ac017"
      },
      "source": [
        "print(sp.encode('진짜 최고의 영화입니다 ㅋㅋ', out_type=str))\n",
        "print(sp.encode('진짜 최고의 영화입니다 ㅋㅋ', out_type=int))"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['▁진짜', '▁최고의', '▁영화입니다', '▁ᄏᄏ']\n",
            "[54, 200, 821, 85]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VkXBpOl8QvDb"
      },
      "source": [
        "### 03) SubwordTextEncoder\n",
        "* 텐서플로우를 통해 사용할 수 있는 서브워드 토크나이저\n",
        "* BPE와 유사한 알고리즘인 Wordpiece Model을 채택. 패키지로 쉽게 단어들을 서브워드들로 분리 가능\n",
        "#### 1. IMDB 리뷰 토큰화하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqePDFWCQpWK"
      },
      "source": [
        "import tensorflow_datasets as tfds\n",
        "import urllib.request\n",
        "import pandas as pd"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "io1jNDtRQ70I",
        "outputId": "4e85f944-4cf5-4e87-ad01-9df47c3d5fa4"
      },
      "source": [
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/LawrenceDuan/IMDb-Review-Analysis/master/IMDb_Reviews.csv\", filename=\"IMDb_Reviews.csv\")"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('IMDb_Reviews.csv', <http.client.HTTPMessage at 0x7fc80b40ded0>)"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UvRf1qkSQ88d"
      },
      "source": [
        "train_df = pd.read_csv('IMDb_Reviews.csv')"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XMC7VQPVQ98z",
        "outputId": "2af1cbf4-012c-454b-8639-4ea86535890c"
      },
      "source": [
        "# 토큰화를 수행해야 할 데이터\n",
        "train_df['review']"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0        My family and I normally do not watch local mo...\n",
              "1        Believe it or not, this was at one time the wo...\n",
              "2        After some internet surfing, I found the \"Home...\n",
              "3        One of the most unheralded great works of anim...\n",
              "4        It was the Sixties, and anyone with long hair ...\n",
              "                               ...                        \n",
              "49995    the people who came up with this are SICK AND ...\n",
              "49996    The script is so so laughable... this in turn,...\n",
              "49997    \"So there's this bride, you see, and she gets ...\n",
              "49998    Your mind will not be satisfied by this nobud...\n",
              "49999    The chaser's war on everything is a weekly sho...\n",
              "Name: review, Length: 50000, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4LFRv6cRBgM"
      },
      "source": [
        "# tfds.features.text.SubwordTextEncoder.build_from_corpus의 인자로 토큰화 할 데이터를 넣어줌\n",
        "# 서브워드들로 이루어진 단어 집합(Vocabulary)를 생성하고, 각 서브워드에 고유한 정수를 부여\n",
        "\n",
        "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
        "    train_df['review'], target_vocab_size=2**13)"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Me9mQIGGRZRD",
        "outputId": "dc0402a6-f345-4ac7-d89d-a5b7301415d6"
      },
      "source": [
        "print(tokenizer.subwords[:100])"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['the_', ', ', '. ', 'a_', 'and_', 'of_', 'to_', 's_', 'is_', 'br', 'in_', 'I_', 'that_', 'this_', 'it_', ' /><', ' />', 'was_', 'The_', 't_', 'as_', 'with_', 'for_', '.<', 'on_', 'but_', 'movie_', 'are_', ' (', 'have_', 'his_', 'film_', 'not_', 'be_', 'you_', 'ing_', ' \"', 'ed_', 'it', 'd_', 'an_', 'at_', 'by_', 'he_', 'one_', 'who_', 'from_', 'y_', 'or_', 'e_', 'like_', 'all_', '\" ', 'they_', 'so_', 'just_', 'has_', ') ', 'about_', 'her_', 'out_', 'This_', 'some_', 'movie', 'ly_', 'film', 'very_', 'more_', 'It_', 'what_', 'would_', 'when_', 'if_', 'good_', 'up_', 'which_', 'their_', 'only_', 'even_', 'my_', 'really_', 'had_', 'can_', 'no_', 'were_', 'see_', '? ', 'she_', 'than_', '! ', 'there_', 'been_', 'get_', 'into_', 'will_', ' - ', 'much_', 'n_', 'because_', 'ing']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4gu-DgfERjGn",
        "outputId": "761d924a-63a3-4b80-e712-8f87d4fa83a0"
      },
      "source": [
        "print(train_df['review'][20])"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pretty bad PRC cheapie which I rarely bother to watch over again, and it's no wonder -- it's slow and creaky and dull as a butter knife. Mad doctor George Zucco is at it again, turning a dimwitted farmhand in overalls (Glenn Strange) into a wolf-man. Unfortunately, the makeup is virtually non-existent, consisting only of a beard and dimestore fangs for the most part. If it were not for Zucco and Strange's presence, along with the cute Anne Nagel, this would be completely unwatchable. Strange, who would go on to play Frankenstein's monster for Unuiversal in two years, does a Lenny impression from \"Of Mice and Men\", it seems.<br /><br />*1/2 (of Four)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TReRcht3RlXs"
      },
      "source": [
        "# encode()를 통해서 입력한 데이터에 대해서 정수 인코딩을 수행한 결과를 얻을 수 있음\n",
        "print('Tokenized sample question: {}'.format(tokenizer.encode(train_df['review'][20])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l6p7A3H0Rwe6",
        "outputId": "3c9fdffb-832d-4c6e-f976-847b40e66636"
      },
      "source": [
        "# 임의로 선택한 짧은 문장에 대해서 정수 인코딩 결과를 확인하고, 이를 다시 역으로 디코딩\n",
        "# train_df에 존재하는 문장 중 일부를 발췌\n",
        "sample_string = \"It's mind-blowing to me that this film was even made.\"\n",
        "\n",
        "# 인코딩한 결과를 tokenized_string에 저장\n",
        "tokenized_string = tokenizer.encode(sample_string)\n",
        "print('정수 인코딩 후의 문장 {}'.format(tokenized_string))\n",
        "\n",
        "# 이를 다시 디코딩\n",
        "original_string = tokenizer.decode(tokenized_string)\n",
        "print('기존 문장 : {}'.format(original_string))"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "정수 인코딩 후의 문장 [137, 8051, 8, 910, 8057, 2169, 36, 7, 103, 830, 1601, 8044, 14, 32, 18, 79, 681, 8058]\n",
            "기존 문장 : It's mind-blowing to me taht this film was even made.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kdxHspfzSJ_M",
        "outputId": "5952063a-91f0-4a15-dfaa-b63fd00af59f"
      },
      "source": [
        "print('단어 집합의 크기(Vocab size) :', tokenizer.vocab_size)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "단어 집합의 크기(Vocab size) : 8268\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y1a06v8VSL6s",
        "outputId": "2bfc2487-a971-4343-e59e-334720ca1b36"
      },
      "source": [
        "for ts in tokenized_string:\n",
        "  print ('{} ----> {}'.format(ts, tokenizer.decode([ts])))"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "137 ----> It\n",
            "8051 ----> '\n",
            "8 ----> s \n",
            "910 ----> mind\n",
            "8057 ----> -\n",
            "2169 ----> blow\n",
            "36 ----> ing \n",
            "7 ----> to \n",
            "103 ----> me \n",
            "830 ----> ta\n",
            "1601 ----> ht\n",
            "8044 ---->  \n",
            "14 ----> this \n",
            "32 ----> film \n",
            "18 ----> was \n",
            "79 ----> even \n",
            "681 ----> made\n",
            "8058 ----> .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VIXqNo76SOSx",
        "outputId": "2d6f31bb-3b88-414f-fe37-80d5c26db1c7"
      },
      "source": [
        "# 기존 예제 문장 중 even이라는 단어에 임의로 xyz라는 3개의 글자 추가\n",
        "# 현재 토크나이저가 even이라는 단어를 이미 하나의 서브워드로 인식하고 있는 상황에서 \n",
        "# 나머지 xyz를 어떻게 분리하는지를 확인하기 위함\n",
        "\n",
        "sample_string = \"It's mind-blowing to me that this film was evenxyz made.\"\n",
        "\n",
        "# 인코딩한 결과를 tokenized_string에 저장\n",
        "tokenized_string = tokenizer.encode(sample_string)\n",
        "print('정수 인코딩 후의 문장 {}'.format(tokenized_string))\n",
        "\n",
        "# 이를 다시 디코딩\n",
        "original_string = tokenizer.decode(tokenized_string)\n",
        "print('기존 문장 : {}'.format(original_string))"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "정수 인코딩 후의 문장 [137, 8051, 8, 910, 8057, 2169, 36, 7, 103, 13, 14, 32, 18, 7974, 8132, 8133, 997, 681, 8058]\n",
            "기존 문장 : It's mind-blowing to me that this film was evenxyz made.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xMbqfhIDSfVB",
        "outputId": "555307c8-e48c-4f1d-d5d8-e85810bf60cf"
      },
      "source": [
        "for ts in tokenized_string:\n",
        "  print ('{} ----> {}'.format(ts, tokenizer.decode([ts])))\n",
        "\n",
        "# evenxyz에서 even을 독립적으로 분리하고 xyz는 훈련 데이터에서 하나의 단어로서 등장한 적이 없으므로\n",
        "# 각각 전부 분리"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "137 ----> It\n",
            "8051 ----> '\n",
            "8 ----> s \n",
            "910 ----> mind\n",
            "8057 ----> -\n",
            "2169 ----> blow\n",
            "36 ----> ing \n",
            "7 ----> to \n",
            "103 ----> me \n",
            "13 ----> that \n",
            "14 ----> this \n",
            "32 ----> film \n",
            "18 ----> was \n",
            "7974 ----> even\n",
            "8132 ----> x\n",
            "8133 ----> y\n",
            "997 ----> z \n",
            "681 ----> made\n",
            "8058 ----> .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tOTUNItSsv_"
      },
      "source": [
        "#### 2. 네이버 영화 리뷰 토큰화하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5uC91-OzShcf"
      },
      "source": [
        "import tensorflow_datasets as tfds\n",
        "import urllib.request"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Wle1-kOSxgT",
        "outputId": "a1799500-345e-4f0e-e19d-6c4074d6ac98"
      },
      "source": [
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\", filename=\"ratings_train.txt\")\n",
        "train_data = pd.read_table('ratings_train.txt')\n",
        "print(train_data.isnull().sum())"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "id          0\n",
            "document    5\n",
            "label       0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Zc1UdyQS4tD",
        "outputId": "45b8c334-4f48-43f7-973c-fd6c967c6572"
      },
      "source": [
        "train_data = train_data.dropna(how = 'any') # Null 값이 존재하는 행 제거\n",
        "print(train_data.isnull().values.any())"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lfrzDpRS7sr"
      },
      "source": [
        "# 인자로 네이버 영화 리뷰 데이터를 넣어서\n",
        "# 서브워드들로 이루어진 단어 집합(Vocabulary)를 생성하고\n",
        "# 각 서브워드에 고유한 정수 부여\n",
        "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
        "    train_data['document'], target_vocab_size=2**13)"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_aV5MuewTIAz",
        "outputId": "358a6fb4-b3c8-4402-9828-3a55520826d7"
      },
      "source": [
        "print(tokenizer.subwords[:100])"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['. ', '..', '영화', '이_', '...', '의_', '는_', '도_', '다', ', ', '을_', '고_', '은_', '가_', '에_', '.. ', '한_', '너무_', '정말_', '를_', '고', '게_', '영화_', '지', '... ', '진짜_', '이', '다_', '요', '만_', '? ', '과_', '나', '가', '서_', '지_', '로_', '으로_', '아', '어', '....', '음', '한', '수_', '와_', '도', '네', '그냥_', '나_', '더_', '왜_', '이런_', '면_', '기', '하고_', '보고_', '하는_', '서', '좀_', '리', '자', '스', '안', '! ', '에서_', '영화를_', '미', 'ㅋㅋ', '네요', '시', '주', '라', '는', '오', '없는_', '에', '해', '사', '!!', '영화는_', '마', '잘_', '수', '영화가_', '만', '본_', '로', '그_', '지만_', '대', '은', '비', '의', '일', '개', '있는_', '없다', '함', '구', '하']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8bLxeDUiTJ8P",
        "outputId": "e28f4fc0-8ea8-4761-d9b7-e4a1e441884b"
      },
      "source": [
        "# 21번째 샘플을 인코딩\n",
        "print('Tokenized sample question: {}'.format(tokenizer.encode(train_data['document'][20])))"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized sample question: [669, 4700, 17, 1749, 8, 96, 131, 1, 48, 2239, 4, 7466, 32, 1274, 2655, 7, 80, 749, 1254]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FcPf7gb3TNv-",
        "outputId": "92f39cdc-35d1-4421-80f8-07efb3b4cdd2"
      },
      "source": [
        "# 역으로 디코딩\n",
        "sample_string = train_data['document'][21]\n",
        "\n",
        "# 인코딩한 결과를 tokenized_string에 저장\n",
        "tokenized_string = tokenizer.encode(sample_string)\n",
        "print ('정수 인코딩 후의 문장 {}'.format(tokenized_string))\n",
        "\n",
        "# 이를 다시 디코딩\n",
        "original_string = tokenizer.decode(tokenized_string)\n",
        "print ('기존 문장: {}'.format(original_string))"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "정수 인코딩 후의 문장 [570, 892, 36, 584, 159, 7091, 201]\n",
            "기존 문장: 보면서 웃지 않는 건 불가능하다\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uwegFxG-TQTU",
        "outputId": "a95cce96-7a54-402f-a8c2-315ce9dd54c6"
      },
      "source": [
        "# 임의로 작성한 짧은 문장에 대해서 정수 인코딩 결과 확인하고 역으로 디코딩\n",
        "# 킄핫핫ㅎ 추가\n",
        "\n",
        "sample_string = '이 영화 굉장히 재밌다 킄핫핫ㅎ'\n",
        "\n",
        "# 인코딩한 결과를 tokenized_string에 저장\n",
        "tokenized_string = tokenizer.encode(sample_string)\n",
        "print ('정수 인코딩 후의 문장 {}'.format(tokenized_string))\n",
        "\n",
        "# 이를 다시 디코딩\n",
        "original_string = tokenizer.decode(tokenized_string)\n",
        "print ('기존 문장: {}'.format(original_string))"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "정수 인코딩 후의 문장 [4, 23, 1364, 2157, 8235, 8128, 8130, 8235, 8147, 8169, 8235, 8147, 8169, 393]\n",
            "기존 문장: 이 영화 굉장히 재밌다 킄핫핫ㅎ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aSWi_xJVTRHn",
        "outputId": "3ddfdbe1-e0e2-4704-b110-4791aaae015e"
      },
      "source": [
        "for ts in tokenized_string:\n",
        "  print ('{} ----> {}'.format(ts, tokenizer.decode([ts])))"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4 ----> 이 \n",
            "23 ----> 영화 \n",
            "1364 ----> 굉장히 \n",
            "2157 ----> 재밌다 \n",
            "8235 ----> �\n",
            "8128 ----> �\n",
            "8130 ----> �\n",
            "8235 ----> �\n",
            "8147 ----> �\n",
            "8169 ----> �\n",
            "8235 ----> �\n",
            "8147 ----> �\n",
            "8169 ----> �\n",
            "393 ----> ㅎ\n"
          ]
        }
      ]
    }
  ]
}