{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "What_is_NLP_Ch13_TaggingTask.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FI4ghtcwX-dL"
      },
      "source": [
        "## 13. Tagging Task\n",
        "* Tagging task: 자연어처리 분야에서 각 단어가 어떤 유형에 속해있는지를 알아냄\n",
        "* 크게 2가지\n",
        "1. 개체명 인식(Named Entity Recognition): 각 단어의 유형이 사람, 장소, 단체 등 어떤 유형인지를 알아냄\n",
        "2. 품사 태깅(Part of Speech Tagging): 각 단어의 품사가 명사, 동사, 형용사인지를 알아냄\n",
        "* Keras를 이용해서 인공 신경망을 이용한 개체명 인식기와 품사 태거를 만드는 미니 프로젝트 수행"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wqq-0p54YYba"
      },
      "source": [
        "### 1) 케라스를 이용한 태깅 작업 개요(Tagging Task using Keras)\n",
        "* 이번 챕터에서는 케라스(Keras)로 인공 신경망을 이용하여 태깅 작업을 하는 모델을 만듦\n",
        "* 개체명 인식기와 품사 태거. 두 작업의 공통점은 **RNN의 다-대-다(Many-to-Many)** 작업이면서 또한 앞, 뒤 시점의 입력을 모두 참고하는 **양방향 RNN(Bidirectional RNN)**을 사용한다는 점"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VhIFjIzJY08a"
      },
      "source": [
        "#### 1. 훈련 데이터에 대한 이해\n",
        "* 텍스트 분류 작업과 동일하게 지도 학습(Supervised learning)에 속함\n",
        "* 태깅을 해야 하는 단어 데이터 X, 레이블에 해당되는 태깅 정보 데이터는 y \n",
        "\n",
        "\n",
        "* X와 y 데이터의 쌍(pair)는 병렬 구조를 가진다는 특성. X와 y의 각 데이터의 길이는 같음\n",
        "* 각 데이터는 정수 인코딩 과정을 거친 후, 모든 데이터의 길이를 동일하게 맞춰주기 위한 패딩(Padding) 작업을 거침"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-sT8djJ4Y1CP"
      },
      "source": [
        "#### 2. 시퀀스 레이블링(Sequence Labeling)\n",
        "* 입력 시퀀스에 대하여 레이블 시퀀스를 각각 부여하는 작업을 Sequence Labeling Task라고 함\n",
        "\n",
        "#### 3. 양방향 LSTM(Bidirectional LSTM)\n",
        "> model.add(Bidirectional(LSTM(hidden_size, return_sequences=True)))\n",
        "\n",
        "* 이전 시점의 단어 정보 뿐만 아니라 다음 시점의 단어 정보도 참고하기 위하여 bidirectional LSTM\n",
        "* 인자 정보: 인자값을 하나 줄 경우에는 이는 hidden_size, 결과적으로 output_dim이 됨"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6m2MxlwUY1Fj"
      },
      "source": [
        "#### 4. RNN의 다-대-다(Many-to-Many) 문제\n",
        "* RNN의 은닉층은 모든 시점에 대해서 은닉 상태값을 출력할 수 있고, 마지막 시점에 대해서만 은닉 상태값을 출력할 수 있음\n",
        "  * 이는 인자로 **return_sequences=True**를 넣을 것인지, 넣지 않을 것인지로 설정\n",
        "  * 태깅 작업은 Many-to-Many로 True로 설정\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ku53mbS1aJrN"
      },
      "source": [
        "## 2). 양방향 LSTM을 이용한 품사 태깅(Part-of-speech Tagging using Bi-LSTM)\n",
        "* 2챕터 토큰화 챕터에서는 NLTK와 KoNLPy를 이용해서 이미 기존에 있는 모델로 품사 태깅을 수행했지만, 여기에서는 직접 양방향 LSTM을 이용한 품사 태깅을 수행하는 모델\n",
        "### 1. 품사 태깅 데이터에 대한 이해와 전처리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rWBo3dx2bSZJ",
        "outputId": "94b0f350-233f-4a65-e1f8-801e4041b805"
      },
      "source": [
        "!pip install nltk"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iPLz9yHAbY9z",
        "outputId": "f697e24c-31ea-4a54-8ea5-d0169a6efb82"
      },
      "source": [
        "import nltk\n",
        "nltk.download('treebank')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/treebank.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t6uuB4H1YpYi"
      },
      "source": [
        "# 양방향 LSTM을 이용해서 품사 태깅을 하는 모델\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzwBrcUCa_mS"
      },
      "source": [
        "* NLTK를 이용하면 영어 코퍼스에서 투큰화와 품사 태깅 전처리를 진행한 문장 데이터를 받아올 수 있음\n",
        "* 여기서는 해당 데이터를 훈련시켜 품사 태깅을 수행하는 모델을 만듦"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RgpjIkYVXt9y",
        "outputId": "094963c1-a233-4490-9e0d-0fad30e407ab"
      },
      "source": [
        "# 전체 문장 샘플의 개수 확인\n",
        "tagged_sentences = nltk.corpus.treebank.tagged_sents() # 토큰화에 품사 태깅이 된 데이터 받아오기\n",
        "print(\"품사 태깅이 된 문장 개수: \", len(tagged_sentences)) # 문장 샘플의 개수 출력"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "품사 태깅이 된 문장 개수:  3914\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kzJKRGwYbMbT",
        "outputId": "2dccd3a0-a305-424d-bafe-04399e576841"
      },
      "source": [
        "# 첫번째 샘플만 출력\n",
        "print(tagged_sentences[0])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ('61', 'CD'), ('years', 'NNS'), ('old', 'JJ'), (',', ','), ('will', 'MD'), ('join', 'VB'), ('the', 'DT'), ('board', 'NN'), ('as', 'IN'), ('a', 'DT'), ('nonexecutive', 'JJ'), ('director', 'NN'), ('Nov.', 'NNP'), ('29', 'CD'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwDdJp1Jbf0_"
      },
      "source": [
        "* 훈련을 시키려면 훈련 데이터에서 단어에 해당되는 부분과 품사 태깅 정보에 해당되는 부분은 분리시켜야 함. \n",
        "  * 즉, Pierre, Vinken 같이, NNP와 NNP 같이\n",
        "  * 이런 경우, **zip()** 함수가 유용한 역할. **zip() 함수는 동일한 개수를 가지는 시퀀스 자료형에서 각 순서에 등장하는 원소끼리 묶어주는 역할**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5i4PVYdbdL1"
      },
      "source": [
        "sentences, pos_tags = [], []\n",
        "for tagged_sentence in tagged_sentences: #3914개의 문장 샘플을 1개씩 불러옴\n",
        "  sentence, tag_info = zip(*tagged_sentence) # 각 샘플에서 단어들은 sentence에 품사 태깅 정보들은 tag_info에 저장\n",
        "  sentences.append(list(sentence)) #각 샘플에서 단어 정보만 저장\n",
        "  pos_tags.append(list(tag_info)) #각 샘플에서 품사 태깅 정보만 저장\n",
        "\n",
        "  #sentences.append(list(sentence))에서 list 없으면 ()로 묶임"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lC2HHnlVcaux",
        "outputId": "ed1d4ab8-e0a8-499b-fb05-027351e9d76e"
      },
      "source": [
        "print(sentences[0])\n",
        "print(pos_tags[0])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Pierre', 'Vinken', ',', '61', 'years', 'old', ',', 'will', 'join', 'the', 'board', 'as', 'a', 'nonexecutive', 'director', 'Nov.', '29', '.']\n",
            "['NNP', 'NNP', ',', 'CD', 'NNS', 'JJ', ',', 'MD', 'VB', 'DT', 'NN', 'IN', 'DT', 'JJ', 'NN', 'NNP', 'CD', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "id": "czf3NZGlcced",
        "outputId": "74588415-7e50-41d2-c265-45ae0b1aa003"
      },
      "source": [
        "# 전체 데이터의 길이 분포\n",
        "print('샘플의 최대 길이 : %d' % max(len(l) for l in sentences))\n",
        "print('샘플의 평균 길이 : %f' % (sum(map(len, sentences))/len(sentences)))\n",
        "plt.hist([len(s) for s in sentences], bins=50)\n",
        "plt.xlabel('length of samples')\n",
        "plt.ylabel('number of samples')\n",
        "plt.show()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "샘플의 최대 길이 : 271\n",
            "샘플의 평균 길이 : 25.722024\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XdVk6vmcsxH"
      },
      "source": [
        "# 케라스 토크나이저를 다음과 같이 함수로 구현\n",
        "def tokenize(samples):\n",
        "  tokenizer = Tokenizer()\n",
        "  tokenizer.fit_on_texts(samples)\n",
        "  return tokenizer"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-acp-lvQc7XU"
      },
      "source": [
        "# 문장 데이터에 대해서는 src_tokenizer, 레이블에 해당되는 품사 태깅 정보에 대해서는 tar_tokenizer 사용\n",
        "src_tokenizer = tokenize(sentences)\n",
        "tar_tokenizer = tokenize(pos_tags)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5gkfsAJ-c-ji",
        "outputId": "c7935fcf-76b9-4508-eb78-4e8cba280d89"
      },
      "source": [
        "# 단어 집합과 품사 태깅 정보 집합의 크기 확인\n",
        "vocab_size = len(src_tokenizer.word_index) + 1\n",
        "tag_size = len(tar_tokenizer.word_index) + 1\n",
        "print('단어 집합의 크기 : {}'.format(vocab_size))\n",
        "print('태깅 정보 집합의 크기 : {}'.format(tag_size))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "단어 집합의 크기 : 11388\n",
            "태깅 정보 집합의 크기 : 47\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OG0pQOpsdRZX"
      },
      "source": [
        "# 정수 인코딩\n",
        "X_train = src_tokenizer.texts_to_sequences(sentences)\n",
        "y_train = tar_tokenizer.texts_to_sequences(pos_tags)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "contEKJ-deyW",
        "outputId": "1ad2d80b-4056-4b72-c415-a19fc393079d"
      },
      "source": [
        "print(X_train[:2])\n",
        "print(y_train[:2])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[5601, 3746, 1, 2024, 86, 331, 1, 46, 2405, 2, 131, 27, 6, 2025, 332, 459, 2026, 3], [31, 3746, 20, 177, 4, 5602, 2915, 1, 2, 2916, 637, 147, 3]]\n",
            "[[3, 3, 8, 10, 6, 7, 8, 21, 13, 4, 1, 2, 4, 7, 1, 3, 10, 9], [3, 3, 17, 1, 2, 3, 3, 8, 4, 3, 19, 1, 9]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNKKMMZCdhGs"
      },
      "source": [
        "# 샘플의 모든 길이를 임의로 150으로 맞춤\n",
        "# 케라스의 pad_sequences()\n",
        "max_len = 150\n",
        "X_train = pad_sequences(X_train, padding='post', maxlen=max_len)\n",
        "y_train = pad_sequences(y_train, padding='post', maxlen=max_len)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UAUNSot4dr0R"
      },
      "source": [
        "# 모든 샘플의 길이가 150\n",
        "# 이제 훈련 데이터 비율로 분리\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.2, random_state=777)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Iot9SQUdzb9"
      },
      "source": [
        "# 레이블에 해당되는 태깅 정보에 대해서 원-핫 인코딩 수행\n",
        "y_train = to_categorical(y_train, num_classes=tag_size)\n",
        "y_test = to_categorical(y_test, num_classes=tag_size)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fs7yuyvjd7Xu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ec389f7-bd88-4263-b53b-3cecb8fa292c"
      },
      "source": [
        "# 각 데이터에 대한 크기 확인\n",
        "print('훈련 샘플 문장의 크기 : {}'.format(X_train.shape))\n",
        "print('훈련 샘플 레이블의 크기 : {}'.format(y_train.shape))\n",
        "print('테스트 샘플 문장의 크기 : {}'.format(X_test.shape))\n",
        "print('테스트 샘플 레이블의 크기 : {}'.format(y_test.shape))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "훈련 샘플 문장의 크기 : (3131, 150)\n",
            "훈련 샘플 레이블의 크기 : (3131, 150, 47)\n",
            "테스트 샘플 문장의 크기 : (783, 150)\n",
            "테스트 샘플 레이블의 크기 : (783, 150, 47)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FggM8FtTerfZ"
      },
      "source": [
        "### 2. 양방향 LSTM(Bi-directional LSTM)으로 POS Tagger 만들기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "am5wqjXYeq_x"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, InputLayer, Bidirectional, TimeDistributed, Embedding\n",
        "from tensorflow.keras.optimizers import Adam"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a31VrxEie_cc"
      },
      "source": [
        "embedding_dim = 128\n",
        "hidden_units = 128\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, embedding_dim, mask_zero=True))\n",
        "model.add(Bidirectional(LSTM(hidden_units, return_sequences=True)))\n",
        "model.add(TimeDistributed(Dense(tag_size, activation=('softmax'))))\n",
        "model.compile(loss='categorical_crossentropy', optimizer=Adam(0.001), metrics=['accuracy'])"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fyexIWq1f-O0"
      },
      "source": [
        "TimeDistributed는 many 출력을 가진 모델에 사용\n",
        "* 각 시간별로 출력되는 모든 출력값에 대해서 cost를 구해서 gradient를 구함\n",
        "\n",
        "* one 출력 모델은 최종 출력 하나에서 gradient를 계산해 앞으로 보내고,\n",
        "many의 경우는 중간 출력 결과에 대해서도 gradient 계산"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9rzn54sfTxO",
        "outputId": "77248d45-2b5e-47de-bcf6-e15e0fcc0b80"
      },
      "source": [
        "model.fit(X_train, y_train, batch_size=128, epochs=6, validation_data=(X_test, y_test))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/6\n",
            "25/25 [==============================] - 46s 2s/step - loss: 0.5942 - accuracy: 0.1348 - val_loss: 0.5160 - val_accuracy: 0.1870\n",
            "Epoch 2/6\n",
            "25/25 [==============================] - 36s 1s/step - loss: 0.5036 - accuracy: 0.1915 - val_loss: 0.4810 - val_accuracy: 0.2184\n",
            "Epoch 3/6\n",
            "25/25 [==============================] - 36s 1s/step - loss: 0.4524 - accuracy: 0.3604 - val_loss: 0.4014 - val_accuracy: 0.4558\n",
            "Epoch 4/6\n",
            "25/25 [==============================] - 37s 1s/step - loss: 0.3407 - accuracy: 0.5173 - val_loss: 0.2744 - val_accuracy: 0.6004\n",
            "Epoch 5/6\n",
            "25/25 [==============================] - 36s 1s/step - loss: 0.2207 - accuracy: 0.6991 - val_loss: 0.1719 - val_accuracy: 0.7793\n",
            "Epoch 6/6\n",
            "25/25 [==============================] - 36s 1s/step - loss: 0.1277 - accuracy: 0.8526 - val_loss: 0.1044 - val_accuracy: 0.8709\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fa0b7e83450>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12hrlCvmgEOF",
        "outputId": "d88a284e-97f9-4c7c-e462-b17336164208"
      },
      "source": [
        "print(\"\\n 테스트 정확도: %.4f\" % (model.evaluate(X_test, y_test)[1]))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "25/25 [==============================] - 3s 106ms/step - loss: 0.1044 - accuracy: 0.8709\n",
            "\n",
            " 테스트 정확도: 0.8709\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55tcyVYzhHl9"
      },
      "source": [
        "* 실제 맞추고 있는지를 특정 테스트 데이터를 주고 직접 출력해서 확인\n",
        "* 인덱스로부터 단어와 품사 태깅 정보를 리턴하는 index_to_word와 index_to_tag를 만들고 이를 이용하여 실제값과 예측값을 출력"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1jRqTFU8hSMV",
        "outputId": "ea2d3e92-dda2-4d99-f0d4-0cc21e1da687"
      },
      "source": [
        "index_to_word = src_tokenizer.index_word\n",
        "index_to_tag = tar_tokenizer.index_word\n",
        "\n",
        "i = 10 # 확인하고 싶은 테스트용 샘플 인덱스\n",
        "y_predicted = model.predict(np.array([X_test[i]])) #입력한 테스트용 샘플에 대해서 예측 y를 리턴\n",
        "y_predicted = np.argmax(y_predicted, axis=-1) # 원-핫 인코딩을 다시 정수 인코딩으로 변경함\n",
        "true = np.argmax(y_test[i], -1) # 원핫 인코딩을 다시 정수 인코딩으로 변경\n",
        "\n",
        "print(\"{:15}|{:5}|{}\".format(\"단어\", \"실제값\", \"예측값\"))\n",
        "print(35 * \"-\")\n",
        "\n",
        "for w, t, pred in zip(X_test[i], true, y_predicted[0]):\n",
        "    if w != 0: # PAD값은 제외함.\n",
        "        print(\"{:17}: {:7} {}\".format(index_to_word[w], index_to_tag[t].upper(), index_to_tag[pred].upper()))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "단어             |실제값  |예측값\n",
            "-----------------------------------\n",
            "in               : IN      IN\n",
            "addition         : NN      NN\n",
            ",                : ,       ,\n",
            "buick            : NNP     NNP\n",
            "is               : VBZ     VBZ\n",
            "a                : DT      DT\n",
            "relatively       : RB      JJ\n",
            "respected        : VBN     VBN\n",
            "nameplate        : NN      NN\n",
            "among            : IN      IN\n",
            "american         : NNP     NNP\n",
            "express          : NNP     NNP\n",
            "card             : NN      NN\n",
            "holders          : NNS     NNS\n",
            ",                : ,       ,\n",
            "says             : VBZ     VBZ\n",
            "0                : -NONE-  -NONE-\n",
            "*t*-1            : -NONE-  -NONE-\n",
            "an               : DT      DT\n",
            "american         : NNP     NNP\n",
            "express          : NNP     NNP\n",
            "spokeswoman      : NN      NN\n",
            ".                : .       .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7t1aOnorRWY"
      },
      "source": [
        "### 3. 양방향 LSTM + CRF(Bidirectional LSTM + CRF)로 POS Tagger 만들기\n",
        "* 양방향 LSTM에 추가적으로 사용하면 성능을 높일 수 있다고 알려진 CRF layer를 추가하여 실습 진행\n",
        "* tf2crf라는 패키지 설치"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OO0M10N5hrYz",
        "outputId": "4d416aeb-db47-49b5-9034-c6f31a62ee2d"
      },
      "source": [
        "!pip install tf2crf"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tf2crf\n",
            "  Downloading tf2crf-0.1.33-py2.py3-none-any.whl (7.3 kB)\n",
            "Requirement already satisfied: tensorflow>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from tf2crf) (2.6.0)\n",
            "Collecting tensorflow-addons>=0.8.2\n",
            "  Downloading tensorflow_addons-0.14.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 5.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio<2.0,>=1.37.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->tf2crf) (1.41.0)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->tf2crf) (1.15.0)\n",
            "Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->tf2crf) (2.6.0)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->tf2crf) (1.6.3)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->tf2crf) (0.12.0)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->tf2crf) (1.1.0)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->tf2crf) (3.3.0)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->tf2crf) (0.37.0)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->tf2crf) (1.12)\n",
            "Requirement already satisfied: tensorflow-estimator~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->tf2crf) (2.6.0)\n",
            "Requirement already satisfied: clang~=5.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->tf2crf) (5.0)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->tf2crf) (3.7.4.3)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->tf2crf) (1.1.2)\n",
            "Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->tf2crf) (0.4.0)\n",
            "Requirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->tf2crf) (3.1.0)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->tf2crf) (0.2.0)\n",
            "Requirement already satisfied: keras~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->tf2crf) (2.6.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->tf2crf) (3.17.3)\n",
            "Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->tf2crf) (1.19.5)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->tf2crf) (1.12.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0->tensorflow>=2.1.0->tf2crf) (1.5.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.1.0->tf2crf) (0.6.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.1.0->tf2crf) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.1.0->tf2crf) (0.4.6)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.1.0->tf2crf) (57.4.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.1.0->tf2crf) (1.35.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.1.0->tf2crf) (2.23.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.1.0->tf2crf) (3.3.4)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.1.0->tf2crf) (1.8.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow>=2.1.0->tf2crf) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow>=2.1.0->tf2crf) (4.7.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow>=2.1.0->tf2crf) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow>=2.1.0->tf2crf) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow>=2.1.0->tf2crf) (4.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow>=2.1.0->tf2crf) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow>=2.1.0->tf2crf) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow>=2.1.0->tf2crf) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow>=2.1.0->tf2crf) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow>=2.1.0->tf2crf) (2021.5.30)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow>=2.1.0->tf2crf) (3.1.1)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons>=0.8.2->tf2crf) (2.7.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.6->tensorflow>=2.1.0->tf2crf) (3.6.0)\n",
            "Installing collected packages: tensorflow-addons, tf2crf\n",
            "Successfully installed tensorflow-addons-0.14.0 tf2crf-0.1.33\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMgnpur2reJA"
      },
      "source": [
        "# 이전 모델에 CRF layer를 추가한 새로운 모델 만들기\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, TimeDistributed, Dense, Input\n",
        "from tf2crf import CRF, ModelWithCRFLoss"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8qGBL7YAr2pR",
        "outputId": "1cc6574f-10e6-4875-f10b-6129e3c72c90"
      },
      "source": [
        "inputs = Input(shape=(None, ), dtype='int32')\n",
        "output = Embedding(vocab_size, 128, mask_zero=True)(inputs)\n",
        "output = Bidirectional(LSTM(128, return_sequences=True))(output)\n",
        "crf = CRF(tag_size) ##\n",
        "output = crf(output) ##\n",
        "base_model = Model(inputs, output)\n",
        "\n",
        "model = ModelWithCRFLoss(base_model, sparse_target = False) ##\n",
        "model.build(input_shape=(None, 22))\n",
        "model.compile(optimizer='adam')"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py:376: UserWarning: CRF decoding models have serialization issues in TF >=2.5 . Please see isse #2476\n",
            "  return py_builtins.overload_of(f)(*args)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nLtV_OZbsQ2O",
        "outputId": "5ed8d281-0797-486f-e7a2-c8e1728a1cf0"
      },
      "source": [
        "history = model.fit(X_train, y_train, batch_size = 32, epochs = 5, validation_split = 0.2, verbose = 1)\n",
        "\n",
        "'''\n",
        "함수 인자로 verbose가 있으면 함수 수행시 발생하는 상세한 정보들을 표준 출력으로 자세히 내보낼 것인가를 나타냅니다. \n",
        "보통 0 은 출력하지 않고, 1은 자세히, 2는 함축적인 정보만 출력하는 형태\n",
        "'''"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py:376: UserWarning: CRF decoding models have serialization issues in TF >=2.5 . Please see isse #2476\n",
            "  return py_builtins.overload_of(f)(*args)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "79/79 [==============================] - 71s 763ms/step - loss: 76.7682 - accuracy: 0.2125 - val_loss_val: 57.4945 - val_val_accuracy: 0.4231\n",
            "Epoch 2/5\n",
            "79/79 [==============================] - 57s 726ms/step - loss: 37.5177 - accuracy: 0.6161 - val_loss_val: 21.0109 - val_val_accuracy: 0.8043\n",
            "Epoch 3/5\n",
            "79/79 [==============================] - 57s 728ms/step - loss: 12.3420 - accuracy: 0.8963 - val_loss_val: 9.6588 - val_val_accuracy: 0.9020\n",
            "Epoch 4/5\n",
            "79/79 [==============================] - 58s 730ms/step - loss: 5.1287 - accuracy: 0.9570 - val_loss_val: 7.1016 - val_val_accuracy: 0.9222\n",
            "Epoch 5/5\n",
            "79/79 [==============================] - 57s 726ms/step - loss: 2.9149 - accuracy: 0.9740 - val_loss_val: 6.3677 - val_val_accuracy: 0.9240\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bd-l67CsX4I"
      },
      "source": [
        "print(\"\\n 테스트 정확도: %.4f\" % (model.evaluate(X_test, y_test)[1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wmfs8EnUsqX7"
      },
      "source": [
        "## 3) 개체명 인식 Named Entity Recognition\n",
        "* 코퍼스로부터 각 개체 entity의 유형을 인식하는 개체명 인식\n",
        "* 개체명 인식을 사용하면 코퍼스로부터 어떤 단어가 사람, 장소, 조직 등을 의미하는 단어인지 찾을 수 있음\n",
        "\n",
        "### 1. 개체명 인식 Named Entity Recognition이란?\n",
        "* 개체명 인식이란, 말 그대로 이름을 가진 개체 named entity를 인식하겠다는 것을 의미\n",
        "* 좀 더 쉽게 설명하면, 어떤 이름을 의미하는 단어를 보고는 그 단어가 어떤 유형인지 인식하는 것\n",
        "* ex) 유정이는 2018년에 골드만삭스에 입사했다. \n",
        "  > person, organization, time에 대해서 개체명 인식을 수행하는 모델이라면 다음과 같은 결과를 보여줌. 유정 - 사람, 2018년 - 시간, 골드만삭스 - 조직\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NgsW8x30tRuS"
      },
      "source": [
        "* 개체명 인식 모델이 개체명을 인식하기 위해서는 보통 **전처리 과정**이 필요. 개체명 모델에 따라서는 품사 정보(POS Tagging, Part-Of-Speech Tagging)를 입력으로 요구하기도 함"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vMOe_QTtdAg"
      },
      "source": [
        "## 2. NLTK를 이용한 개체명 인식(Named Entity Recognition using NLTK)\n",
        "* NLTK에서는 개체명 인식기 (NER Chunker)를 지원하고 있으므로, 별도 개체명 인식기를 구현할 필요 없이 NLTK를 사용해서 개체명 인식을 수행 가능"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pgMpZezVtQ27",
        "outputId": "5f4b4f65-9b5d-4678-a847-11973308796b"
      },
      "source": [
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m5bZ-Ilhtpw3",
        "outputId": "78dc163b-f967-473e-e62d-ff2af15d0d8f"
      },
      "source": [
        "from nltk import word_tokenize, pos_tag, ne_chunk\n",
        "sentence = \"James is working at Disney in London\"\n",
        "sentence = pos_tag(word_tokenize(sentence))\n",
        "print(sentence) #토큰화와 품사 태깅을 동시 수행"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('James', 'NNP'), ('is', 'VBZ'), ('working', 'VBG'), ('at', 'IN'), ('Disney', 'NNP'), ('in', 'IN'), ('London', 'NNP')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U3dBSXXrt2O1",
        "outputId": "0720e101-9749-4ec6-ec7c-2a07aeb8b3c2"
      },
      "source": [
        "# 개체명 인식\n",
        "sentence = ne_chunk(sentence)\n",
        "print(sentence)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  (PERSON James/NNP)\n",
            "  is/VBZ\n",
            "  working/VBG\n",
            "  at/IN\n",
            "  (ORGANIZATION Disney/NNP)\n",
            "  in/IN\n",
            "  (GPE London/NNP))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3p7KKc_9uLyJ"
      },
      "source": [
        "* **ne_chunk는 개체명을 태깅하기 위해서 앞서 품사 태깅(pos_tag)이 수행되어야 함**\n",
        "* James는 PERSON, Disney는 ORGANIZATION, London은 위치GPE라고 정상적으로 개체명 인식이 수행된 것"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0zM4RoPuemv"
      },
      "source": [
        "## 4) 개체명 인식의 BIO 표현 이해하기\n",
        "* 개체명 인식은 챗봇 등에서 필요로 하는 주요 전처리 작업이지만, 그 자체로도 까다로운 작업\n",
        "* 도메인 또는 목적에 특화되도록 개체명 인식을 정확하게 하는 방법 중 하나는 기존에 공개된 개체명 인식기를 사용하는 것이 아니라, ** 직접 목적에 맞는 데이터를 준비하여 기계를 훈련시켜 모델을 만드는 방법**\n",
        "* 여기서는 **양방향 LSTM을 이용해서 개체명 인식기를 만들어 봄**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-x0tS7oYux4r"
      },
      "source": [
        "### 1. BIO 표현\n",
        "* 개체명 인식에서 코퍼스로부터 개체명을 인식하기 위한 방법으로는 여러 방법이 있지만, 가장 보편적인 방법은 IOB(또는 BIO) 방법\n",
        "* B: 개체명이 begin, I: 개체명의 inside(내부 부분), O: 개체명이 outside(아닌 부분)\n",
        "\n",
        "```\n",
        "# 해리포터, 메가박스\n",
        "\n",
        "해 B-movie\n",
        "리 I-movie\n",
        "포 I-movie\n",
        "터 I-movie\n",
        "보 O\n",
        "러 O\n",
        "메 B-theater\n",
        "가 I-theater\n",
        "박 I-theater\n",
        "스 I-theater\n",
        "가 O\n",
        "자 O\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J52O7x2NuyXS"
      },
      "source": [
        "### 2. 개체명 인식 데이터 이해하기\n",
        "* 실습을 통한 양방향 LSTM을 이용한 개체명 인식에 대해서 더 자세히 알아보기\n",
        "* CONLL2003은 개체명 인식을 위한 전통적인 영어 데이터셋. 해당 데이터를 가지고 훈련하여 개체명 인식 모델 만들기\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8WuP-SG5v3ig",
        "outputId": "7e67efd9-ecbf-4b20-e9b5-84ea9b80ef06"
      },
      "source": [
        "!git clone https://github.com/Franck-Dernoncourt/NeuroNER.git"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'NeuroNER'...\n",
            "remote: Enumerating objects: 675, done.\u001b[K\n",
            "remote: Counting objects:  33% (1/3)\u001b[K\rremote: Counting objects:  66% (2/3)\u001b[K\rremote: Counting objects: 100% (3/3)\u001b[K\rremote: Counting objects: 100% (3/3), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 675 (delta 0), reused 0 (delta 0), pack-reused 672\u001b[K\n",
            "Receiving objects: 100% (675/675), 123.78 MiB | 33.13 MiB/s, done.\n",
            "Resolving deltas: 100% (394/394), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZghnZKwDv3sK",
        "outputId": "af689883-bb4a-41e6-bb0d-c7dbdb931b4f"
      },
      "source": [
        "!ls -ltr"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 8\n",
            "drwxr-xr-x 1 root root 4096 Nov  1 13:35 sample_data\n",
            "drwxr-xr-x 5 root root 4096 Nov  9 03:34 NeuroNER\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NClo2uGzU-3"
      },
      "source": [
        "import re\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NZVFcI3ytWn"
      },
      "source": [
        "# 위에서 설명한 개체명 인식 데이터를 읽어 전처리 수행\n",
        "files = 'NeuroNER/neuroner/data/conll2003/en/train.txt'\n",
        "\n",
        "f = open(files, 'r')\n",
        "tagged_sentences = []\n",
        "sentence = []\n",
        "\n",
        "for line in f:\n",
        "  if len(line) == 0 or line.startswith('-DOCSTART') or line[0]=='\\n':\n",
        "    if len(sentence) > 0:\n",
        "      tagged_sentences.append(sentence)\n",
        "      sentence = []\n",
        "    continue\n",
        "  splits = line.split(' ') #공백 기준으로 속성 구분\n",
        "  splits[-1] = re.sub(r'\\n', '', splits[-1]) #줄바꿈 표시 \\n을 제거\n",
        "  word = splits[0].lower() # 단어들은 소문자로 바꿔서 저장\n",
        "  sentence.append([word, splits[-1]]) #단어와 개체명 태깅만 기록"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_m6xQzowuJUX",
        "outputId": "84b1dec3-995a-47b9-bc1c-5526515eab07"
      },
      "source": [
        "print(\"전체 샘플 개수: \", len(tagged_sentences)) # 전체 샘플의 개수 출력"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "전체 샘플 개수:  14041\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71gEQaFyze11",
        "outputId": "58799cd9-dbff-40dd-d756-0392c34819f4"
      },
      "source": [
        "print(tagged_sentences[0]) # 첫번째 샘플 출력"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['eu', 'B-ORG'], ['rejects', 'O'], ['german', 'B-MISC'], ['call', 'O'], ['to', 'O'], ['boycott', 'O'], ['british', 'B-MISC'], ['lamb', 'O'], ['.', 'O']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OmW7a9zqzjHE"
      },
      "source": [
        "* 그런데 훈련을 시키려면 훈련 데이터에서 단어에 해당되는 부분과 개체명 태깅 정보에 해당되는 부분을 분리시켜야 함\n",
        "* ex) eu와 rejects는 같이, B-ORG와 O를 같이 저장\n",
        "  * zip() 함수 활용. 동일한 개수를 가지는 시퀀스 자료형에서 각 순서에 등장하는 원소들끼리 묶어주는 역할."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dSA5irPXzfdj"
      },
      "source": [
        "sentences, ner_tags = [], []\n",
        "for tagged_sentence in tagged_sentences: #14,041개의 문장 샘플을 1개씩 불러옴\n",
        "  sentence, tag_info = zip(*tagged_sentence) #각 샘플에서 단어들은 sentence에 개체명 태깅 정보들은 tag_info에 저장\n",
        "  sentences.append(list(sentence)) # 각 샘플에서 단어 정보만 저장\n",
        "  ner_tags.append(list(tag_info)) # 각 샘플에서 개체명 태깅 정보만 저장\n",
        "\n",
        "# 각 문장 샘플에 대해서 단어는 sentences에 태깅 정보는 ner_tags에 저장\n",
        "# 임의로 첫번재 문장 샘플 출력"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "de-ceAKLzfgW",
        "outputId": "9f59cc1e-d8f7-4cd4-8218-4914ce489dcc"
      },
      "source": [
        "print(sentences[0])\n",
        "print(ner_tags[0])"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['eu', 'rejects', 'german', 'call', 'to', 'boycott', 'british', 'lamb', '.']\n",
            "['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eCsAKWIwzfjl",
        "outputId": "ccdb10a3-8c2d-44b8-cb6d-38dfce22f54e"
      },
      "source": [
        "# sentences는 예측을 위한 X, ner_tags는 예측 대상인 y\n",
        "print(sentences[12])\n",
        "print(ner_tags[12])"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['only', 'france', 'and', 'britain', 'backed', 'fischler', \"'s\", 'proposal', '.']\n",
            "['O', 'B-LOC', 'O', 'B-LOC', 'O', 'B-PER', 'O', 'O', 'O']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "id": "OkKmAryh0VCb",
        "outputId": "3b0a7036-d2ef-4601-c049-452226f83907"
      },
      "source": [
        "# 전체 데이터의 길이 분포\n",
        "print('샘플의 최대 길이 : %d' % max(len(l) for l in sentences))\n",
        "print('샘플의 평균 길이 : %f' % (sum(map(len, sentences))/len(sentences)))\n",
        "plt.hist([len(s) for s in sentences], bins=50)\n",
        "plt.xlabel('length of samples')\n",
        "plt.ylabel('number of samples')\n",
        "plt.show()"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "샘플의 최대 길이 : 113\n",
            "샘플의 평균 길이 : 14.501887\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4l92V74Y0oUE"
      },
      "source": [
        "# 케라스 토크나이저를 통해서 토큰화와 정수 인코딩 진행\n",
        "# 문장 데이터에 있는 모든 단어를 사용하지 않고 높은 빈도수를 가진 상위 약 4000개의 단어만을 사용\n",
        "\n",
        "# 토큰화\n",
        "max_words = 4000\n",
        "src_tokenizer = Tokenizer(num_words=max_words, oov_token='OOV') #문장 데이터\n",
        "src_tokenizer.fit_on_texts(sentences)\n",
        "\n",
        "tar_tokenizer = Tokenizer() #레이블에 해당되는 개체명 태깅 정보\n",
        "tar_tokenizer.fit_on_texts(ner_tags)"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43R5U06V09Qi",
        "outputId": "a0040bb5-1c20-4930-a7d0-e03f73f953ba"
      },
      "source": [
        "vocab_size = max_words\n",
        "tag_size = len(tar_tokenizer.word_index) + 1\n",
        "print('단어 집합의 크기 : {}'.format(vocab_size))\n",
        "print('개체명 태깅 정보 집합의 크기 : {}'.format(tag_size))"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "단어 집합의 크기 : 4000\n",
            "개체명 태깅 정보 집합의 크기 : 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i04PHuL51HvQ"
      },
      "source": [
        "# 정수 인코딩\n",
        "X_train = src_tokenizer.texts_to_sequences(sentences)\n",
        "y_train = tar_tokenizer.texts_to_sequences(ner_tags)"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3hWmcMeY1USH",
        "outputId": "44c53814-7011-4a41-eab0-9b54f7f9b6a1"
      },
      "source": [
        "print(X_train[0])\n",
        "print(y_train[0])"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[989, 1, 205, 629, 7, 3939, 216, 1, 3]\n",
            "[4, 1, 7, 1, 1, 1, 7, 1, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYUIA25Z1Vin"
      },
      "source": [
        "# 문장 데이터에 대해서는 일부 단어가 'OOV'로 대체된 상황\n",
        "# 확인 위해 다시 디코딩(정수->텍스트 데이터) 작업을 함\n",
        "# 이를 위해 인덱스로부터 단어를 리턴하는 index_to_word를 만듦\n",
        "\n",
        "index_to_word = src_tokenizer.index_word\n",
        "index_to_ner = tar_tokenizer.index_word"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LAsOdxdg1kWZ",
        "outputId": "20999e1c-67a9-4224-c42f-04579b3ce149"
      },
      "source": [
        "# 정수 인코딩 된 첫번째 문장 다시 디코딩\n",
        "decoded = []\n",
        "for index in X_train[0]: #첫번째 샘플 안의 인덱스들에 대해서\n",
        "  decoded.append(index_to_word[index]) #다시 단어로 변환\n",
        "\n",
        "print('기존 문장 : {}'.format(sentences[0]))\n",
        "print('빈도수가 낮은 단어가 OOV 처리된 문장 : {}'.format(decoded))"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "기존 문장 : ['eu', 'rejects', 'german', 'call', 'to', 'boycott', 'british', 'lamb', '.']\n",
            "빈도수가 낮은 단어가 OOV 처리된 문장 : ['eu', 'OOV', 'german', 'call', 'to', 'boycott', 'british', 'OOV', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R45n7zYI1s8Y"
      },
      "source": [
        "# 샘플들의 모든 길이를 임의 70으로 맞추기\n",
        "# 케라스의 pad_sequences()\n",
        "\n",
        "max_len = 70\n",
        "X_train = pad_sequences(X_train, padding='post', maxlen=max_len)\n",
        "y_train = pad_sequences(y_train, padding='post', maxlen=max_len)"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nEmISAxd19Uz"
      },
      "source": [
        "# 데이터 분리\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size = 0.2, random_state=777)"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aizyh8iy2EaA"
      },
      "source": [
        "# 레이블에 해당하는 태깅 정보에 대해서 원-핫 인코딩 수행\n",
        "y_train = to_categorical(y_train, num_classes=tag_size)\n",
        "y_test = to_categorical(y_test, num_classes=tag_size)"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CKPFgwQ92O-2",
        "outputId": "90baf2c9-cda5-4187-9035-b42a86599da4"
      },
      "source": [
        "# 각 데이터에 대한 크기\n",
        "print('훈련 샘플 문장의 크기 : {}'.format(X_train.shape))\n",
        "print('훈련 샘플 레이블의 크기 : {}'.format(y_train.shape))\n",
        "print('테스트 샘플 문장의 크기 : {}'.format(X_test.shape))\n",
        "print('테스트 샘플 레이블의 크기 : {}'.format(y_test.shape))"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "훈련 샘플 문장의 크기 : (11232, 70)\n",
            "훈련 샘플 레이블의 크기 : (11232, 70, 10)\n",
            "테스트 샘플 문장의 크기 : (2809, 70)\n",
            "테스트 샘플 레이블의 크기 : (2809, 70, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PGyqyf92UsS"
      },
      "source": [
        "#### 4. 양방향 LSTM(Bi-directional LSTM)으로 개체명 인식기 만들기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AYsomp2n2R6W"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, LSTM, Bidirectional, TimeDistributed\n",
        "from tensorflow.keras.optimizers import Adam"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pD2rIaU2sEt"
      },
      "source": [
        "# 모델\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=vocab_size, output_dim=128, input_length=max_len, mask_zero=True))\n",
        "model.add(Bidirectional(LSTM(256, return_sequences=True)))\n",
        "model.add(TimeDistributed(Dense(tag_size, activation='softmax')))\n",
        "\n",
        "# 패딩을 하느라 숫자 0이 많아질 경우, Embedding()에 mask_zero=Trueㄹ르 설정하여\n",
        "# 데이터에서 숫자 0은 패딩을 의미하므로 연산에서 제외시킨다는 옵션 줌"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBpXmzoE2sPX"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer=Adam(0.001), metrics=['accuracy'])"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hr73i_q13Pjt",
        "outputId": "d9083944-1b3e-4483-8fda-82ba76ec56f9"
      },
      "source": [
        "model.fit(X_train, y_train, batch_size=128, epochs=8, validation_data=(X_test, y_test))"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/8\n",
            "88/88 [==============================] - 134s 1s/step - loss: 0.1833 - accuracy: 0.8266 - val_loss: 0.1214 - val_accuracy: 0.8334\n",
            "Epoch 2/8\n",
            "88/88 [==============================] - 124s 1s/step - loss: 0.0968 - accuracy: 0.8598 - val_loss: 0.0760 - val_accuracy: 0.8848\n",
            "Epoch 3/8\n",
            "88/88 [==============================] - 123s 1s/step - loss: 0.0658 - accuracy: 0.9054 - val_loss: 0.0541 - val_accuracy: 0.9231\n",
            "Epoch 4/8\n",
            "88/88 [==============================] - 121s 1s/step - loss: 0.0470 - accuracy: 0.9335 - val_loss: 0.0419 - val_accuracy: 0.9418\n",
            "Epoch 5/8\n",
            "88/88 [==============================] - 115s 1s/step - loss: 0.0360 - accuracy: 0.9492 - val_loss: 0.0363 - val_accuracy: 0.9496\n",
            "Epoch 6/8\n",
            "88/88 [==============================] - 112s 1s/step - loss: 0.0297 - accuracy: 0.9577 - val_loss: 0.0339 - val_accuracy: 0.9539\n",
            "Epoch 7/8\n",
            "88/88 [==============================] - 113s 1s/step - loss: 0.0253 - accuracy: 0.9637 - val_loss: 0.0332 - val_accuracy: 0.9543\n",
            "Epoch 8/8\n",
            "88/88 [==============================] - 113s 1s/step - loss: 0.0223 - accuracy: 0.9677 - val_loss: 0.0325 - val_accuracy: 0.9564\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fa0b8d73e10>"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sT0vf3CO3XBM",
        "outputId": "77c4dc20-1008-4161-9219-0306483a34f3"
      },
      "source": [
        "print(\"\\n 테스트 정확도: %.4f\" % (model.evaluate(X_test, y_test)[1]))"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "88/88 [==============================] - 13s 148ms/step - loss: 0.0325 - accuracy: 0.9564\n",
            "\n",
            " 테스트 정확도: 0.9564\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N-B9O_QD3_PR",
        "outputId": "1b90a5de-446b-498e-dc36-62e2b50e745e"
      },
      "source": [
        "# 실제로 맞추고 있는지를 테스트 데이터를 주고 직접 실제값과 비교\n",
        "# 앞서 만들어 둔 인덱스로부터 단어와 개체명 태깅 정보를 리턴하는 index_to_word와 index_to_ner을 사용하여\n",
        "# 테스트 데이터에 대한 예측값과 실제값을 비교 출력\n",
        "\n",
        "i=10 # 확인하고 싶은 테스트용 샘플의 인덱스.\n",
        "y_predicted = model.predict(np.array([X_test[i]])) # 입력한 테스트용 샘플에 대해서 예측 y를 리턴\n",
        "y_predicted = np.argmax(y_predicted, axis=-1) # 원-핫 인코딩을 다시 정수 인코딩으로 변경함.\n",
        "true = np.argmax(y_test[i], -1) # 원-핫 인코딩을 다시 정수 인코딩으로 변경함.\n",
        "\n",
        "print(\"{:15}|{:5}|{}\".format(\"단어\", \"실제값\", \"예측값\"))\n",
        "print(35 * \"-\")\n",
        "\n",
        "for w, t, pred in zip(X_test[i], true, y_predicted[0]):\n",
        "    if w != 0: # PAD값은 제외함.\n",
        "        print(\"{:17}: {:7} {}\".format(index_to_word[w], index_to_ner[t].upper(), index_to_ner[pred].upper()))"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "단어             |실제값  |예측값\n",
            "-----------------------------------\n",
            "sarah            : B-PER   B-PER\n",
            "brady            : I-PER   I-PER\n",
            ",                : O       O\n",
            "whose            : O       O\n",
            "republican       : B-MISC  B-MISC\n",
            "husband          : O       O\n",
            "was              : O       O\n",
            "OOV              : O       B-PER\n",
            "OOV              : O       O\n",
            "in               : O       O\n",
            "an               : O       O\n",
            "OOV              : O       O\n",
            "attempt          : O       O\n",
            "on               : O       O\n",
            "president        : O       O\n",
            "ronald           : B-PER   B-PER\n",
            "reagan           : I-PER   I-PER\n",
            ",                : O       O\n",
            "took             : O       O\n",
            "centre           : O       O\n",
            "stage            : O       O\n",
            "at               : O       O\n",
            "the              : O       O\n",
            "democratic       : B-MISC  B-MISC\n",
            "national         : I-MISC  I-MISC\n",
            "convention       : I-MISC  I-MISC\n",
            "on               : O       O\n",
            "monday           : O       O\n",
            "night            : O       O\n",
            "to               : O       O\n",
            "OOV              : O       O\n",
            "president        : O       O\n",
            "bill             : B-PER   B-PER\n",
            "clinton          : I-PER   I-PER\n",
            "'s               : O       O\n",
            "gun              : O       O\n",
            "control          : O       O\n",
            "efforts          : O       O\n",
            ".                : O       O\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6i0JDn64C17"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}