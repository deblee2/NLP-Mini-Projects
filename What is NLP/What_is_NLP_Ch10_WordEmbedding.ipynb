{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "What_is_NLP_Ch10_WordEmbedding.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-r5A2ttu8j5k"
      },
      "source": [
        "## 01) Word Embedding\n",
        "* 단어를 벡터로 표현하는 방법\n",
        "* 단어를 밀집 표현으로 변환\n",
        "\n",
        "### 1. 희소 표현 Sparse Representation\n",
        "* 정의: 벡터/행렬의 값이 대부분이 0으로 표현되는 방법\n",
        "  * 원-핫 벡터는 희소 벡터\n",
        "* 문제점: 단어 개수 늘어나면 벡터의 차원이 한없이 커짐.. 공간적 낭비 + 단어의 의미 담지 못함\n",
        "\n",
        "### 2. 밀집 표현 Dense Representation\n",
        "  * 벡터의 차원을 단어 집합의 크기로 상정하지 않음\n",
        "  * 사용자가 설정한 값으로 모든 단어의 벡터 표현의 차원을 맞춤\n",
        "  * 0과 1만이 아니라 실수값 가짐\n",
        "\n",
        "### 3. 워드 임베딩\n",
        "  * 단어를 dense vector의 형태로 표현하는 방법\n",
        "  * Aka Embedding vector (밀집 벡터를 워드 임베딩 과정을 통해 나온 결과)\n",
        "  * ex) LSA, Word2Vec, FastText, Glove"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEayKtDL9a6J"
      },
      "source": [
        "## 02) Word2Vec\n",
        "* 단어 간 유사도를 반영할 수 있도록 단어의 의미 벡터화 해야됨\n",
        "### 1. 희소 표현 Sparse Representation\n",
        "  * 희소 벡터(ex. 원-핫 벡터)는 각 단어 간 유사성 표현 못함\n",
        "    * 단어의 '의미'를 다차원 공간에 벡터화. 이러한 표현 방법을 **분산 표현(Distributed represntation)**\n",
        "\n",
        "### 2. 분산 표현 Distributed Representation\n",
        "  * Based on 분포 가설 Distributional hypothesis: 비슷한 위치에서 등장하는 단어들은 비슷한 의미를 가짐\n",
        "  * 요약: 희소 차원은 고차원에 각 차원이 분리된 표현 방법, 분산 표현은 저차원에 단어의 의미를 여러 차원에다가 분산하여 표현. ==> 단어 간 유사도 계싼 가능\n",
        "\n",
        "### 3. CBOW(Continuous Bag of Words)\n",
        "  * Word2Vec은 CBOW와 Skip-Gram 두 가지\n",
        "  * CBOW: 주변에 있는 단어(context word)들을 가지고 중간에 있는 단어(center word) 예측\n",
        "  * Skip-Gram: 중간에 있는 단어로 주변 단어들을 예측  \n",
        "\n",
        "  * Window: 중심 단어를 예측하기 위해서 앞, 뒤로 몇 개의 단어를 볼지 범위\n",
        "  * Sliding window: Window를 계속 움직여서 주변 단어와 중심 단어 선택을 바꿔가며 학습을 위한 데이터셋 만들기\n",
        "\n",
        "  * 과정 설명: \n",
        "  1. 입력층의 입력으로서 앞, 뒤로 사용자가 정한 윈도우 크기 범위 안 주변 단어들의 원-핫 벡터가 들어감\n",
        "  2. 출력층에서 예측하고자 하는 '중간 단어의 원-핫 벡터'가 필요\n",
        "    * 이는 Word2Vec의 학습을 위해서 필요\n",
        "  * 추가: Word2Vec은 Deep Learning Model이 아님.\n",
        "    * 딥러닝: 입력층과 출력층 사이의 은닉층의 개수가 충분히 쌓인 신경망 학습\n",
        "    * Word2vec: 입력층과 출력층 사이 하나의 은닉층만 존재. Shallow Neural Network\n",
        "      * (+) 은닉층에 활성화 함수 존재하지 않으며, 룩업 테이블이라는 연산을 담당하는 층.. 그래서 투사층 projection layer이라고도 부름  \n",
        "\n",
        "  * 동작 메커니즘:\n",
        "  1. 투사층의 크기가 M(입력층은 VxM)\n",
        "  2. 입력층과 투사층 사이의 가중치 W는 VxM, 투사층과 출력층 사이의 가중치 W'는 MxV\n",
        "  * 투사층에서 평균을 구하는 부분은 **CBOW가 Skip-Gram과 다른 차이점**\n",
        "    * Skip-Gram은 입력이 중심 단어 하나. 투사층에서 벡터의 평균을 구하지 않음\n",
        "\n",
        "### 4. Skip-Gram\n",
        "  * 전반적으로 성능 더 좋음  \n",
        "\n",
        "### 5. NNLM vs. Word2Vec\n",
        "#### 차이점\n",
        "1. \n",
        "  * 신경망 언어 모델(NNLM): 언어 모델로 **다음 대상 예측**\n",
        "  * Word2Vec: 워드 임베딩 자체가 목적. **다음 단어가 아닌 중심 단어를 예측하여 학습**\n",
        "    * => NNLM은 예측 단어의 이전 단어들만 참고, Word2Vec은 예측 단어의 전, 후 단어들을 모두 참고\n",
        "2. Word2Vec은 NNLM에 존재하던 활성화 함수가 있는 은닉층 제거\n",
        "  * 투사층 다음에 바로 출력층으로 연결\n",
        "* (+) 속도에 차이나는 이유\n",
        "  * 계층적 소프트맥스(Hierarchical softmax)와 네거티브 샘플링(Negative sampling)\n",
        "    * Word2Vec은 출력층에서의 연산에서 V를 log(V)로 바꿔 배는 빠른 학습속도 가짐"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2SwSNDQGG96"
      },
      "source": [
        "## 03) 영어/한국어 Word2Vec 실습\n",
        "* gensim 패키지 이용\n",
        "\n",
        "### 1. 영어 Word2Vec 만들기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4PyTn_t99UHs"
      },
      "source": [
        "import urllib.request\n",
        "import zipfile\n",
        "from lxml import etree\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CKAkGr8lGVDw",
        "outputId": "d4c0cb30-5099-4845-e1a7-bbedac91a7e3"
      },
      "source": [
        "# 데이터 다운로드\n",
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/GaoleMeng/RNN-and-FFNN-textClassification/master/ted_en-20160408.xml\", filename=\"ted_en-20160408.xml\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('ted_en-20160408.xml', <http.client.HTTPMessage at 0x7fa4eebbe550>)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IdmVsTcwKMH_",
        "outputId": "6c37479a-185b-417f-d335-3bab546fff36"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_HDkD36GYxW"
      },
      "source": [
        "* 훈련 데이터 파일은 xml 문법으로 작성되어 있어 자연어 얻기 위해서는 전처리 필요\n",
        "  * <content>와 </content> 사이의 내용만\n",
        "* 배경음 나타내는 (Laughter), (Applause)도 제거"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PlUhejaEGX6Y"
      },
      "source": [
        "# 전처리 코드\n",
        "targetXML = open('ted_en-20160408.xml', 'r', encoding='UTF8')\n",
        "target_text = etree.parse(targetXML)\n",
        "\n",
        "# xml 파일로부터 <content>와 </content> 사이의 내용만 가져옴\n",
        "parse_text = '\\n'.join(target_text.xpath('//content/text()'))\n",
        "\n",
        "# 정규표현식의 sub 모듈로 content 중간의 (Audio), (Laughter) 등의 배경음 부분 제거\n",
        "# 해당 코드는 괄호로 구성된 내용 제거\n",
        "content_text = re.sub(r'\\([^)]*\\)', '', parse_text)\n",
        "\n",
        "# 입력 코퍼스에 대해서 NLTK를 이용하여 문장 토큰화 수행\n",
        "sent_text = sent_tokenize(content_text)\n",
        "\n",
        "# 각 문장에 대해서 구두점 제거, 대문자를 소문자로 변환\n",
        "normalized_text = []\n",
        "for string in sent_text:\n",
        "  tokens = re.sub(r\"[^a-z0-9]+\", \" \", string.lower())\n",
        "  normalized_text.append(tokens)\n",
        "\n",
        "# 각 문장에 대해서 NLTK를 이용하여 단어 토큰화를 수행\n",
        "result = [word_tokenize(sentence) for sentence in normalized_text]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73ed2shtKG9c",
        "outputId": "e5a75052-8aed-4b6f-ffd5-0a7d5c43647e"
      },
      "source": [
        "print('총 샘플의 개수 : {}'.format(len(result)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "총 샘플의 개수 : 273424\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "unUoUMzFKJWz",
        "outputId": "b998490d-a101-4640-f39f-160a49ec4f8c"
      },
      "source": [
        "# 샘플 3개만 출력\n",
        "for line in result[:3]:\n",
        "    print(line)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['here', 'are', 'two', 'reasons', 'companies', 'fail', 'they', 'only', 'do', 'more', 'of', 'the', 'same', 'or', 'they', 'only', 'do', 'what', 's', 'new']\n",
            "['to', 'me', 'the', 'real', 'real', 'solution', 'to', 'quality', 'growth', 'is', 'figuring', 'out', 'the', 'balance', 'between', 'two', 'activities', 'exploration', 'and', 'exploitation']\n",
            "['both', 'are', 'necessary', 'but', 'it', 'can', 'be', 'too', 'much', 'of', 'a', 'good', 'thing']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2Mhec_DKbjK"
      },
      "source": [
        "### 3) Word2Vec 훈련시키기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZpxsw_aKTu1"
      },
      "source": [
        "from gensim.models import Word2Vec\n",
        "from gensim.models import KeyedVectors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7DbTUIhKonw"
      },
      "source": [
        "Word2Vec의 하이퍼 파라미터값\n",
        "* size: 워드 벡터의 특징 값. 임베딩된 벡터의 차원\n",
        "* window: context window 크기\n",
        "* min_count: 단어 최소 빈도 수 제한\n",
        "* workers: 학습을 위한 프로세스 수\n",
        "* sg: 0은 cbow, 1은 skip-gram"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UuGAyd7eKg78"
      },
      "source": [
        "model = Word2Vec(sentences=result, size=100, window=5, min_count=5, workers=4, sg=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bE7URzgOKn4w",
        "outputId": "830d61c3-835f-4387-f2a7-454288c46cda"
      },
      "source": [
        "# Word2Vec에 대해서 학습 진행\n",
        "# 입력 단어에 대해서 가장 유사한 단어 출력\n",
        "\n",
        "model_result = model.wv.most_similar(\"man\")\n",
        "print(model_result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('woman', 0.8435317277908325), ('guy', 0.7985468506813049), ('lady', 0.7817904949188232), ('boy', 0.7606765031814575), ('gentleman', 0.7366722822189331), ('girl', 0.7296925187110901), ('soldier', 0.7029783725738525), ('kid', 0.7020815014839172), ('poet', 0.6805630326271057), ('king', 0.66712486743927)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udMayI8QK8pP"
      },
      "source": [
        "# Word2Vec 모델 저장하고 로드\n",
        "model.wv.save_word2vec_format('eng_w2v') # 모델 저장\n",
        "loaded_model = KeyedVectors.load_word2vec_format('eng_w2v') #모델 로드"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TVJ44x8CLJWo",
        "outputId": "9b645270-54fd-428d-d42c-ce1386797f42"
      },
      "source": [
        "# 로드한 모델에 대해서 다시 man과 유사한 단어 출력\n",
        "model_result = loaded_model.most_similar('man')\n",
        "print(model_result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('woman', 0.8435317277908325), ('guy', 0.7985468506813049), ('lady', 0.7817904949188232), ('boy', 0.7606765031814575), ('gentleman', 0.7366722822189331), ('girl', 0.7296925187110901), ('soldier', 0.7029783725738525), ('kid', 0.7020815014839172), ('poet', 0.6805630326271057), ('king', 0.66712486743927)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cv4GS4gWQAyK"
      },
      "source": [
        "### 2. 한국어 Word2Vec 만들기 (네이버 영화 리뷰)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uu6d8naLQb77",
        "outputId": "f945d64f-7806-4787-ea3d-c171caace028"
      },
      "source": [
        "!pip install konlpy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting konlpy\n",
            "  Downloading konlpy-0.5.2-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4 MB 5.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.19.5)\n",
            "Collecting beautifulsoup4==4.6.0\n",
            "  Downloading beautifulsoup4-4.6.0-py3-none-any.whl (86 kB)\n",
            "\u001b[K     |████████████████████████████████| 86 kB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tweepy>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (3.10.0)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n",
            "Collecting JPype1>=0.7.0\n",
            "  Downloading JPype1-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (448 kB)\n",
            "\u001b[K     |████████████████████████████████| 448 kB 65.5 MB/s \n",
            "\u001b[?25hCollecting colorama\n",
            "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "Installing collected packages: JPype1, colorama, beautifulsoup4, konlpy\n",
            "  Attempting uninstall: beautifulsoup4\n",
            "    Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "Successfully installed JPype1-1.3.0 beautifulsoup4-4.6.0 colorama-0.4.4 konlpy-0.5.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SzGnno4HLOWF"
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import urllib.request\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "from konlpy.tag import Okt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UtFrEV1iQK6j",
        "outputId": "2bcd2a5d-857f-455a-fbc4-2f6e6e9842eb"
      },
      "source": [
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings.txt\", filename=\"ratings.txt\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('ratings.txt', <http.client.HTTPMessage at 0x7fa4cb26d110>)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLmekUtiQMS2"
      },
      "source": [
        "train_data = pd.read_table('ratings.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "id": "YsdhDeH2QN1J",
        "outputId": "6edf8bb5-12d9-4f5d-9c4d-3f0754ff5e82"
      },
      "source": [
        "train_data[:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>document</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>8112052</td>\n",
              "      <td>어릴때보고 지금다시봐도 재밌어요ㅋㅋ</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>8132799</td>\n",
              "      <td>디자인을 배우는 학생으로, 외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4655635</td>\n",
              "      <td>폴리스스토리 시리즈는 1부터 뉴까지 버릴께 하나도 없음.. 최고.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9251303</td>\n",
              "      <td>와.. 연기가 진짜 개쩔구나.. 지루할거라고 생각했는데 몰입해서 봤다.. 그래 이런...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>10067386</td>\n",
              "      <td>안개 자욱한 밤하늘에 떠 있는 초승달 같은 영화.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         id                                           document  label\n",
              "0   8112052                                어릴때보고 지금다시봐도 재밌어요ㅋㅋ      1\n",
              "1   8132799  디자인을 배우는 학생으로, 외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산...      1\n",
              "2   4655635               폴리스스토리 시리즈는 1부터 뉴까지 버릴께 하나도 없음.. 최고.      1\n",
              "3   9251303  와.. 연기가 진짜 개쩔구나.. 지루할거라고 생각했는데 몰입해서 봤다.. 그래 이런...      1\n",
              "4  10067386                        안개 자욱한 밤하늘에 떠 있는 초승달 같은 영화.      1"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j6_mWHXrQPaz",
        "outputId": "3b7a70ad-4572-4ce0-ca63-a019cdcd7876"
      },
      "source": [
        "print(len(train_data))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "200000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_feQbe5MQQ3x"
      },
      "source": [
        "# null 값 존재 유무 확인\n",
        "print(train_data.isnull().values.any())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q5EgrVLwQV7u",
        "outputId": "37894153-8ee3-4ae7-8e29-1647766cba3d"
      },
      "source": [
        "train_data = train_data.dropna(how = 'any') #null 값이 존재하는 행 제거\n",
        "print(train_data.isnull().values.any())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4P6k3bm4Qj_x",
        "outputId": "26f24fd7-c3c2-4404-bdae-020ecac2c676"
      },
      "source": [
        "print(len(train_data))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "199992\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qI4drT7QnjB"
      },
      "source": [
        "train_data['document'] = train_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "id": "D0G9bEaYQ0K2",
        "outputId": "57d1c242-bbeb-49d5-bfda-fede9dfb28ea"
      },
      "source": [
        "train_data[:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>document</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>8112052</td>\n",
              "      <td>어릴때보고 지금다시봐도 재밌어요ㅋㅋ</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>8132799</td>\n",
              "      <td>디자인을 배우는 학생으로 외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산업...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4655635</td>\n",
              "      <td>폴리스스토리 시리즈는 부터 뉴까지 버릴께 하나도 없음 최고</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9251303</td>\n",
              "      <td>와 연기가 진짜 개쩔구나 지루할거라고 생각했는데 몰입해서 봤다 그래 이런게 진짜 영화지</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>10067386</td>\n",
              "      <td>안개 자욱한 밤하늘에 떠 있는 초승달 같은 영화</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         id                                           document  label\n",
              "0   8112052                                어릴때보고 지금다시봐도 재밌어요ㅋㅋ      1\n",
              "1   8132799  디자인을 배우는 학생으로 외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산업...      1\n",
              "2   4655635                   폴리스스토리 시리즈는 부터 뉴까지 버릴께 하나도 없음 최고      1\n",
              "3   9251303   와 연기가 진짜 개쩔구나 지루할거라고 생각했는데 몰입해서 봤다 그래 이런게 진짜 영화지      1\n",
              "4  10067386                         안개 자욱한 밤하늘에 떠 있는 초승달 같은 영화      1"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wan0HLs0Q2fu"
      },
      "source": [
        "# 불용어 정의\n",
        "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LhWfiN6FRTY4"
      },
      "source": [
        "# import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aS_pcCZvRFZ2"
      },
      "source": [
        "from tqdm import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jM4qJpXXQ6Pi",
        "outputId": "999e3286-e25a-468b-d86b-97aff507d23d"
      },
      "source": [
        "# 형태소 분석기 okt 사용한 토큰화 작업\n",
        "okt = Okt()\n",
        "\n",
        "tokenized_data = []\n",
        "for sentence in tqdm(train_data['document']):\n",
        "    tokenized_sentence = okt.morphs(sentence, stem=True) # 형태소분석.. 토큰화\n",
        "    stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords] # 불용어 제거\n",
        "    tokenized_data.append(stopwords_removed_sentence)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 199992/199992 [20:18<00:00, 164.09it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "MqEelcchRBt2",
        "outputId": "368b399d-c031-4850-8b21-70c83dd4c6a2"
      },
      "source": [
        "# 리뷰 길이 분포 확인\n",
        "print('리뷰의 최대 길이 :',max(len(l) for l in tokenized_data))\n",
        "print('리뷰의 평균 길이 :',sum(map(len, tokenized_data))/len(tokenized_data))\n",
        "plt.hist([len(s) for s in tokenized_data], bins=50)\n",
        "plt.xlabel('length of samples')\n",
        "plt.ylabel('number of samples')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "리뷰의 최대 길이 : 72\n",
            "리뷰의 평균 길이 : 10.716703668146726\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fr7WtMMPRbrq"
      },
      "source": [
        "from gensim.models import Word2Vec\n",
        "model = Word2Vec(sentences = tokenized_data, size = 100, window = 5, min_count = 5, workers = 4, sg = 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qanJsGQeRn9j",
        "outputId": "2ed0f26b-72a1-42da-8c92-44268c1164b2"
      },
      "source": [
        "# 완성된 임베딩 매트릭스의 크기 확인\n",
        "model.wv.vectors.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(16477, 100)"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vhO9H2zcRq5Q",
        "outputId": "053ffa0d-23af-473a-9f00-6cbe745812b2"
      },
      "source": [
        "print(model.wv.most_similar(\"최민식\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('한석규', 0.8778828382492065), ('송강호', 0.8502885103225708), ('박중훈', 0.850069522857666), ('이정재', 0.8448969721794128), ('최민수', 0.844124972820282), ('엄정화', 0.8440042734146118), ('강지환', 0.842502236366272), ('김명민', 0.837706446647644), ('안성기', 0.8365885019302368), ('주진모', 0.833758533000946)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E0OubXb3Ruy6",
        "outputId": "2fe3833c-d82d-48d7-edb8-7785d2463570"
      },
      "source": [
        "print(model.wv.most_similar(\"히어로\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('무협', 0.8730586767196655), ('호러', 0.8645143508911133), ('슬래셔', 0.8605858087539673), ('느와르', 0.8421926498413086), ('물', 0.8336173295974731), ('무비', 0.8279067277908325), ('멜로', 0.811174213886261), ('물의', 0.8094125390052795), ('헐리우드', 0.7883111238479614), ('교과서', 0.7843461036682129)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xc0kCzzNRzO6"
      },
      "source": [
        "### 3. 사전 훈련된 Word2Vec 임베딩(Pretrained Word2Vec embedding) 소개\n",
        "* 구글이 제공하는 사전 훈련된 Word2Vec 모델 사용\n",
        "* 사전훈련된 3백만 개의 단어 벡터 제공\n",
        "* 각 임베딩 벡터의 차원은 300"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SWYeBzerSP8U"
      },
      "source": [
        "import gensim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wAoUo0qjYzvi",
        "outputId": "ad3f8147-5739-4214-c06a-f7804a278e92"
      },
      "source": [
        "!git clone https://github.com/mmihaltz/word2vec-GoogleNews-vectors.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'word2vec-GoogleNews-vectors'...\n",
            "remote: Enumerating objects: 20, done.\u001b[K\n",
            "remote: Total 20 (delta 0), reused 0 (delta 0), pack-reused 20\u001b[K\n",
            "Unpacking objects: 100% (20/20), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n9YF7uNyY3mP",
        "outputId": "8d2d1f0c-032a-409f-dc2f-7c9c6dc62a01"
      },
      "source": [
        "import os\n",
        "\n",
        "os.listdir('word2vec-GoogleNews-vectors')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['README.md',\n",
              " 'GoogleNews-vectors-negative300.bin.gz',\n",
              " '.gitattributes',\n",
              " '.git']"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tiVV0YkHZOGO",
        "outputId": "ff9bee7e-bf07-4e2c-af6f-7bef8fe458e1"
      },
      "source": [
        "#a = os.system('GoogleNews-vectors-negative300.bin.gz')\n",
        "#path = get_file(MODEL + '.gz', 'https://deeplearning4jblob.blob.core.windows.net/resources/wordvectors/%s.gz' % MODEL)\n",
        "!wget -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\"\n",
        " \n",
        "model = gensim.models.KeyedVectors.load_word2vec_format(\"GoogleNews-vectors-negative300.bin.gz\",binary = True)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-10-22 14:31:18--  https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.133.48\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.133.48|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1647046227 (1.5G) [application/x-gzip]\n",
            "Saving to: ‘GoogleNews-vectors-negative300.bin.gz’\n",
            "\n",
            "GoogleNews-vectors- 100%[===================>]   1.53G  15.4MB/s    in 1m 46s  \n",
            "\n",
            "2021-10-22 14:33:05 (14.8 MB/s) - ‘GoogleNews-vectors-negative300.bin.gz’ saved [1647046227/1647046227]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uyKw0OzrZSBF",
        "outputId": "6ab881d5-8d42-4ada-ef97-3cecf0a95d4f"
      },
      "source": [
        "# import zipfile\n",
        "\n",
        "# zip_file=zipfile.ZipFile(\"test.zip\")\n",
        "# zip_file.extractall()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "32512"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_IaYcHbS8Kw"
      },
      "source": [
        "# model = gensim.models.KeyedVectors.load_word2vec_format('', binary=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LjOZlgn2TA_I",
        "outputId": "562f56ea-3ae4-4281-8579-4e7f15cf296e"
      },
      "source": [
        "print(model.vectors.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3000000, 300)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I5iJCkHpTCvB",
        "outputId": "2c2630b0-3ad7-4cea-b990-45ee7c5d08ef"
      },
      "source": [
        "print (model.similarity('this', 'is'))\n",
        "print (model.similarity('post', 'book'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.40797037\n",
            "0.057204384\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sUrf6BeUTFTJ",
        "outputId": "824dc9e2-da72-4d42-bcee-310b150e50fc"
      },
      "source": [
        "print(model['book'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 0.11279297 -0.02612305 -0.04492188  0.06982422  0.140625    0.03039551\n",
            " -0.04370117  0.24511719  0.08740234 -0.05053711  0.23144531 -0.07470703\n",
            "  0.21875     0.03466797 -0.14550781  0.05761719  0.00671387 -0.00701904\n",
            "  0.13183594 -0.25390625  0.14355469 -0.140625   -0.03564453 -0.21289062\n",
            " -0.24804688  0.04980469 -0.09082031  0.14453125  0.05712891 -0.10400391\n",
            " -0.19628906 -0.20507812 -0.27539062  0.03063965  0.20117188  0.17382812\n",
            "  0.09130859 -0.10107422  0.22851562 -0.04077148  0.02709961 -0.00106049\n",
            "  0.02709961  0.34179688 -0.13183594 -0.078125    0.02197266 -0.18847656\n",
            " -0.17480469 -0.05566406 -0.20898438  0.04858398 -0.07617188 -0.15625\n",
            " -0.05419922  0.01672363 -0.02722168 -0.11132812 -0.03588867 -0.18359375\n",
            "  0.28710938  0.01757812  0.02185059 -0.05664062 -0.01251221  0.01708984\n",
            " -0.21777344 -0.06787109  0.04711914 -0.00668335  0.08544922 -0.02209473\n",
            "  0.31835938  0.01794434 -0.02246094 -0.03051758 -0.09570312  0.24414062\n",
            "  0.20507812  0.05419922  0.29101562  0.03637695  0.04956055 -0.06689453\n",
            "  0.09277344 -0.10595703 -0.04370117  0.19726562 -0.03015137  0.05615234\n",
            "  0.08544922 -0.09863281 -0.02392578 -0.08691406 -0.22460938 -0.16894531\n",
            "  0.09521484 -0.0612793  -0.03015137 -0.265625   -0.13378906  0.00139618\n",
            "  0.01794434  0.10107422  0.13964844  0.06445312 -0.09765625 -0.11376953\n",
            " -0.24511719 -0.15722656  0.00457764  0.12988281 -0.03540039 -0.08105469\n",
            "  0.18652344  0.03125    -0.09326172 -0.04760742  0.23730469  0.11083984\n",
            "  0.08691406  0.01916504  0.21386719 -0.0065918  -0.08984375 -0.02502441\n",
            " -0.09863281 -0.05639648 -0.26757812  0.19335938 -0.08886719 -0.25976562\n",
            "  0.05957031 -0.10742188  0.09863281  0.1484375   0.04101562  0.00340271\n",
            " -0.06591797 -0.02941895  0.20019531 -0.00521851  0.02355957 -0.13671875\n",
            " -0.12597656 -0.10791016  0.0067749   0.15917969  0.0145874  -0.15136719\n",
            "  0.07519531 -0.02905273  0.01843262  0.20800781  0.25195312 -0.11523438\n",
            " -0.23535156  0.04101562 -0.11035156  0.02905273  0.22460938 -0.04272461\n",
            "  0.09667969  0.11865234  0.08007812  0.07958984  0.3125     -0.14941406\n",
            " -0.234375    0.06079102  0.06982422 -0.14355469 -0.05834961 -0.36914062\n",
            " -0.10595703  0.00738525  0.24023438 -0.10400391 -0.02124023  0.05712891\n",
            " -0.11621094 -0.16894531 -0.06396484 -0.12060547  0.08105469 -0.13769531\n",
            " -0.08447266  0.12792969 -0.15429688  0.17871094  0.2421875  -0.06884766\n",
            "  0.03320312  0.04394531 -0.04589844  0.03686523 -0.07421875 -0.01635742\n",
            " -0.24121094 -0.08203125 -0.01733398  0.0291748   0.10742188  0.11279297\n",
            "  0.12890625  0.01416016 -0.28710938  0.16503906 -0.25585938  0.2109375\n",
            " -0.19238281  0.22363281  0.04541016  0.00872803  0.11376953  0.375\n",
            "  0.09765625  0.06201172  0.12109375 -0.24316406  0.203125    0.12158203\n",
            "  0.08642578  0.01782227  0.17382812  0.01855469  0.03613281 -0.02124023\n",
            " -0.02905273 -0.04541016  0.1796875   0.06494141 -0.13378906 -0.09228516\n",
            "  0.02172852  0.02099609  0.07226562  0.3046875  -0.27539062 -0.30078125\n",
            "  0.08691406 -0.22949219  0.0546875  -0.34179688 -0.00680542 -0.0291748\n",
            " -0.03222656  0.16210938  0.01141357  0.23339844 -0.0859375  -0.06494141\n",
            "  0.15039062  0.17675781  0.08251953 -0.26757812 -0.11669922  0.01330566\n",
            "  0.01818848  0.10009766 -0.09570312  0.109375   -0.16992188 -0.23046875\n",
            " -0.22070312  0.0625      0.03662109 -0.125       0.05151367 -0.18847656\n",
            "  0.22949219  0.26367188 -0.09814453  0.06176758  0.11669922  0.23046875\n",
            "  0.32617188  0.02038574 -0.03735352 -0.12255859  0.296875   -0.25\n",
            " -0.08544922 -0.03149414  0.38085938  0.02929688 -0.265625    0.42382812\n",
            " -0.1484375   0.14355469 -0.03125     0.00717163 -0.16601562 -0.15820312\n",
            "  0.03637695 -0.16796875 -0.01483154  0.09667969 -0.05761719 -0.00515747]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDsgCWhBSNT5"
      },
      "source": [
        "최근 Word2Vec은 추천시스템에도 사용됨"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nESy-jBiTspH"
      },
      "source": [
        "## 04) 네거티브 샘플링을 이용한 Word2Vec 구현 Skip-Gram with Negative Sampling\n",
        "### 1. 네거티브 샘플링\n",
        "* Word2Vec의 출력층: 소프트맥스 함수를 지난 단어 집합 크기의 벡터와 실제값인 원-핫 벡터와의 오차를 구하고 이로부터 임베딩 테이블에 있는 모든 단어에 대한 임베딩 벡터 값을 업데이트\n",
        "* 네거티브 샘플링: Word2Vec이 학습 과정에서 전체 단어 집합이 아니라 일부 단어 집합에만 집중할 수 있도록 함\n",
        "  * 하나의 중심 단어에 대해서 작은 단어 집합 만든 후 마지막 단계를 이진 분류 문제로 변환\n",
        "  * 기존의 단어 집합의 크기만큼 선택지를 두고 다중 클래스 분류 문제를 풀던 Word2Vec보다 훨씬 연산량에서 효율적\n",
        "\n",
        "\n",
        "### 2. 네거티브 샘플링 Skip-Gram (SGNS)\n",
        "* Skip-Gram: 입력은 중심 단어, 모델의 예측은 주변 단어\n",
        "* SGNS: 중심 단어와 주변 단어 **모두**가 입력, 이 **두 단어가 실제로 윈도우 크기 내에 존재하는 이웃 관계**인지 그 확률 예측  \n",
        "  > 주변 단어 관계까 아닌 단어들을 입력2로 삼기 위하여 레이블을 0으로  \n",
        "\n",
        "* 두 테이블 중 하나는 입력 1인 중심단어의 테이블 룩업을 위한 임베딩 테이블, 하나는 입력 2인 주변 단어의 테이블 룩업을 위한 임베딩 테이블\n",
        "* 각 임베딩 테이블을 통해 테이블 룩업하여 임베딩 벡터로 변환\n",
        "* 중심 단어와 주변 단어의 내적값을 이 모델의 예측값, 레이블과의 오차로부터 역전파하여 중심 단어와 주변 단어의 임베딩 벡터값을 업데이트"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sY2hwXJSV9ka"
      },
      "source": [
        "### 3. 20 뉴스그룹 데이터 전처리하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36Y1EAzsRyjy"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1SRXSN6WWCGd",
        "outputId": "f81495f2-b703-4abc-8716-65411cb93838"
      },
      "source": [
        "# 하나의 샘플에 최소 단어 2개는 있어야 함\n",
        "dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))\n",
        "documents = dataset.data\n",
        "print('총 샘플 수 : ', len(documents))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading 20news dataset. This may take a few minutes.\n",
            "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "총 샘플 수 :  11314\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ss9YE1iTWPfp"
      },
      "source": [
        "news_df = pd.DataFrame({'document':documents})\n",
        "# 특수 문자 제거\n",
        "news_df['clean_doc'] = news_df['document'].str.replace(\"[^a-zA-Z]\", \" \")\n",
        "# 길이가 3 이하인 단어는 제거\n",
        "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: ' '.join([w for w in x.split() if len(w) > 3]))\n",
        "# 전체 단어에 대한 소문자 변환\n",
        "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: x.lower())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GX8JgPMyWsly",
        "outputId": "68625283-44d7-428e-b9b3-87ae563f7fff"
      },
      "source": [
        "# null값 있는지 확인\n",
        "news_df.isnull().values.any()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lAZVbIQZW97T",
        "outputId": "4332c3e7-dd2d-4db3-df76-752b4575c606"
      },
      "source": [
        "# 빈 값 유무 확인\n",
        "# 모든 빈 값을 Null 값으로 변환, 다시 확인\n",
        "news_df.replace(\"\", float(\"NaN\"), inplace=True)\n",
        "news_df.isnull().values.any()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fw1vZ7t4XMAE",
        "outputId": "e084ffa1-964e-4904-d069-b10527544d4c"
      },
      "source": [
        "# Null값 제거\n",
        "news_df.dropna(inplace=True)\n",
        "print('총 샘플 수 :',len(news_df))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "총 샘플 수 : 10995\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ff0Ht8anXmFd",
        "outputId": "b3c02604-5441-434e-a4f2-e0e7dd647105"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZwq2_YrXRLd"
      },
      "source": [
        "# 불용어를 제거\n",
        "stop_words = stopwords.words('english')\n",
        "tokenized_doc = news_df['clean_doc'].apply(lambda x: x.split())\n",
        "tokenized_doc = tokenized_doc.apply(lambda x: [item for item in x if item not in stop_words])\n",
        "tokenized_doc = tokenized_doc.to_list()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0iuiCURHXUx1",
        "outputId": "90fa1faa-6c7a-44b1-d615-51b03918e7a2"
      },
      "source": [
        "# 불용어 제거하여 단어의 수 줄어듦. 모든 샘플 중 단어가 1개 이하인 경우 모두 찾아 제거\n",
        "drop_train = [index for index, sentence in enumerate(tokenized_doc) if len(sentence) <= 1]\n",
        "tokenized_doc = np.delete(tokenized_doc, drop_train, axis=0)\n",
        "print('총 샘플 수 :',len(tokenized_doc))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "총 샘플 수 : 10940\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return array(a, dtype, copy=False, order=order)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfhaoI2OaqI2"
      },
      "source": [
        "# 단어 집합 생성, 정수 인코딩 진행\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(tokenized_doc)\n",
        "\n",
        "word2idx = tokenizer.word_index\n",
        "idx2word = {value : key for key, value in word2idx.items()}\n",
        "encoded = tokenizer.texts_to_sequences(tokenized_doc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EfPjWMSNavWN",
        "outputId": "0214948a-1d57-4b7e-8611-98036e8ed0b6"
      },
      "source": [
        "print(encoded[:2])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[9, 59, 603, 207, 3278, 1495, 474, 702, 9470, 13686, 5533, 15227, 702, 442, 702, 70, 1148, 1095, 1036, 20294, 984, 705, 4294, 702, 217, 207, 1979, 15228, 13686, 4865, 4520, 87, 1530, 6, 52, 149, 581, 661, 4406, 4988, 4866, 1920, 755, 10668, 1102, 7837, 442, 957, 10669, 634, 51, 228, 2669, 4989, 178, 66, 222, 4521, 6066, 68, 4295], [1026, 532, 2, 60, 98, 582, 107, 800, 23, 79, 4522, 333, 7838, 864, 421, 3825, 458, 6488, 458, 2700, 4730, 333, 23, 9, 4731, 7262, 186, 310, 146, 170, 642, 1260, 107, 33568, 13, 985, 33569, 33570, 9471, 11491]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IhgTxwaRaxcY",
        "outputId": "6bd0acb6-e8b2-46ef-c6c2-7d6f1eb2a8ad"
      },
      "source": [
        "# 단어 집합 크기 확인\n",
        "vocab_size = len(word2idx) + 1 \n",
        "print('단어 집합의 크기 :', vocab_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "단어 집합의 크기 : 64277\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPB7vdj-a047"
      },
      "source": [
        "### 4. 네거티브 샘플링 통한 데이터셋 구성\n",
        "* 토큰화, 정제, 정규화, 불용어 제거, 정수 인코딩까지 일반적인 전처리 과정 거침\n",
        "* 네거티브 샘플링 위해서 keras의 skipgrams 사용\n",
        "* 상위 10개의 뉴스그룹 샘플에 대해서만 수행"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-eRey7XqbDjV"
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import skipgrams\n",
        "# 네거티브 샘플링\n",
        "skip_grams = [skipgrams(sample, vocabulary_size=vocab_size, window_size=10) for sample in encoded[:10]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "URUgTd7HbNZI",
        "outputId": "6b2d747b-30f2-4529-bc7b-a34e77c13dfb"
      },
      "source": [
        "# 첫번째 샘플인 skip_grams[0] 내 skipgrams로 형성된 데이터셋 확인\n",
        "pairs, labels = skip_grams[0][0], skip_grams[0][1]\n",
        "for i in range(5):\n",
        "      print(\"({:s} ({:d}), {:s} ({:d})) -> {:d}\".format(\n",
        "          idx2word[pairs[i][0]], pairs[i][0], \n",
        "          idx2word[pairs[i][1]], pairs[i][1], \n",
        "          labels[i]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(guilt (4989), chow (20699)) -> 0\n",
            "(existance (4865), achtung (26602)) -> 0\n",
            "(israeli (442), thermocouple (63915)) -> 0\n",
            "(subsidizing (15228), degree (1530)) -> 1\n",
            "(think (6), sodom (13039)) -> 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lWZ6SxwFbgo7",
        "outputId": "472ae5ab-beb0-4202-8041-8161cc8dd3e0"
      },
      "source": [
        "print('전체 샘플 수 :',len(skip_grams))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "전체 샘플 수 : 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-LVFWhDcbjmc",
        "outputId": "da609f16-821b-46ce-9d67-9d6e4a466219"
      },
      "source": [
        "# 첫번째 뉴스그룹 샘플에 대해서 생긴 pairs와 labels의 개수\n",
        "print(len(pairs))\n",
        "print(len(labels))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2220\n",
            "2220\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sL_hyoPgbocZ"
      },
      "source": [
        "# 모든 뉴스그룹 샘플에 대해서 수행\n",
        "skip_grams = [skipgrams(sample, vocabulary_size=vocab_size, window_size=10) for sample in encoded]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIqIO5sDbsTb"
      },
      "source": [
        "### 5. Skip-Gram with Negative Sampling SGNS 구현"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCZoj3eBbv3n"
      },
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Embedding, Reshape, Activation, Input\n",
        "from tensorflow.keras.layers import Dot\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from IPython.display import SVG"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xqd3Xk9XbyNq"
      },
      "source": [
        "# 임베딩 벡터 차원\n",
        "embed_size = 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHeBShjAb06r"
      },
      "source": [
        "# 모델 설계\n",
        "# 두 개의 임베딩 테이블 생성\n",
        "\n",
        "# 중심 단어를 위한 임베딩 테이블\n",
        "w_inputs = Input(shape=(1, ), dtype='int32')\n",
        "word_embedding = Embedding(vocab_size, embed_size)(w_inputs)\n",
        "\n",
        "# 주변 단어를 위한 임베딩 테이블\n",
        "c_inputs = Input(shape=(1, ), dtype='int32')\n",
        "context_embedding  = Embedding(vocab_size, embed_size)(c_inputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmP7QGEAcBt_"
      },
      "source": [
        "# 각 단어는 임베딩 테이블을 거쳐 내적 수행, 내적의 결과는 1 또는 0으로 예측하기 위해서 시그모이드 함수를 활성화 함수로 거쳐 최종 예측값 얻음\n",
        "dot_product = Dot(axes=2)([word_embedding, context_embedding])\n",
        "dot_product = Reshape((1,), input_shape=(1, 1))(dot_product)\n",
        "output = Activation('sigmoid')(dot_product)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LrmPusgcHAq"
      },
      "source": [
        "model = Model(inputs=[w_inputs, c_inputs], outputs=output)\n",
        "model.summary()\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam')\n",
        "plot_model(model, to_file='model3.png', show_shapes=True, show_layer_names=True, rankdir='TB')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lmdr2wmUcJAZ"
      },
      "source": [
        "# 모델의 학습\n",
        "for epoch in range(1, 6):\n",
        "    loss = 0\n",
        "    for _, elem in enumerate(skip_grams):\n",
        "        first_elem = np.array(list(zip(*elem[0]))[0], dtype='int32')\n",
        "        second_elem = np.array(list(zip(*elem[0]))[1], dtype='int32')\n",
        "        labels = np.array(elem[1], dtype='int32')\n",
        "        X = [first_elem, second_elem]\n",
        "        Y = labels\n",
        "        loss += model.train_on_batch(X,Y)  \n",
        "    print('Epoch :',epoch, 'Loss :',loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzpoVM1UcMlk"
      },
      "source": [
        "### 6. 결과 확인"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVv0wV8PcLNm"
      },
      "source": [
        "import gensim\n",
        "\n",
        "# 학습된 임베딩 벡터들은 vector.txt에 저장\n",
        "f = open('vectors.txt' ,'w')\n",
        "f.write('{} {}\\n'.format(vocab_size-1, embed_size))\n",
        "vectors = model.get_weights()[0]\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    f.write('{} {}\\n'.format(word, ' '.join(map(str, list(vectors[i, :])))))\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-QnEaBh_cSpp"
      },
      "source": [
        "# 쉽게 유사도 구할 수 있음\n",
        "w2v = gensim.models.KeyedVectors.load_word2vec_format('./vectors.txt', binary=False)\n",
        "w2v.most_similar(positive=['soldiers'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Zrn7T-YcYS8"
      },
      "source": [
        "w2v.most_similar(positive=['doctor'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LkpVUbtdcZ5e"
      },
      "source": [
        "w2v.most_similar(positive=['police'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wF6kT9kIcaCz"
      },
      "source": [
        "w2v.most_similar(positive=['knife'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NG9pO9WvcaLq"
      },
      "source": [
        "w2v.most_similar(positive=['engine'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hneh9Qdck-W"
      },
      "source": [
        "## 05) 글로브 GloVe\n",
        "* 카운트 기반과 예측 기반을 모두 사용하는 방법론\n",
        "* 2014년에 미국 스탠포드 대학에서 개발한 단어 임베딩 방법론\n",
        "* 앞서 카운트 기반의 LSA(Latent Semantic Analysis)와 예측 기반 Word2Vec의 단점 지적하며 이를 보완\n",
        "\n",
        "### 1. 기존 방법론에 대한 비판\n",
        "* LSA: DTM이나 TD-IDF 행렬과 같이 **각 문서에서의 각 단어의 빈도수를 카운트 한 행렬이라는 전체적인 통계 정보를 입력으로 받아 차원을 축소(Truncated SVD)하여 잠재된 의미 끌어내느 방법론**\n",
        "  * 카운트 기반으로 코퍼스의 전체적인 통계 정보 고려. 하지만 **단어 의미의 유추 작업 Analogy task에는 성능이 떨어짐**  \n",
        "\n",
        "* Word2Vec: 예측 기반. 단어 간 유추 작업에는 LSA보다 뛰어남. **임베딩 벡터가 윈도우 크기 내에서만 주변 단어를 고려하기 때문에 코퍼스의 전체적인 통계 정보 반영 못함**\n",
        "* 이러한 점을 비판하여 카운트 기반과 예측 기반 방법론 두 가지 모두 사용"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96dj5aiadTeH"
      },
      "source": [
        "### 2. 윈도우 기반 동시 등장 행렬 Window based Co-occurrence Matrix\n",
        "* 단어의 동시 등장 행렬 Co-occurrence Matrix: 행과 열을 전체 단어 집합의 단어들로 구성. \n",
        "  * i 단어의 Window Size 내에서 k 단어가 등장한 횟수를 i행 k열에 기재한 행렬\n",
        "  * Transpose해도 동일한 행렬이 된다는 특징\n",
        "\n",
        "\n",
        "### 3. 동시 등장 확률 Co-occurence Probability\n",
        "* P(k|i): 동시 등장 행렬로부터 특정 단어 i의 전체 등장 횟수를 카운트하고, 특정 단어 i가 등장했을 때 어떤 단어 k가 등장한 횟수를 카운트하여 계산한 조건부 확률\n",
        "  * i: Center Word, k: Context Word\n",
        "  * 중심 단어 i의 행의 모든 값을 더한 값을 분모, i행 k열의 값을 분자로 한 값\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5dnTVvteQhk"
      },
      "source": [
        "### 4. 손실 함수 Loss Function\n",
        "* X: 동시 등장 행렬 Co-occurence Matrix\n",
        "* Xij: 중심 단어 i가 등장했을 때 윈도우 내 주변 단어 j가 등장하는 횟수\n",
        "* sigma j Xij: 동시 등장 행렬에서 i행의 값을 모두 더한 값\n",
        "* Pik = P(k|i) = Xik/Xi: 중심 단어 i가 등장했을 때 윈도우 내 주변 단어 k가 등장할 확률\n",
        "  * ex) P(solid|ice) =  ice가 등장했을 때 solid가 등장할 확률\n",
        "* Pik/Pjk\n",
        "  ex) P(solid|ice)/P(solid|steam) = 8.9\n",
        "* wi: 중심 단어 i의 임베딩 벡터\n",
        "* ὦk: 주변 단어 k의 임베딩 벡터\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZuT8Mol5e7oO"
      },
      "source": [
        "* GloVe in one sentence\n",
        "  > 임베딩 된 중심 단어와 주변 단어 벡터의 내적이 전체 코퍼스에서 동시 등장 확률이 되도록 만드는 것\n",
        "  > dot product(wi ὦk) ≈ P(k|i) = Pik\n",
        "\n",
        "  > 더 정확하게는,\n",
        "  > dot product(wi ὦk) ≈ logP(k|i) = logPik "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NiLZPCNXfX68"
      },
      "source": [
        "* 손실 함수: 단어 간의 관계 잘 표현해야 함\n",
        "  > F(wi, wj, ὦk) = Pik/Pjk\n",
        "* F의 목적: 두 단어 사이의 동시 등장 확률의 크기 관계 비(ratio) 정보를 벡터 공간에 인코딩 하는 것이 목적\n",
        "* F의 입력: \n",
        "  > F(wi - wj, ὦk) = Pik/Pjk\n",
        "* 우변은 스칼라, 좌변은 벡터. Dot product 수행\n",
        "  > F((wi - wj)Tὦk) = Pik/Pjk\n",
        "  * Linear space에서의 단어 의미 관계 표현 위해 뺄셈과 내적 택함  \n",
        "\n",
        "\n",
        "* F가 만족해야 할 필수 조건: 중심 단어 w와 주변 단어 ὦ라는 선택 기준은 실제로는 무작위 선택이므로 둘의 관계는 자유롭게 교환될 수 있도록 해야됨\n",
        "  * **F가 실수의 덧셈과 양수의 곱셈에 대해서 *준동형 Homomorphism*을 만족하도록 함**\n",
        "    > F(a+b) = F(a)F(b)\n",
        "* F에 적용\n",
        "  > F(v1Tv2 + v3Tv4) = F(v1Tv2)F(v3Tv4)\n",
        "* 뺄셈에 대한 준동형식으로 변경\n",
        "  > F(v1Tv2 - v3Tv4) = F(v1Tv2) / F(v3Tv4)\n",
        "* 이를 Glove씩에 적용\n",
        "  > F((wi - wj)Tὦk) = F(wiTὦk) / F(wjTὦk)\n",
        "* 우변\n",
        "  > Pik/Pjk = F(wiTὦk) / F(wjTὦk)\n",
        "  > F(wiTὦk) = Pik = Xik/Xi\n",
        "* 좌변\n",
        "  > F(wiTὦk - wjTὦk) = F(wiTὦk) / F(wjTὦk)\n",
        "\n",
        "* F 함수에 지수 함수\n",
        "  > expF(wiTὦk - wjTὦk) = expF(wiTὦk) / expF(wjTὦk)\n",
        "\n",
        "  > wiTὦk = logPik = log(Xik/Xi) = logXik - logXi\n",
        "\n",
        "* wi와 ὦk는 값 바뀌어도 식 성립\n",
        "* logXi항을 wi에 대한 편향 bi로 대체\n",
        "  > wiTὦk + bi + b tilde - logXik\n",
        "* 손실함수 일반화\n",
        "  * Loss func = V sigma m, n = (wmTὦn + bm + b tilde n - logXmn)^2\n",
        "    * V는 단어 집합의 크기. \n",
        "    * Still 문제.. \n",
        "        * logXik에서 Xik 값이 0이 될 수 있으므로 log(1+Xik)로 변경\n",
        "        * 동시 등장 행렬 X는 DTM처럼 희소 행렬 Sparse Matrix일 가능성 다분.\n",
        "          * Xik 갑에 영향을 받는 가중치 함수 Weighting function f(Xik)를 손실 함수에 도임\n",
        "            * Xik 값이 작으면 상대적으로 함수의 값은 작은데, 크면 최대값이 정해져 있음\n",
        "              > f(x) = min(1, (x/xmax)^3/4)\n",
        "* **최종**\n",
        "  > Loss func = V sigma m, n = f(Xmn)(wmTὦn + bm + b tilde n - logXmn)^2        \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QUxHL5tjhGO"
      },
      "source": [
        "### 5. GloVe 훈련시키기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JtnPu70ajW5N",
        "outputId": "101378c6-087c-4c25-9721-1e51e0363567"
      },
      "source": [
        "!python -m pip install glove_python_binary"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: glove_python_binary in /usr/local/lib/python3.7/dist-packages (0.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from glove_python_binary) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from glove_python_binary) (1.4.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KcVMOrCkmhb2",
        "outputId": "49d4e2d1-752f-4339-d7cc-8a9cb83e5f8b"
      },
      "source": [
        "# glov_python_library가 계속 import error가 나서 대신 mittens를 사용했음\n",
        "# 근데 mittens에는 Corpus가 없어 오류가 남..\n",
        "!pip install -U mittens\n",
        "from mittens import GloVe as Glove"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: mittens in /usr/local/lib/python3.7/dist-packages (0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from mittens) (1.19.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AmERgEMidPYV"
      },
      "source": [
        "# from glove_pybind import *\n",
        "# from .Glove import Corpus, Glove\n",
        "\n",
        "corpus = Corpus() \n",
        "\n",
        "# 훈련 데이터로부터 GloVe에서 사용할 동시 등장 행렬 생성\n",
        "corpus.fit(result, window=5)\n",
        "glove = Glove(no_components=100, learning_rate=0.05)\n",
        "\n",
        "# 학습에 이용할 쓰레드의 개수는 4로 설정, 에포크는 20.\n",
        "glove.fit(corpus.matrix, epochs=20, no_threads=4, verbose=True)\n",
        "glove.add_dictionary(corpus.dictionary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rA5m068jjkku"
      },
      "source": [
        "print(glove.most_similar(\"man\"))\n",
        "print(glove.most_similar(\"boy\"))\n",
        "print(glove.most_similar(\"university\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HpFgPdpXd15M"
      },
      "source": [
        "## 06) 패스트텍스트(FastText)\n",
        "* 페이스북 개발\n",
        "* Word2Vec는 단어를 쪼개질 수 없는 단위로 생각\n",
        "* FastText는 하나의 단어 안에도 여러 단어들이 존재하는 것, 즉 **내부 단어(subword)를 고려**하여 학습\n",
        "\n",
        "### 1. 내부 단어(subword)의 학습\n",
        "* FastText에서는 각 단어는 글자 단위 n-gram의 구성으로 취급함\n",
        "  * n=3, apple은 app,ppl,ple로 분리\n",
        "    > n = 3 ==> <ap, app, ppl, ple, le>, <apple>\n",
        "  * n의 최솟값과 최댓갑. 기본적으로는 3과 6으로 설정.\n",
        "    > <ap, app, ppl, ppl, le>, <app, appl, pple, ple>, <appl, pple>, ..., <apple>\n",
        "  * 내부 단어들을 벡터화(aka Word2Vec 수행)\n",
        "  > apple = <ap + app + ppl + ppl + le> + <app + appl + pple + ple> + <appl + pple> + , ..., +<apple>\n",
        "\n",
        "### 2. 모르는 단어(Out of Vocabulary, OOV)에 대한 대응\n",
        "  * 장점: 데이터셋만 충분하다면 **subword를 통해 OOV에 대해서도 다른 단어와의 유사도를 계산 가능**\n",
        "    * ex. birthplace를 몰라도 birth와 place라는 subword가 있다면 birthplace의 벡터를 얻을 수 있음\n",
        "    * 모르는 단어에 제대로 대처할 수 없는 **Word2Vec, GloVe와는 다른 점**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmQQ31kDgv48"
      },
      "source": [
        "### 3. 단어 집합 내 빈도 수가 적었던 단어(Rare Word)에 대한 대응\n",
        "* Word2Vec의 경우에는 rare word에 대해서는 임베딩의 정확도가 높지 않다는 단점\n",
        "  * 참고할 수 있는 경우의 수가 적다보니 정확하게 임베딩이 되지 않는 경우\n",
        "* **하지만** FastText의 경우, **단어가 희귀 단어라도, 그 단어의 n-gram이 다른 단어의 n-gram과 겹치는 경우**라면, Word2Vec과 비교하여 **비교적 높은 임베딩 벡터값을 얻음**\n",
        "* FastText가 노이즈가 많은 코퍼스에서 강점을 가진 것 또한 이와 같은 이유\n",
        "  * Word2Vec에서는 오타가 섞인 단어는 임베딩이 제대로 되지 않지만, FastText는 이에 대해서도 일정 수준의 성능을 보임\n",
        "    * ex. 단어 apple과 appple의 경우, 실제로 많은 개수의 동일한 n-gram을 가짐\n",
        "\n",
        "\n",
        "### 4. 실습으로 비교하는 Word2Vec Vs. FastText\n",
        "#### 1) Word2Vec\n",
        "* 학습 데이터에 존재하지 않는 단어에 대해서는 임베딩 벡터가 존재하지 않아 단어의 유사도를 계산 불가능\n",
        "\n",
        "\n",
        "#### 2) FastText\n",
        "* 유사한 단어를 계산해서 출력하고 있음\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XdGTYnn0g21E"
      },
      "source": [
        "from gensim.models import FastText\n",
        "model = FastText(result, size=100, window=5, min_count=5, workers=4, sg=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMEAmQmEg6MA"
      },
      "source": [
        "model.wv.most_similar(\"electrofishing\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqscu5pphJOb"
      },
      "source": [
        "#### 5. 한국어에서의 FastText\n",
        "1. 음절 단위\n",
        "* ex. n=3, '자연어처리' => <자연, 자연어, 연어처, 어처리, 처리>\n",
        "\n",
        "2. 자모 단위(총성, 중성, 종성 단위)\n",
        "* 오타나 노이즈 측면에서 더 강한 임베딩 기대 가능\n",
        "> 분리된 결과 : ㅈ ㅏ _ ㅇ ㅕ ㄴ ㅇ ㅓ _ ㅊ ㅓ _ ㄹ ㅣ _\n",
        "* n = 3일 때 n-gram 적용\n",
        "> < ㅈ ㅏ, ㅈ ㅏ _, ㅏ _ ㅇ, ... 중략>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClC2PmaWhjQw"
      },
      "source": [
        "## 07) 자모 단위 한국어 FastText 학습하기\n",
        "## 08) 사전 훈련된 워드 임베딩 Pre-trained Word Embedding\n",
        "* 케라스의 임베딩 층과 사전 훈련된 임베딩층을 가져와서 사용\n",
        "* 훈련 데이터의 단어들을 임베딩 층을 구현하여 임베딩 벡터로 학습하는 경우.\n",
        "  * 케라스에서는 이를 Embedding()이라는 도구를 사용하여 구현\n",
        "* **그런데** 방대한 코퍼스를 가지고 Word2Vec, FastText, GloVe 등을 통해서 이미 미리 훈련된 임베딩 벡터를 불러오는 방법 사용 경우도 있음"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRSFhXnAiLvX"
      },
      "source": [
        "### 1. Keras Embedding layer\n",
        "* 케라스는 훈련 데이터의 단어들에 대해 워드 임베딩을 수행하는 도구 Embedding()을 제공\n",
        "* Embedding()은 인공 신경망 구조 관점에서 임베딩 층(embedding layer)을 구현\n",
        "\n",
        "\n",
        "#### 1) 임베딩 층은 룩업 테이블이다.\n",
        "* 임베딩 층의 입력으로 사용하기 위해서 입력 시퀀스의 각 단어들은 모두 정수 인코딩이 되어있어야 합니다.\n",
        "  > 어떤 단어 -> 단어에 부여된 고유한 정수값 -> 임베딩 층 통과 -> 밀집 벡터\n",
        "* 임베딩 층은 입력 정수에 대해 dense vector로 맵핑하고 이 dense vector는 인공 신경망의 학습 과정에서 가중치가 학습되는 것과 같은 방식으로 훈련\n",
        "* 이 dense vector를 임베딩 벡터라고 부름\n",
        "\n",
        "\n",
        "* 정수를 밀집 벡터/임베딩 벡터로 맵핑한다는 것의 의미?\n",
        "  * 특정 단어와 맵핑되는 정수를 인덱스로 가지는 테이블로부터 임베딩 벡터 값을 가져오는 룩업 테이블이라고 볼 수 있음\n",
        "    * 그리고 이 테이블은 단어 집합의 크기만큼의 행을 가지므로 모든 단어는 고유한 임베딩 벡터를 가짐\n",
        "\n",
        "\n",
        "* (+) 임베딩 층의 입력이 원-핫 벡터가 아니어도 동작한다!\n",
        "  * 케라스는 단어를 정수 인덱스로 바꾸고 원-핫 벡터로 한 번 더 바꾸고나서 임베딩 층의 입력으로 사용하는 것이 아니라, 단어를 정수 인덱스로만 바꾼채로 임베딩 층의 입력으로 사용해도 룩업 테이블된 결과인 임베딩 벡터를 리턴"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SpJRqQ34joCg"
      },
      "source": [
        "# 케라스의 임베딩 층 구현 코드\n",
        "# 아래의 각 인자는 저자가 임의로 선정한 숫자들이며 의미있는 선정 기준이 아님\n",
        "v = Embedding(20000, 128, input_length = 500)\n",
        "# vocab_size = 20000 텍스트 데이터의 전체 단어 집합의 크기\n",
        "# output_dim = 128 워드 임베딩 후의 임베딩 벡터의 차원\n",
        "# input_length = 500 입력 시퀀스의 길이"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aj1uLbd0jnH9"
      },
      "source": [
        "* Embedding()은 (# of samples, input_length)인 2D 정수 텐서 입력받음\n",
        "  * 각 sample은 정수 인코딩이 된 결과로, 정수의 시퀀스\n",
        "* Embedding()은 워드 임베딩 작업 수행하고 (# of samples, input_length, embedding word dimensionality)인 3D 실수 텐서 리턴\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3KC6Ny9jQ7P"
      },
      "source": [
        "#### 2) 임베딩 층 사용하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4g2-6PntkD3S"
      },
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OVJ1npWskMXs"
      },
      "source": [
        "# 문장과 레이블 데이터를 만들었음\n",
        "# 긍정인 문장은 레이블 1, 부정인 문장은 레이블이 0\n",
        "sentences = ['nice great best amazing', 'stop lies', 'pitiful nerd', 'excellent work', 'supreme quality', 'bad', 'hhighly respectable']\n",
        "y_train = [1, 0, 0, 1, 1, 0, 1]"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mHu88TjRkMNc",
        "outputId": "7d954ba4-fb42-4c59-d543-e3b41a6c005e"
      },
      "source": [
        "# 케라스의 Tokenizer()를 사용하여 토큰화 시킴\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print(vocab_size)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rbI9GRuzkL98",
        "outputId": "49330cde-8377-46bf-df8e-afc4d79c935c"
      },
      "source": [
        "# 각 문장에 대해서 정수 인코딩 수행\n",
        "X_encoded = tokenizer.texts_to_sequences(sentences)\n",
        "print(X_encoded)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1, 2, 3, 4], [5, 6], [7, 8], [9, 10], [11, 12], [13], [14, 15]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yeZOvnxGk2OW",
        "outputId": "942bacbe-deb1-4338-e4e6-532fc7a325e6"
      },
      "source": [
        "# 가장 길이가 긴 문장의 길이\n",
        "max_len = max(len(l) for l in X_encoded)\n",
        "print(max_len)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9AKryqAolbuv",
        "outputId": "abf86103-76c1-4122-a750-e1ec56ff138e"
      },
      "source": [
        "# 모든 문장을 패딩하여 길이를 4로 만들어줌\n",
        "# 훈련 데이터에 대한 전처리가 끝남\n",
        "X_train = pad_sequences(X_encoded, maxlen=max_len, padding='post')\n",
        "y_train = np.array(y_train)\n",
        "print(X_train)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 1  2  3  4]\n",
            " [ 5  6  0  0]\n",
            " [ 7  8  0  0]\n",
            " [ 9 10  0  0]\n",
            " [11 12  0  0]\n",
            " [13  0  0  0]\n",
            " [14 15  0  0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3dxXqZZ1lbk2"
      },
      "source": [
        "# 모델 설계\n",
        "# 출력층에 1개의 뉴런에 활성화 함수로는 시그모이드 함수를 사용하여 이진 분류 수행\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, Flatten\n",
        "\n",
        "embedding_dim = 4\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, embedding_dim, input_length=max_len))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W16TXXm2lbaB",
        "outputId": "92b1cfa0-36b3-4a2e-838f-cac1414cd72b"
      },
      "source": [
        "# 테스트 데이터에 대한 정확도가 아니며, 훈련 데이터도 양이 적어서 정확도에는 의미는 없지만\n",
        "# 말하고자 하는 바: 현재 각 단어들의 임베딩 벡터들의 값은 학습 과정에서 다른 가중치들과 함께 학습된 값\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "model.fit(X_train, y_train, epochs=100, verbose=2)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "1/1 - 1s - loss: 0.6866 - acc: 0.7143\n",
            "Epoch 2/100\n",
            "1/1 - 0s - loss: 0.6850 - acc: 0.7143\n",
            "Epoch 3/100\n",
            "1/1 - 0s - loss: 0.6834 - acc: 0.7143\n",
            "Epoch 4/100\n",
            "1/1 - 0s - loss: 0.6819 - acc: 0.7143\n",
            "Epoch 5/100\n",
            "1/1 - 0s - loss: 0.6803 - acc: 0.7143\n",
            "Epoch 6/100\n",
            "1/1 - 0s - loss: 0.6787 - acc: 0.7143\n",
            "Epoch 7/100\n",
            "1/1 - 0s - loss: 0.6771 - acc: 0.8571\n",
            "Epoch 8/100\n",
            "1/1 - 0s - loss: 0.6755 - acc: 0.8571\n",
            "Epoch 9/100\n",
            "1/1 - 0s - loss: 0.6740 - acc: 0.8571\n",
            "Epoch 10/100\n",
            "1/1 - 0s - loss: 0.6724 - acc: 0.8571\n",
            "Epoch 11/100\n",
            "1/1 - 0s - loss: 0.6708 - acc: 0.8571\n",
            "Epoch 12/100\n",
            "1/1 - 0s - loss: 0.6692 - acc: 0.8571\n",
            "Epoch 13/100\n",
            "1/1 - 0s - loss: 0.6675 - acc: 0.8571\n",
            "Epoch 14/100\n",
            "1/1 - 0s - loss: 0.6659 - acc: 0.8571\n",
            "Epoch 15/100\n",
            "1/1 - 0s - loss: 0.6643 - acc: 0.8571\n",
            "Epoch 16/100\n",
            "1/1 - 0s - loss: 0.6627 - acc: 0.8571\n",
            "Epoch 17/100\n",
            "1/1 - 0s - loss: 0.6611 - acc: 0.8571\n",
            "Epoch 18/100\n",
            "1/1 - 0s - loss: 0.6594 - acc: 0.8571\n",
            "Epoch 19/100\n",
            "1/1 - 0s - loss: 0.6578 - acc: 0.8571\n",
            "Epoch 20/100\n",
            "1/1 - 0s - loss: 0.6562 - acc: 0.8571\n",
            "Epoch 21/100\n",
            "1/1 - 0s - loss: 0.6545 - acc: 0.8571\n",
            "Epoch 22/100\n",
            "1/1 - 0s - loss: 0.6529 - acc: 0.8571\n",
            "Epoch 23/100\n",
            "1/1 - 0s - loss: 0.6512 - acc: 0.8571\n",
            "Epoch 24/100\n",
            "1/1 - 0s - loss: 0.6495 - acc: 0.8571\n",
            "Epoch 25/100\n",
            "1/1 - 0s - loss: 0.6479 - acc: 0.8571\n",
            "Epoch 26/100\n",
            "1/1 - 0s - loss: 0.6462 - acc: 0.8571\n",
            "Epoch 27/100\n",
            "1/1 - 0s - loss: 0.6445 - acc: 0.8571\n",
            "Epoch 28/100\n",
            "1/1 - 0s - loss: 0.6428 - acc: 0.8571\n",
            "Epoch 29/100\n",
            "1/1 - 0s - loss: 0.6411 - acc: 0.8571\n",
            "Epoch 30/100\n",
            "1/1 - 0s - loss: 0.6394 - acc: 0.8571\n",
            "Epoch 31/100\n",
            "1/1 - 0s - loss: 0.6377 - acc: 0.8571\n",
            "Epoch 32/100\n",
            "1/1 - 0s - loss: 0.6360 - acc: 0.8571\n",
            "Epoch 33/100\n",
            "1/1 - 0s - loss: 0.6342 - acc: 0.8571\n",
            "Epoch 34/100\n",
            "1/1 - 0s - loss: 0.6325 - acc: 0.8571\n",
            "Epoch 35/100\n",
            "1/1 - 0s - loss: 0.6308 - acc: 0.8571\n",
            "Epoch 36/100\n",
            "1/1 - 0s - loss: 0.6290 - acc: 0.8571\n",
            "Epoch 37/100\n",
            "1/1 - 0s - loss: 0.6273 - acc: 0.8571\n",
            "Epoch 38/100\n",
            "1/1 - 0s - loss: 0.6255 - acc: 0.8571\n",
            "Epoch 39/100\n",
            "1/1 - 0s - loss: 0.6238 - acc: 0.8571\n",
            "Epoch 40/100\n",
            "1/1 - 0s - loss: 0.6220 - acc: 0.8571\n",
            "Epoch 41/100\n",
            "1/1 - 0s - loss: 0.6202 - acc: 0.8571\n",
            "Epoch 42/100\n",
            "1/1 - 0s - loss: 0.6184 - acc: 0.8571\n",
            "Epoch 43/100\n",
            "1/1 - 0s - loss: 0.6166 - acc: 0.8571\n",
            "Epoch 44/100\n",
            "1/1 - 0s - loss: 0.6148 - acc: 1.0000\n",
            "Epoch 45/100\n",
            "1/1 - 0s - loss: 0.6130 - acc: 1.0000\n",
            "Epoch 46/100\n",
            "1/1 - 0s - loss: 0.6112 - acc: 1.0000\n",
            "Epoch 47/100\n",
            "1/1 - 0s - loss: 0.6094 - acc: 1.0000\n",
            "Epoch 48/100\n",
            "1/1 - 0s - loss: 0.6075 - acc: 1.0000\n",
            "Epoch 49/100\n",
            "1/1 - 0s - loss: 0.6057 - acc: 1.0000\n",
            "Epoch 50/100\n",
            "1/1 - 0s - loss: 0.6038 - acc: 1.0000\n",
            "Epoch 51/100\n",
            "1/1 - 0s - loss: 0.6020 - acc: 1.0000\n",
            "Epoch 52/100\n",
            "1/1 - 0s - loss: 0.6001 - acc: 1.0000\n",
            "Epoch 53/100\n",
            "1/1 - 0s - loss: 0.5983 - acc: 1.0000\n",
            "Epoch 54/100\n",
            "1/1 - 0s - loss: 0.5964 - acc: 1.0000\n",
            "Epoch 55/100\n",
            "1/1 - 0s - loss: 0.5945 - acc: 1.0000\n",
            "Epoch 56/100\n",
            "1/1 - 0s - loss: 0.5926 - acc: 1.0000\n",
            "Epoch 57/100\n",
            "1/1 - 0s - loss: 0.5907 - acc: 1.0000\n",
            "Epoch 58/100\n",
            "1/1 - 0s - loss: 0.5888 - acc: 1.0000\n",
            "Epoch 59/100\n",
            "1/1 - 0s - loss: 0.5869 - acc: 1.0000\n",
            "Epoch 60/100\n",
            "1/1 - 0s - loss: 0.5850 - acc: 1.0000\n",
            "Epoch 61/100\n",
            "1/1 - 0s - loss: 0.5831 - acc: 1.0000\n",
            "Epoch 62/100\n",
            "1/1 - 0s - loss: 0.5811 - acc: 1.0000\n",
            "Epoch 63/100\n",
            "1/1 - 0s - loss: 0.5792 - acc: 1.0000\n",
            "Epoch 64/100\n",
            "1/1 - 0s - loss: 0.5773 - acc: 1.0000\n",
            "Epoch 65/100\n",
            "1/1 - 0s - loss: 0.5753 - acc: 1.0000\n",
            "Epoch 66/100\n",
            "1/1 - 0s - loss: 0.5733 - acc: 1.0000\n",
            "Epoch 67/100\n",
            "1/1 - 0s - loss: 0.5714 - acc: 1.0000\n",
            "Epoch 68/100\n",
            "1/1 - 0s - loss: 0.5694 - acc: 1.0000\n",
            "Epoch 69/100\n",
            "1/1 - 0s - loss: 0.5674 - acc: 1.0000\n",
            "Epoch 70/100\n",
            "1/1 - 0s - loss: 0.5655 - acc: 1.0000\n",
            "Epoch 71/100\n",
            "1/1 - 0s - loss: 0.5635 - acc: 1.0000\n",
            "Epoch 72/100\n",
            "1/1 - 0s - loss: 0.5615 - acc: 1.0000\n",
            "Epoch 73/100\n",
            "1/1 - 0s - loss: 0.5595 - acc: 1.0000\n",
            "Epoch 74/100\n",
            "1/1 - 0s - loss: 0.5575 - acc: 1.0000\n",
            "Epoch 75/100\n",
            "1/1 - 0s - loss: 0.5555 - acc: 1.0000\n",
            "Epoch 76/100\n",
            "1/1 - 0s - loss: 0.5534 - acc: 1.0000\n",
            "Epoch 77/100\n",
            "1/1 - 0s - loss: 0.5514 - acc: 1.0000\n",
            "Epoch 78/100\n",
            "1/1 - 0s - loss: 0.5494 - acc: 1.0000\n",
            "Epoch 79/100\n",
            "1/1 - 0s - loss: 0.5474 - acc: 1.0000\n",
            "Epoch 80/100\n",
            "1/1 - 0s - loss: 0.5453 - acc: 1.0000\n",
            "Epoch 81/100\n",
            "1/1 - 0s - loss: 0.5433 - acc: 1.0000\n",
            "Epoch 82/100\n",
            "1/1 - 0s - loss: 0.5412 - acc: 1.0000\n",
            "Epoch 83/100\n",
            "1/1 - 0s - loss: 0.5392 - acc: 1.0000\n",
            "Epoch 84/100\n",
            "1/1 - 0s - loss: 0.5371 - acc: 1.0000\n",
            "Epoch 85/100\n",
            "1/1 - 0s - loss: 0.5350 - acc: 1.0000\n",
            "Epoch 86/100\n",
            "1/1 - 0s - loss: 0.5330 - acc: 1.0000\n",
            "Epoch 87/100\n",
            "1/1 - 0s - loss: 0.5309 - acc: 1.0000\n",
            "Epoch 88/100\n",
            "1/1 - 0s - loss: 0.5288 - acc: 1.0000\n",
            "Epoch 89/100\n",
            "1/1 - 0s - loss: 0.5267 - acc: 1.0000\n",
            "Epoch 90/100\n",
            "1/1 - 0s - loss: 0.5247 - acc: 1.0000\n",
            "Epoch 91/100\n",
            "1/1 - 0s - loss: 0.5226 - acc: 1.0000\n",
            "Epoch 92/100\n",
            "1/1 - 0s - loss: 0.5205 - acc: 1.0000\n",
            "Epoch 93/100\n",
            "1/1 - 0s - loss: 0.5184 - acc: 1.0000\n",
            "Epoch 94/100\n",
            "1/1 - 0s - loss: 0.5163 - acc: 1.0000\n",
            "Epoch 95/100\n",
            "1/1 - 0s - loss: 0.5142 - acc: 1.0000\n",
            "Epoch 96/100\n",
            "1/1 - 0s - loss: 0.5121 - acc: 1.0000\n",
            "Epoch 97/100\n",
            "1/1 - 0s - loss: 0.5100 - acc: 1.0000\n",
            "Epoch 98/100\n",
            "1/1 - 0s - loss: 0.5078 - acc: 1.0000\n",
            "Epoch 99/100\n",
            "1/1 - 0s - loss: 0.5057 - acc: 1.0000\n",
            "Epoch 100/100\n",
            "1/1 - 0s - loss: 0.5036 - acc: 1.0000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fd40e0d8e50>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lK8RRNZcsCjX"
      },
      "source": [
        "### 2. 사전 훈련된 워드 임베딩 Pre-Trained Word Embedding 사용하기\n",
        "* GloVe 다운로드 링크 : http://nlp.stanford.edu/data/glove.6B.zip\n",
        "* Word2Vec 다운로드 링크 : https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yv7OX84ps3H-"
      },
      "source": [
        "#### 1) 사전 훈련된 GloVe 사용하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dEAiLI-ntFh6"
      },
      "source": [
        "from urllib.request import urlretrieve, urlopen\n",
        "import gzip\n",
        "import zipfile"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NP8n_dLxsokt"
      },
      "source": [
        "# 임베딩 층을 설계하기 위한 과정부터 달라짐\n",
        "# 압축본 안에 4개의 파일. 이 중 glove.6B.100d.txt 파일 사용"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57BtJbjttJya"
      },
      "source": [
        "urlretrieve(\"http://nlp.stanford.edu/data/glove.6B.zip\", filename=\"glove.6B.zip\")\n",
        "zf = zipfile.ZipFile('glove.6B.zip')\n",
        "zf.extractall()\n",
        "zf.close()"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FZDoAZ4UtzBF",
        "outputId": "c9aba6b6-401a-4d9d-d4fd-95efe3de5ba5"
      },
      "source": [
        "# glove.6B.100d.txt에 있는 모든 임베딩 벡터들 불러모음\n",
        "# 형식은 파이썬의 Dictionary 구조\n",
        "embedding_dict = dict()\n",
        "\n",
        "f = open('glove.6B.100d.txt', encoding=\"utf-8\")\n",
        "\n",
        "for line in f:\n",
        "  word_vector = line.split()\n",
        "  word = word_vector[0]\n",
        "\n",
        "  # 100개의 값을 가지는 array로 변환\n",
        "  word_vector_arr = np.asarray(word_vector[1:], dtype='float32')\n",
        "  embedding_dict[word] = word_vector_arr\n",
        "f.close()\n",
        "\n",
        "print('%s개의 Embedding vector가 있습니다.' % len(embedding_dict))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "400000개의 Embedding vector가 있습니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eIMDn0qDuTlg",
        "outputId": "be58763b-111f-431e-8550-4a333e39bb70"
      },
      "source": [
        "# 임의의 단어 respectable에 대해서 임베딩 벡터 출력\n",
        "print(embedding_dict['respectable'])\n",
        "print(len(embedding_dict['respectable']))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.049773   0.19903    0.10585    0.1391    -0.32395    0.44053\n",
            "  0.3947    -0.22805   -0.25793    0.49768    0.15384   -0.08831\n",
            "  0.0782    -0.8299    -0.037788   0.16772   -0.45197   -0.17085\n",
            "  0.74756    0.98256    0.81872    0.28507    0.16178   -0.48626\n",
            " -0.006265  -0.92469   -0.30625   -0.067318  -0.046762  -0.76291\n",
            " -0.0025264 -0.018795   0.12882   -0.52457    0.3586     0.43119\n",
            " -0.89477   -0.057421  -0.53724    0.25587    0.55195    0.44698\n",
            " -0.24252    0.29946    0.25776   -0.8717     0.68426   -0.05688\n",
            " -0.1848    -0.59352   -0.11227   -0.57692   -0.013593   0.18488\n",
            " -0.32507   -0.90171    0.17672    0.075601   0.54896   -0.21488\n",
            " -0.54018   -0.45882   -0.79536    0.26331    0.18879   -0.16363\n",
            "  0.3975     0.1099     0.1164    -0.083499   0.50159    0.35802\n",
            "  0.25677    0.088546   0.42108    0.28674   -0.71285   -0.82915\n",
            "  0.15297   -0.82712    0.022112   1.067     -0.31776    0.1211\n",
            " -0.069755  -0.61327    0.27308   -0.42638   -0.085084  -0.17694\n",
            " -0.0090944  0.1109     0.62543   -0.23682   -0.44928   -0.3667\n",
            " -0.21616   -0.19187   -0.032502   0.38025  ]\n",
            "100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QkNMD2u7ufna",
        "outputId": "89d459a7-c97b-46f3-a84c-27bfabc9f474"
      },
      "source": [
        "# 벡터값이 출력되며 길이는 100인 것을 확인\n",
        "# 단어 집합 크기의 행과 100개의 열을 가지는 행렬 생성. 행렬의 값은 전부 0으로 채움. 이 행렬에 사전 훈련된 임베딩 값을 넣어줄 것.\n",
        "\n",
        "embedding_matrix = np.zeros((vocab_size, 100))\n",
        "np.shape(embedding_matrix)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(16, 100)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HI-60nb1usIV",
        "outputId": "acfb8bee-e96e-4273-d023-6b0635680666"
      },
      "source": [
        "print(tokenizer.word_index.items())"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_items([('nice', 1), ('great', 2), ('best', 3), ('amazing', 4), ('stop', 5), ('lies', 6), ('pitiful', 7), ('nerd', 8), ('excellent', 9), ('work', 10), ('supreme', 11), ('quality', 12), ('bad', 13), ('hhighly', 14), ('respectable', 15)])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pxQSjl1iu0LT",
        "outputId": "6b8febe1-3522-4ed8-b323-7579e7a836ff"
      },
      "source": [
        "# 사전 훈련된 GloVe에서 great의 벡터값 확인\n",
        "print(embedding_dict['great'])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.013786   0.38216    0.53236    0.15261   -0.29694   -0.20558\n",
            " -0.41846   -0.58437   -0.77355   -0.87866   -0.37858   -0.18516\n",
            " -0.128     -0.20584   -0.22925   -0.42599    0.3725     0.26077\n",
            " -1.0702     0.62916   -0.091469   0.70348   -0.4973    -0.77691\n",
            "  0.66045    0.09465   -0.44893    0.018917   0.33146   -0.35022\n",
            " -0.35789    0.030313   0.22253   -0.23236   -0.19719   -0.0053125\n",
            " -0.25848    0.58081   -0.10705   -0.17845   -0.16206    0.087086\n",
            "  0.63029   -0.76649    0.51619    0.14073    1.019     -0.43136\n",
            "  0.46138   -0.43585   -0.47568    0.19226    0.36065    0.78987\n",
            "  0.088945  -2.7814    -0.15366    0.01015    1.1798     0.15168\n",
            " -0.050112   1.2626    -0.77527    0.36031    0.95761   -0.11385\n",
            "  0.28035   -0.02591    0.31246   -0.15424    0.3778    -0.13599\n",
            "  0.2946    -0.31579    0.42943    0.086969   0.019169  -0.27242\n",
            " -0.31696    0.37327    0.61997    0.13889    0.17188    0.30363\n",
            " -1.2776     0.044423  -0.52736   -0.88536   -0.19428   -0.61947\n",
            " -0.10146   -0.26301   -0.061707   0.36627   -0.95223   -0.39346\n",
            " -0.69183   -1.0426     0.28855    0.63056  ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQ3RMgxSu3sP"
      },
      "source": [
        "# 훈련 데이터의 단어 집합의 모든 단어에 대해서 사전 훈련된 GloVe의 임베딩 벡터들을 맵핑한 후에 great의 벡터값이 잘 들어갔는지 확인\n",
        "for word, index in tokenizer.word_index.items():\n",
        "  # 단어와 맵핑되는 사전 훈련된 임베딩 벡터값\n",
        "  vector_value = embedding_dict.get(word)\n",
        "  if vector_value is not None:\n",
        "    embedding_matrix[index] = vector_value"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ionens4DvYx-",
        "outputId": "569bba09-cb74-4e2c-b913-4c0de7fb0d20"
      },
      "source": [
        "embedding_matrix[2]\n",
        "\n",
        "# 이전에 확인한 사전에 훈련된 GloVe에서의 great의 벡터값과 일치"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.013786  ,  0.38216001,  0.53236002,  0.15261   , -0.29694   ,\n",
              "       -0.20558   , -0.41846001, -0.58437002, -0.77354997, -0.87866002,\n",
              "       -0.37858   , -0.18516   , -0.12800001, -0.20584001, -0.22925   ,\n",
              "       -0.42598999,  0.3725    ,  0.26076999, -1.07019997,  0.62915999,\n",
              "       -0.091469  ,  0.70348001, -0.4973    , -0.77691001,  0.66044998,\n",
              "        0.09465   , -0.44893   ,  0.018917  ,  0.33146   , -0.35021999,\n",
              "       -0.35789001,  0.030313  ,  0.22253001, -0.23236001, -0.19719   ,\n",
              "       -0.0053125 , -0.25848001,  0.58081001, -0.10705   , -0.17845   ,\n",
              "       -0.16205999,  0.087086  ,  0.63028997, -0.76648998,  0.51618999,\n",
              "        0.14072999,  1.01900005, -0.43136001,  0.46138   , -0.43584999,\n",
              "       -0.47567999,  0.19226   ,  0.36065   ,  0.78987002,  0.088945  ,\n",
              "       -2.78139997, -0.15366   ,  0.01015   ,  1.17980003,  0.15167999,\n",
              "       -0.050112  ,  1.26259995, -0.77526999,  0.36030999,  0.95761001,\n",
              "       -0.11385   ,  0.28035   , -0.02591   ,  0.31246001, -0.15424   ,\n",
              "        0.37779999, -0.13598999,  0.29460001, -0.31579   ,  0.42943001,\n",
              "        0.086969  ,  0.019169  , -0.27241999, -0.31696001,  0.37327   ,\n",
              "        0.61997002,  0.13889   ,  0.17188001,  0.30362999, -1.27760005,\n",
              "        0.044423  , -0.52736002, -0.88536   , -0.19428   , -0.61947   ,\n",
              "       -0.10146   , -0.26301   , -0.061707  ,  0.36627001, -0.95222998,\n",
              "       -0.39346001, -0.69182998, -1.04260004,  0.28854999,  0.63055998])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eSdKEnvuvgxJ",
        "outputId": "874f89be-f128-408f-8aa7-da38bcc1615a"
      },
      "source": [
        "# Embedding layer에 우리가 만든 매트릭스를 초기값으로 설정\n",
        "# 현재 실습에서 사전 훈련된 워드 임베딩을 100차원의 값인 것으로 사용하고 있기 때문에 임베딩 층의 output_dim의 인자값으로 100을 주어야 함\n",
        "# 사전 훈련된 워드 임베딩을 그대로 사용할 것이므로, 별도로 더 이상 훈련을 하지 않겠다는 옵션을 줌\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, Flatten\n",
        "\n",
        "model = Sequential()\n",
        "e = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length = max_len, trainable=False)\n",
        "model.add(e)\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "model.fit(X_train, y_train, epochs=100, verbose=2)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "1/1 - 0s - loss: 0.8543 - acc: 0.2857\n",
            "Epoch 2/100\n",
            "1/1 - 0s - loss: 0.8315 - acc: 0.2857\n",
            "Epoch 3/100\n",
            "1/1 - 0s - loss: 0.8094 - acc: 0.2857\n",
            "Epoch 4/100\n",
            "1/1 - 0s - loss: 0.7879 - acc: 0.2857\n",
            "Epoch 5/100\n",
            "1/1 - 0s - loss: 0.7672 - acc: 0.4286\n",
            "Epoch 6/100\n",
            "1/1 - 0s - loss: 0.7471 - acc: 0.4286\n",
            "Epoch 7/100\n",
            "1/1 - 0s - loss: 0.7276 - acc: 0.4286\n",
            "Epoch 8/100\n",
            "1/1 - 0s - loss: 0.7088 - acc: 0.4286\n",
            "Epoch 9/100\n",
            "1/1 - 0s - loss: 0.6905 - acc: 0.4286\n",
            "Epoch 10/100\n",
            "1/1 - 0s - loss: 0.6729 - acc: 0.4286\n",
            "Epoch 11/100\n",
            "1/1 - 0s - loss: 0.6559 - acc: 0.5714\n",
            "Epoch 12/100\n",
            "1/1 - 0s - loss: 0.6394 - acc: 0.5714\n",
            "Epoch 13/100\n",
            "1/1 - 0s - loss: 0.6234 - acc: 0.5714\n",
            "Epoch 14/100\n",
            "1/1 - 0s - loss: 0.6080 - acc: 0.7143\n",
            "Epoch 15/100\n",
            "1/1 - 0s - loss: 0.5931 - acc: 0.7143\n",
            "Epoch 16/100\n",
            "1/1 - 0s - loss: 0.5786 - acc: 0.8571\n",
            "Epoch 17/100\n",
            "1/1 - 0s - loss: 0.5646 - acc: 0.8571\n",
            "Epoch 18/100\n",
            "1/1 - 0s - loss: 0.5511 - acc: 0.8571\n",
            "Epoch 19/100\n",
            "1/1 - 0s - loss: 0.5379 - acc: 0.8571\n",
            "Epoch 20/100\n",
            "1/1 - 0s - loss: 0.5252 - acc: 0.8571\n",
            "Epoch 21/100\n",
            "1/1 - 0s - loss: 0.5129 - acc: 0.8571\n",
            "Epoch 22/100\n",
            "1/1 - 0s - loss: 0.5009 - acc: 0.8571\n",
            "Epoch 23/100\n",
            "1/1 - 0s - loss: 0.4893 - acc: 0.8571\n",
            "Epoch 24/100\n",
            "1/1 - 0s - loss: 0.4780 - acc: 0.8571\n",
            "Epoch 25/100\n",
            "1/1 - 0s - loss: 0.4671 - acc: 0.8571\n",
            "Epoch 26/100\n",
            "1/1 - 0s - loss: 0.4565 - acc: 0.8571\n",
            "Epoch 27/100\n",
            "1/1 - 0s - loss: 0.4462 - acc: 0.8571\n",
            "Epoch 28/100\n",
            "1/1 - 0s - loss: 0.4362 - acc: 0.8571\n",
            "Epoch 29/100\n",
            "1/1 - 0s - loss: 0.4265 - acc: 0.8571\n",
            "Epoch 30/100\n",
            "1/1 - 0s - loss: 0.4171 - acc: 0.8571\n",
            "Epoch 31/100\n",
            "1/1 - 0s - loss: 0.4079 - acc: 1.0000\n",
            "Epoch 32/100\n",
            "1/1 - 0s - loss: 0.3990 - acc: 1.0000\n",
            "Epoch 33/100\n",
            "1/1 - 0s - loss: 0.3904 - acc: 1.0000\n",
            "Epoch 34/100\n",
            "1/1 - 0s - loss: 0.3820 - acc: 1.0000\n",
            "Epoch 35/100\n",
            "1/1 - 0s - loss: 0.3738 - acc: 1.0000\n",
            "Epoch 36/100\n",
            "1/1 - 0s - loss: 0.3659 - acc: 1.0000\n",
            "Epoch 37/100\n",
            "1/1 - 0s - loss: 0.3583 - acc: 1.0000\n",
            "Epoch 38/100\n",
            "1/1 - 0s - loss: 0.3508 - acc: 1.0000\n",
            "Epoch 39/100\n",
            "1/1 - 0s - loss: 0.3436 - acc: 1.0000\n",
            "Epoch 40/100\n",
            "1/1 - 0s - loss: 0.3365 - acc: 1.0000\n",
            "Epoch 41/100\n",
            "1/1 - 0s - loss: 0.3297 - acc: 1.0000\n",
            "Epoch 42/100\n",
            "1/1 - 0s - loss: 0.3231 - acc: 1.0000\n",
            "Epoch 43/100\n",
            "1/1 - 0s - loss: 0.3166 - acc: 1.0000\n",
            "Epoch 44/100\n",
            "1/1 - 0s - loss: 0.3103 - acc: 1.0000\n",
            "Epoch 45/100\n",
            "1/1 - 0s - loss: 0.3043 - acc: 1.0000\n",
            "Epoch 46/100\n",
            "1/1 - 0s - loss: 0.2983 - acc: 1.0000\n",
            "Epoch 47/100\n",
            "1/1 - 0s - loss: 0.2926 - acc: 1.0000\n",
            "Epoch 48/100\n",
            "1/1 - 0s - loss: 0.2870 - acc: 1.0000\n",
            "Epoch 49/100\n",
            "1/1 - 0s - loss: 0.2816 - acc: 1.0000\n",
            "Epoch 50/100\n",
            "1/1 - 0s - loss: 0.2763 - acc: 1.0000\n",
            "Epoch 51/100\n",
            "1/1 - 0s - loss: 0.2712 - acc: 1.0000\n",
            "Epoch 52/100\n",
            "1/1 - 0s - loss: 0.2662 - acc: 1.0000\n",
            "Epoch 53/100\n",
            "1/1 - 0s - loss: 0.2613 - acc: 1.0000\n",
            "Epoch 54/100\n",
            "1/1 - 0s - loss: 0.2566 - acc: 1.0000\n",
            "Epoch 55/100\n",
            "1/1 - 0s - loss: 0.2520 - acc: 1.0000\n",
            "Epoch 56/100\n",
            "1/1 - 0s - loss: 0.2475 - acc: 1.0000\n",
            "Epoch 57/100\n",
            "1/1 - 0s - loss: 0.2431 - acc: 1.0000\n",
            "Epoch 58/100\n",
            "1/1 - 0s - loss: 0.2389 - acc: 1.0000\n",
            "Epoch 59/100\n",
            "1/1 - 0s - loss: 0.2348 - acc: 1.0000\n",
            "Epoch 60/100\n",
            "1/1 - 0s - loss: 0.2307 - acc: 1.0000\n",
            "Epoch 61/100\n",
            "1/1 - 0s - loss: 0.2268 - acc: 1.0000\n",
            "Epoch 62/100\n",
            "1/1 - 0s - loss: 0.2230 - acc: 1.0000\n",
            "Epoch 63/100\n",
            "1/1 - 0s - loss: 0.2192 - acc: 1.0000\n",
            "Epoch 64/100\n",
            "1/1 - 0s - loss: 0.2156 - acc: 1.0000\n",
            "Epoch 65/100\n",
            "1/1 - 0s - loss: 0.2121 - acc: 1.0000\n",
            "Epoch 66/100\n",
            "1/1 - 0s - loss: 0.2086 - acc: 1.0000\n",
            "Epoch 67/100\n",
            "1/1 - 0s - loss: 0.2052 - acc: 1.0000\n",
            "Epoch 68/100\n",
            "1/1 - 0s - loss: 0.2019 - acc: 1.0000\n",
            "Epoch 69/100\n",
            "1/1 - 0s - loss: 0.1987 - acc: 1.0000\n",
            "Epoch 70/100\n",
            "1/1 - 0s - loss: 0.1956 - acc: 1.0000\n",
            "Epoch 71/100\n",
            "1/1 - 0s - loss: 0.1925 - acc: 1.0000\n",
            "Epoch 72/100\n",
            "1/1 - 0s - loss: 0.1895 - acc: 1.0000\n",
            "Epoch 73/100\n",
            "1/1 - 0s - loss: 0.1866 - acc: 1.0000\n",
            "Epoch 74/100\n",
            "1/1 - 0s - loss: 0.1838 - acc: 1.0000\n",
            "Epoch 75/100\n",
            "1/1 - 0s - loss: 0.1810 - acc: 1.0000\n",
            "Epoch 76/100\n",
            "1/1 - 0s - loss: 0.1783 - acc: 1.0000\n",
            "Epoch 77/100\n",
            "1/1 - 0s - loss: 0.1756 - acc: 1.0000\n",
            "Epoch 78/100\n",
            "1/1 - 0s - loss: 0.1730 - acc: 1.0000\n",
            "Epoch 79/100\n",
            "1/1 - 0s - loss: 0.1705 - acc: 1.0000\n",
            "Epoch 80/100\n",
            "1/1 - 0s - loss: 0.1680 - acc: 1.0000\n",
            "Epoch 81/100\n",
            "1/1 - 0s - loss: 0.1656 - acc: 1.0000\n",
            "Epoch 82/100\n",
            "1/1 - 0s - loss: 0.1632 - acc: 1.0000\n",
            "Epoch 83/100\n",
            "1/1 - 0s - loss: 0.1609 - acc: 1.0000\n",
            "Epoch 84/100\n",
            "1/1 - 0s - loss: 0.1587 - acc: 1.0000\n",
            "Epoch 85/100\n",
            "1/1 - 0s - loss: 0.1564 - acc: 1.0000\n",
            "Epoch 86/100\n",
            "1/1 - 0s - loss: 0.1543 - acc: 1.0000\n",
            "Epoch 87/100\n",
            "1/1 - 0s - loss: 0.1522 - acc: 1.0000\n",
            "Epoch 88/100\n",
            "1/1 - 0s - loss: 0.1501 - acc: 1.0000\n",
            "Epoch 89/100\n",
            "1/1 - 0s - loss: 0.1480 - acc: 1.0000\n",
            "Epoch 90/100\n",
            "1/1 - 0s - loss: 0.1461 - acc: 1.0000\n",
            "Epoch 91/100\n",
            "1/1 - 0s - loss: 0.1441 - acc: 1.0000\n",
            "Epoch 92/100\n",
            "1/1 - 0s - loss: 0.1422 - acc: 1.0000\n",
            "Epoch 93/100\n",
            "1/1 - 0s - loss: 0.1403 - acc: 1.0000\n",
            "Epoch 94/100\n",
            "1/1 - 0s - loss: 0.1385 - acc: 1.0000\n",
            "Epoch 95/100\n",
            "1/1 - 0s - loss: 0.1367 - acc: 1.0000\n",
            "Epoch 96/100\n",
            "1/1 - 0s - loss: 0.1350 - acc: 1.0000\n",
            "Epoch 97/100\n",
            "1/1 - 0s - loss: 0.1332 - acc: 1.0000\n",
            "Epoch 98/100\n",
            "1/1 - 0s - loss: 0.1316 - acc: 1.0000\n",
            "Epoch 99/100\n",
            "1/1 - 0s - loss: 0.1299 - acc: 1.0000\n",
            "Epoch 100/100\n",
            "1/1 - 0s - loss: 0.1283 - acc: 1.0000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fd406157090>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJrYtUZOwKIT"
      },
      "source": [
        "#### 2) 사전 훈련된 Word2Vec 사용하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJ5rBlTxwKmc"
      },
      "source": [
        "import gensim"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YqyoVjzxwQvZ"
      },
      "source": [
        "# 구글의 사전 훈련된 Word2Vec 모델을 로드하여 word2vec_model에 저장\n",
        "urlretrieve(\"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\", \\\n",
        "                           filename=\"GoogleNews-vectors-negative300.bin.gz\")\n",
        "word2vec_model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uTj5EkPPwXBx",
        "outputId": "dfab9d06-017e-4616-ebb0-1a591d323085"
      },
      "source": [
        "print(word2vec_model.vectors.shape) # 모델의 크기 확인\n",
        "# 300의 차원을 가진 Word2Vec 벡터가 3,000,000개 있음"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3000000, 300)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7t537bd1wbMt",
        "outputId": "3396156d-1b32-4f61-b65c-b04ecc6951be"
      },
      "source": [
        "# 모든 값이 0으로 채워진 임베딩 행렬을 만들어줌\n",
        "# 문제의 단어는 총 16개. 16 x 300의 크기를 가진 행렬을 만듬\n",
        "embedding_matrix = np.zeros((vocab_size, 300))\n",
        "np.shape(embedding_matrix)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(16, 300)"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GM8CnIqawsti"
      },
      "source": [
        "# word2vec_model에서 특정 단어를 입력하면 해당 단어의 임베딩 벡터를 리턴받을텐데, 만약 word2vec_model에 특정 단어의 임베딩 벡터가 없다면 None을 리턴하도록 함\n",
        "def get_vector(word):\n",
        "  if word in word2vec_model:\n",
        "    return word2vec_model[word]\n",
        "  else:\n",
        "    return None"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5H8ojcVgyJyr"
      },
      "source": [
        "# 단어집합으로부터 단어를 1개씩 호출하여 word2vec_model에 해당 단어의 임베딩 벡터값이 존재하는지 확인\n",
        "# if None => 존재한다는 의미. 임베딩 행렬에 해당 단어의 인덱스 위치의 행에 임베딩 벡터의 값을 저장\n",
        "# 현재 풀고자 하는 문제의 16개의 단어와 맵핑되는 임베딩 행렬 완성\n",
        "for word, index in tokenizer.word_index.items():\n",
        "    # 단어와 맵핑되는 사전 훈련된 임베딩 벡터값\n",
        "    vector_value = get_vector(word)\n",
        "    if vector_value is not None:\n",
        "        embedding_matrix[index] = vector_value"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02buuQNTxaNO",
        "outputId": "ba5609c8-418f-4529-db27-60e6a6dd5767"
      },
      "source": [
        "# 기존 word2vec_model에 저장되어 있던 단어 nice의 임베딩 벡터값 확인\n",
        "print(word2vec_model['nice'])"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 0.15820312  0.10595703 -0.18945312  0.38671875  0.08349609 -0.26757812\n",
            "  0.08349609  0.11328125 -0.10400391  0.17871094 -0.12353516 -0.22265625\n",
            " -0.01806641 -0.25390625  0.13183594  0.0859375   0.16113281  0.11083984\n",
            " -0.11083984 -0.0859375   0.0267334   0.34570312  0.15136719 -0.00415039\n",
            "  0.10498047  0.04907227 -0.06982422  0.08642578  0.03198242 -0.02844238\n",
            " -0.15722656  0.11865234  0.36132812  0.00173187  0.05297852 -0.234375\n",
            "  0.11767578  0.08642578 -0.01123047  0.25976562  0.28515625 -0.11669922\n",
            "  0.38476562  0.07275391  0.01147461  0.03466797  0.18164062 -0.03955078\n",
            "  0.04199219  0.01013184 -0.06054688  0.09765625  0.06689453  0.14648438\n",
            " -0.12011719  0.08447266 -0.06152344  0.06347656  0.3046875  -0.35546875\n",
            " -0.2890625   0.19628906 -0.33203125 -0.07128906  0.12792969  0.09619141\n",
            " -0.12158203 -0.08691406 -0.12890625  0.27734375  0.265625    0.1796875\n",
            "  0.12695312  0.06298828 -0.34375    -0.05908203  0.0456543   0.171875\n",
            "  0.08935547  0.14648438 -0.04638672 -0.00842285 -0.0279541   0.234375\n",
            " -0.07470703 -0.13574219  0.00378418  0.19433594  0.05664062 -0.05419922\n",
            "  0.06176758  0.14160156 -0.24121094  0.02539062 -0.15917969 -0.10595703\n",
            "  0.11865234  0.24707031 -0.13574219 -0.20410156 -0.30078125  0.07910156\n",
            " -0.04394531  0.02026367 -0.05786133  0.2109375   0.13574219  0.08349609\n",
            " -0.0098877  -0.10546875 -0.08105469  0.03735352 -0.10351562 -0.10205078\n",
            "  0.23925781 -0.21875     0.05151367  0.06738281  0.07617188  0.04638672\n",
            "  0.03198242 -0.07275391  0.14550781  0.04858398 -0.05664062 -0.07470703\n",
            " -0.0030365  -0.09277344 -0.11083984 -0.03320312 -0.15234375 -0.12207031\n",
            "  0.09814453  0.375       0.00454712 -0.10009766  0.02734375  0.30078125\n",
            " -0.0390625   0.30078125 -0.04541016 -0.00424194  0.13671875 -0.18945312\n",
            " -0.21777344  0.12695312 -0.02746582 -0.18164062  0.08984375 -0.23339844\n",
            "  0.203125    0.2734375  -0.26953125  0.15332031 -0.20703125 -0.01153564\n",
            "  0.12451172  0.05395508 -0.23535156 -0.01409912 -0.09765625  0.20800781\n",
            "  0.19335938  0.14746094  0.28710938 -0.23046875  0.01965332 -0.09619141\n",
            " -0.0703125  -0.04174805 -0.17578125  0.0007019   0.10546875  0.10351562\n",
            "  0.02478027  0.35742188  0.17382812 -0.09570312 -0.18359375  0.23242188\n",
            " -0.14453125 -0.20410156 -0.01867676  0.06640625 -0.2265625  -0.00582886\n",
            " -0.08642578  0.02416992 -0.07324219 -0.29882812 -0.15625     0.07666016\n",
            "  0.19628906 -0.20410156  0.09863281 -0.01672363 -0.18652344 -0.12353516\n",
            " -0.16015625 -0.10058594  0.21777344  0.09375    -0.10058594 -0.03637695\n",
            "  0.15136719 -0.02526855 -0.23730469  0.03417969 -0.00604248  0.15625\n",
            " -0.14257812  0.18066406 -0.35351562  0.25        0.13085938 -0.04296875\n",
            "  0.17089844  0.20507812  0.00680542 -0.08251953 -0.06738281  0.22167969\n",
            " -0.16308594 -0.16699219 -0.02087402  0.11035156  0.06054688 -0.04223633\n",
            " -0.17285156  0.05029297 -0.19824219  0.01495361  0.06542969  0.03271484\n",
            "  0.14453125 -0.08691406 -0.11035156 -0.1484375   0.09667969  0.22363281\n",
            "  0.23535156  0.08398438  0.18164062 -0.10595703 -0.04296875  0.11572266\n",
            " -0.00153351  0.0534668  -0.1328125  -0.33203125 -0.08251953  0.30664062\n",
            "  0.22363281  0.27929688  0.09082031 -0.18066406 -0.00613403 -0.09423828\n",
            " -0.21289062  0.01965332 -0.08105469 -0.06689453 -0.31835938 -0.08447266\n",
            "  0.13574219  0.0625      0.07080078 -0.14257812 -0.11279297  0.01452637\n",
            " -0.06689453  0.03881836  0.19433594  0.09521484  0.11376953 -0.12451172\n",
            "  0.13769531 -0.18847656 -0.05224609  0.15820312  0.09863281 -0.04370117\n",
            " -0.06054688  0.21679688  0.04077148 -0.14648438 -0.18945312 -0.25195312\n",
            " -0.16894531 -0.08642578 -0.08544922  0.18945312 -0.14648438  0.13476562\n",
            " -0.04077148  0.03271484  0.08935547 -0.26757812  0.00836182 -0.21386719]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "urWkCNW6xpzo",
        "outputId": "7c7a08d1-3392-43a9-ec21-35c4320f0f05"
      },
      "source": [
        "# nice는 현재 단어 집합에서 몇 번 인덱스를 가지는지 확인\n",
        "print('단어 nice의 정수 인덱스 :', tokenizer.word_index['nice'])"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "단어 nice의 정수 인덱스 : 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nmc7c4_3xxAX",
        "outputId": "84f6b42b-9e71-4b7c-c698-4e78bd8ec42d"
      },
      "source": [
        "# 1의 값을 가지므로 embedding_matrix의 1번 인덱스에는 단어 'nice'의 임베딩 벡터값이 있어야 함\n",
        "# word2vec_model에서 확인했던 것과 동일한 것을 확인 가능\n",
        "print(embedding_matrix[1])"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 0.15820312  0.10595703 -0.18945312  0.38671875  0.08349609 -0.26757812\n",
            "  0.08349609  0.11328125 -0.10400391  0.17871094 -0.12353516 -0.22265625\n",
            " -0.01806641 -0.25390625  0.13183594  0.0859375   0.16113281  0.11083984\n",
            " -0.11083984 -0.0859375   0.0267334   0.34570312  0.15136719 -0.00415039\n",
            "  0.10498047  0.04907227 -0.06982422  0.08642578  0.03198242 -0.02844238\n",
            " -0.15722656  0.11865234  0.36132812  0.00173187  0.05297852 -0.234375\n",
            "  0.11767578  0.08642578 -0.01123047  0.25976562  0.28515625 -0.11669922\n",
            "  0.38476562  0.07275391  0.01147461  0.03466797  0.18164062 -0.03955078\n",
            "  0.04199219  0.01013184 -0.06054688  0.09765625  0.06689453  0.14648438\n",
            " -0.12011719  0.08447266 -0.06152344  0.06347656  0.3046875  -0.35546875\n",
            " -0.2890625   0.19628906 -0.33203125 -0.07128906  0.12792969  0.09619141\n",
            " -0.12158203 -0.08691406 -0.12890625  0.27734375  0.265625    0.1796875\n",
            "  0.12695312  0.06298828 -0.34375    -0.05908203  0.0456543   0.171875\n",
            "  0.08935547  0.14648438 -0.04638672 -0.00842285 -0.0279541   0.234375\n",
            " -0.07470703 -0.13574219  0.00378418  0.19433594  0.05664062 -0.05419922\n",
            "  0.06176758  0.14160156 -0.24121094  0.02539062 -0.15917969 -0.10595703\n",
            "  0.11865234  0.24707031 -0.13574219 -0.20410156 -0.30078125  0.07910156\n",
            " -0.04394531  0.02026367 -0.05786133  0.2109375   0.13574219  0.08349609\n",
            " -0.0098877  -0.10546875 -0.08105469  0.03735352 -0.10351562 -0.10205078\n",
            "  0.23925781 -0.21875     0.05151367  0.06738281  0.07617188  0.04638672\n",
            "  0.03198242 -0.07275391  0.14550781  0.04858398 -0.05664062 -0.07470703\n",
            " -0.0030365  -0.09277344 -0.11083984 -0.03320312 -0.15234375 -0.12207031\n",
            "  0.09814453  0.375       0.00454712 -0.10009766  0.02734375  0.30078125\n",
            " -0.0390625   0.30078125 -0.04541016 -0.00424194  0.13671875 -0.18945312\n",
            " -0.21777344  0.12695312 -0.02746582 -0.18164062  0.08984375 -0.23339844\n",
            "  0.203125    0.2734375  -0.26953125  0.15332031 -0.20703125 -0.01153564\n",
            "  0.12451172  0.05395508 -0.23535156 -0.01409912 -0.09765625  0.20800781\n",
            "  0.19335938  0.14746094  0.28710938 -0.23046875  0.01965332 -0.09619141\n",
            " -0.0703125  -0.04174805 -0.17578125  0.0007019   0.10546875  0.10351562\n",
            "  0.02478027  0.35742188  0.17382812 -0.09570312 -0.18359375  0.23242188\n",
            " -0.14453125 -0.20410156 -0.01867676  0.06640625 -0.2265625  -0.00582886\n",
            " -0.08642578  0.02416992 -0.07324219 -0.29882812 -0.15625     0.07666016\n",
            "  0.19628906 -0.20410156  0.09863281 -0.01672363 -0.18652344 -0.12353516\n",
            " -0.16015625 -0.10058594  0.21777344  0.09375    -0.10058594 -0.03637695\n",
            "  0.15136719 -0.02526855 -0.23730469  0.03417969 -0.00604248  0.15625\n",
            " -0.14257812  0.18066406 -0.35351562  0.25        0.13085938 -0.04296875\n",
            "  0.17089844  0.20507812  0.00680542 -0.08251953 -0.06738281  0.22167969\n",
            " -0.16308594 -0.16699219 -0.02087402  0.11035156  0.06054688 -0.04223633\n",
            " -0.17285156  0.05029297 -0.19824219  0.01495361  0.06542969  0.03271484\n",
            "  0.14453125 -0.08691406 -0.11035156 -0.1484375   0.09667969  0.22363281\n",
            "  0.23535156  0.08398438  0.18164062 -0.10595703 -0.04296875  0.11572266\n",
            " -0.00153351  0.0534668  -0.1328125  -0.33203125 -0.08251953  0.30664062\n",
            "  0.22363281  0.27929688  0.09082031 -0.18066406 -0.00613403 -0.09423828\n",
            " -0.21289062  0.01965332 -0.08105469 -0.06689453 -0.31835938 -0.08447266\n",
            "  0.13574219  0.0625      0.07080078 -0.14257812 -0.11279297  0.01452637\n",
            " -0.06689453  0.03881836  0.19433594  0.09521484  0.11376953 -0.12451172\n",
            "  0.13769531 -0.18847656 -0.05224609  0.15820312  0.09863281 -0.04370117\n",
            " -0.06054688  0.21679688  0.04077148 -0.14648438 -0.18945312 -0.25195312\n",
            " -0.16894531 -0.08642578 -0.08544922  0.18945312 -0.14648438  0.13476562\n",
            " -0.04077148  0.03271484  0.08935547 -0.26757812  0.00836182 -0.21386719]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T_xkLxtyyd7b",
        "outputId": "1dd8fcd2-bb44-47aa-c12d-460dee32330f"
      },
      "source": [
        "# Embedding에 사전 훈련된 embedding_matrix를 입력으로 넣어주고 모델을 학습\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, Flatten, Input\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Input(shape=(max_len, ), dtype='int32'))\n",
        "e = Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=max_len, trainable=False)\n",
        "model.add(e)\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "model.fit(X_train, y_train, epochs=100, verbose=2)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "1/1 - 0s - loss: 0.6842 - acc: 0.5714\n",
            "Epoch 2/100\n",
            "1/1 - 0s - loss: 0.6662 - acc: 0.7143\n",
            "Epoch 3/100\n",
            "1/1 - 0s - loss: 0.6488 - acc: 0.7143\n",
            "Epoch 4/100\n",
            "1/1 - 0s - loss: 0.6319 - acc: 0.8571\n",
            "Epoch 5/100\n",
            "1/1 - 0s - loss: 0.6155 - acc: 0.8571\n",
            "Epoch 6/100\n",
            "1/1 - 0s - loss: 0.5996 - acc: 0.8571\n",
            "Epoch 7/100\n",
            "1/1 - 0s - loss: 0.5844 - acc: 1.0000\n",
            "Epoch 8/100\n",
            "1/1 - 0s - loss: 0.5696 - acc: 1.0000\n",
            "Epoch 9/100\n",
            "1/1 - 0s - loss: 0.5554 - acc: 1.0000\n",
            "Epoch 10/100\n",
            "1/1 - 0s - loss: 0.5416 - acc: 1.0000\n",
            "Epoch 11/100\n",
            "1/1 - 0s - loss: 0.5284 - acc: 1.0000\n",
            "Epoch 12/100\n",
            "1/1 - 0s - loss: 0.5156 - acc: 1.0000\n",
            "Epoch 13/100\n",
            "1/1 - 0s - loss: 0.5033 - acc: 1.0000\n",
            "Epoch 14/100\n",
            "1/1 - 0s - loss: 0.4915 - acc: 1.0000\n",
            "Epoch 15/100\n",
            "1/1 - 0s - loss: 0.4800 - acc: 1.0000\n",
            "Epoch 16/100\n",
            "1/1 - 0s - loss: 0.4690 - acc: 1.0000\n",
            "Epoch 17/100\n",
            "1/1 - 0s - loss: 0.4583 - acc: 1.0000\n",
            "Epoch 18/100\n",
            "1/1 - 0s - loss: 0.4480 - acc: 1.0000\n",
            "Epoch 19/100\n",
            "1/1 - 0s - loss: 0.4380 - acc: 1.0000\n",
            "Epoch 20/100\n",
            "1/1 - 0s - loss: 0.4284 - acc: 1.0000\n",
            "Epoch 21/100\n",
            "1/1 - 0s - loss: 0.4191 - acc: 1.0000\n",
            "Epoch 22/100\n",
            "1/1 - 0s - loss: 0.4101 - acc: 1.0000\n",
            "Epoch 23/100\n",
            "1/1 - 0s - loss: 0.4014 - acc: 1.0000\n",
            "Epoch 24/100\n",
            "1/1 - 0s - loss: 0.3929 - acc: 1.0000\n",
            "Epoch 25/100\n",
            "1/1 - 0s - loss: 0.3847 - acc: 1.0000\n",
            "Epoch 26/100\n",
            "1/1 - 0s - loss: 0.3768 - acc: 1.0000\n",
            "Epoch 27/100\n",
            "1/1 - 0s - loss: 0.3691 - acc: 1.0000\n",
            "Epoch 28/100\n",
            "1/1 - 0s - loss: 0.3616 - acc: 1.0000\n",
            "Epoch 29/100\n",
            "1/1 - 0s - loss: 0.3543 - acc: 1.0000\n",
            "Epoch 30/100\n",
            "1/1 - 0s - loss: 0.3473 - acc: 1.0000\n",
            "Epoch 31/100\n",
            "1/1 - 0s - loss: 0.3404 - acc: 1.0000\n",
            "Epoch 32/100\n",
            "1/1 - 0s - loss: 0.3338 - acc: 1.0000\n",
            "Epoch 33/100\n",
            "1/1 - 0s - loss: 0.3273 - acc: 1.0000\n",
            "Epoch 34/100\n",
            "1/1 - 0s - loss: 0.3210 - acc: 1.0000\n",
            "Epoch 35/100\n",
            "1/1 - 0s - loss: 0.3149 - acc: 1.0000\n",
            "Epoch 36/100\n",
            "1/1 - 0s - loss: 0.3090 - acc: 1.0000\n",
            "Epoch 37/100\n",
            "1/1 - 0s - loss: 0.3032 - acc: 1.0000\n",
            "Epoch 38/100\n",
            "1/1 - 0s - loss: 0.2976 - acc: 1.0000\n",
            "Epoch 39/100\n",
            "1/1 - 0s - loss: 0.2921 - acc: 1.0000\n",
            "Epoch 40/100\n",
            "1/1 - 0s - loss: 0.2867 - acc: 1.0000\n",
            "Epoch 41/100\n",
            "1/1 - 0s - loss: 0.2815 - acc: 1.0000\n",
            "Epoch 42/100\n",
            "1/1 - 0s - loss: 0.2765 - acc: 1.0000\n",
            "Epoch 43/100\n",
            "1/1 - 0s - loss: 0.2716 - acc: 1.0000\n",
            "Epoch 44/100\n",
            "1/1 - 0s - loss: 0.2668 - acc: 1.0000\n",
            "Epoch 45/100\n",
            "1/1 - 0s - loss: 0.2621 - acc: 1.0000\n",
            "Epoch 46/100\n",
            "1/1 - 0s - loss: 0.2575 - acc: 1.0000\n",
            "Epoch 47/100\n",
            "1/1 - 0s - loss: 0.2531 - acc: 1.0000\n",
            "Epoch 48/100\n",
            "1/1 - 0s - loss: 0.2487 - acc: 1.0000\n",
            "Epoch 49/100\n",
            "1/1 - 0s - loss: 0.2445 - acc: 1.0000\n",
            "Epoch 50/100\n",
            "1/1 - 0s - loss: 0.2404 - acc: 1.0000\n",
            "Epoch 51/100\n",
            "1/1 - 0s - loss: 0.2364 - acc: 1.0000\n",
            "Epoch 52/100\n",
            "1/1 - 0s - loss: 0.2324 - acc: 1.0000\n",
            "Epoch 53/100\n",
            "1/1 - 0s - loss: 0.2286 - acc: 1.0000\n",
            "Epoch 54/100\n",
            "1/1 - 0s - loss: 0.2249 - acc: 1.0000\n",
            "Epoch 55/100\n",
            "1/1 - 0s - loss: 0.2212 - acc: 1.0000\n",
            "Epoch 56/100\n",
            "1/1 - 0s - loss: 0.2177 - acc: 1.0000\n",
            "Epoch 57/100\n",
            "1/1 - 0s - loss: 0.2142 - acc: 1.0000\n",
            "Epoch 58/100\n",
            "1/1 - 0s - loss: 0.2108 - acc: 1.0000\n",
            "Epoch 59/100\n",
            "1/1 - 0s - loss: 0.2075 - acc: 1.0000\n",
            "Epoch 60/100\n",
            "1/1 - 0s - loss: 0.2042 - acc: 1.0000\n",
            "Epoch 61/100\n",
            "1/1 - 0s - loss: 0.2011 - acc: 1.0000\n",
            "Epoch 62/100\n",
            "1/1 - 0s - loss: 0.1980 - acc: 1.0000\n",
            "Epoch 63/100\n",
            "1/1 - 0s - loss: 0.1950 - acc: 1.0000\n",
            "Epoch 64/100\n",
            "1/1 - 0s - loss: 0.1920 - acc: 1.0000\n",
            "Epoch 65/100\n",
            "1/1 - 0s - loss: 0.1891 - acc: 1.0000\n",
            "Epoch 66/100\n",
            "1/1 - 0s - loss: 0.1863 - acc: 1.0000\n",
            "Epoch 67/100\n",
            "1/1 - 0s - loss: 0.1835 - acc: 1.0000\n",
            "Epoch 68/100\n",
            "1/1 - 0s - loss: 0.1808 - acc: 1.0000\n",
            "Epoch 69/100\n",
            "1/1 - 0s - loss: 0.1782 - acc: 1.0000\n",
            "Epoch 70/100\n",
            "1/1 - 0s - loss: 0.1756 - acc: 1.0000\n",
            "Epoch 71/100\n",
            "1/1 - 0s - loss: 0.1731 - acc: 1.0000\n",
            "Epoch 72/100\n",
            "1/1 - 0s - loss: 0.1706 - acc: 1.0000\n",
            "Epoch 73/100\n",
            "1/1 - 0s - loss: 0.1682 - acc: 1.0000\n",
            "Epoch 74/100\n",
            "1/1 - 0s - loss: 0.1659 - acc: 1.0000\n",
            "Epoch 75/100\n",
            "1/1 - 0s - loss: 0.1635 - acc: 1.0000\n",
            "Epoch 76/100\n",
            "1/1 - 0s - loss: 0.1613 - acc: 1.0000\n",
            "Epoch 77/100\n",
            "1/1 - 0s - loss: 0.1591 - acc: 1.0000\n",
            "Epoch 78/100\n",
            "1/1 - 0s - loss: 0.1569 - acc: 1.0000\n",
            "Epoch 79/100\n",
            "1/1 - 0s - loss: 0.1548 - acc: 1.0000\n",
            "Epoch 80/100\n",
            "1/1 - 0s - loss: 0.1527 - acc: 1.0000\n",
            "Epoch 81/100\n",
            "1/1 - 0s - loss: 0.1506 - acc: 1.0000\n",
            "Epoch 82/100\n",
            "1/1 - 0s - loss: 0.1486 - acc: 1.0000\n",
            "Epoch 83/100\n",
            "1/1 - 0s - loss: 0.1467 - acc: 1.0000\n",
            "Epoch 84/100\n",
            "1/1 - 0s - loss: 0.1448 - acc: 1.0000\n",
            "Epoch 85/100\n",
            "1/1 - 0s - loss: 0.1429 - acc: 1.0000\n",
            "Epoch 86/100\n",
            "1/1 - 0s - loss: 0.1410 - acc: 1.0000\n",
            "Epoch 87/100\n",
            "1/1 - 0s - loss: 0.1392 - acc: 1.0000\n",
            "Epoch 88/100\n",
            "1/1 - 0s - loss: 0.1374 - acc: 1.0000\n",
            "Epoch 89/100\n",
            "1/1 - 0s - loss: 0.1357 - acc: 1.0000\n",
            "Epoch 90/100\n",
            "1/1 - 0s - loss: 0.1340 - acc: 1.0000\n",
            "Epoch 91/100\n",
            "1/1 - 0s - loss: 0.1323 - acc: 1.0000\n",
            "Epoch 92/100\n",
            "1/1 - 0s - loss: 0.1307 - acc: 1.0000\n",
            "Epoch 93/100\n",
            "1/1 - 0s - loss: 0.1291 - acc: 1.0000\n",
            "Epoch 94/100\n",
            "1/1 - 0s - loss: 0.1275 - acc: 1.0000\n",
            "Epoch 95/100\n",
            "1/1 - 0s - loss: 0.1260 - acc: 1.0000\n",
            "Epoch 96/100\n",
            "1/1 - 0s - loss: 0.1245 - acc: 1.0000\n",
            "Epoch 97/100\n",
            "1/1 - 0s - loss: 0.1230 - acc: 1.0000\n",
            "Epoch 98/100\n",
            "1/1 - 0s - loss: 0.1215 - acc: 1.0000\n",
            "Epoch 99/100\n",
            "1/1 - 0s - loss: 0.1201 - acc: 1.0000\n",
            "Epoch 100/100\n",
            "1/1 - 0s - loss: 0.1187 - acc: 1.0000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fd3be0c88d0>"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9SvAaB_mWUZ"
      },
      "source": [
        "## 09) 엘모 Embeddings from Language Model, ELMo\n",
        "* 2018년에 제안된 새로운 워드 임베딩 방법론\n",
        "* 언어 모델로 하는 임베딩\n",
        "* 가장 큰 특징: **사전 훈련된 언어 모델 Pre-trained language model을 사용**\n",
        "\n",
        "\n",
        "### 1. ELMo\n",
        "* Bank Accont와 River Bank는 전혀 다른 의미이지만, Word2Vec이나 GloVe 등으로 표현된 임베딩 벡터들은 이를 제대로 반영 못함\n",
        "* 같은 표기의 단어라도 문맥에 따라서 다르게 워드 임베딩하자\n",
        "* **단어를 임베딩하기 전에 전체 문장을 고려해서 임베딩을 하겠다**\n",
        "> 문맥을 반영한 워드 임베딩 Contextualized Word Embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQLUNparzvK4"
      },
      "source": [
        "### 2. biLM (Bidirectional Language Model)의 사전 훈련\n",
        "* ELMo는 위의 그림의 순방향 RNN 뿐만 아니라, 역방향 RNN 또한 활용\n",
        "* 양쪽 방향의 언어 모델을 둘 다 활용한다고 하여 **biLM**\n",
        "\n",
        "* ELMo에서 biLM은 기본적으로 다층 구조 Multi-layer를 전제로\n",
        "* **biLM의 입력이 되는 워드 임베딩 방법**으로는 이 책에서는 다루지 않은 **char CNN** 사용\n",
        "  * 이는 글자(character) 단위로 계산됨\n",
        "  * 마치 서브단어(subword)의 정보를 참고하는 것처럼 문맥과 상관 없이 dog와 doggy 간의 연관성을 찾아낼 수 있고, OOV에도 견고함\n",
        "\n",
        "\n",
        "* **RNN 챕터에서의 양방향 RNN과 ELMo의 biLM은 달라!**\n",
        "  * 양방향 RNN: 순방향 RNN의 은닉 상태와 역방향의 RNN의 은닉 상태를 **다음 층의 입력으로 보내기 전에** 연결 concatenate시킴\n",
        "  * biLM: biLM의 순방향 언어모델과 역방향 언어모델이 각각의 은닉 상태만을 **다음 은닉층으로 보내며 훈련시킨 후에 ELMo 표현으로 사용하기 위해서** 은닉 상태를 연결시키는 것과 다름\n",
        "\n",
        "\n",
        "### 3. biLM의 활용\n",
        "* ELMo가 사전 훈련된 biLM을 통해 입력 문자응로부터 단어를 임베딩하기 위한 과정\n",
        "* ex. play를 임베딩하기 위해서는 해당 시점의 BiLM의 각 층의 출력값을 가져옴. 그리고 순방향 언어 모델과 역방향 언어 모델의 각 층의 출력값을 연결 concatenate하고 추가 작업을 진행\n",
        "* 각 층의 출력값이란 첫번째는 임베딩 층, 나머지 층은 각 츠으이 은닉 상태를 말함\n",
        "* **각 층의 출력값이 가진 정보는 전부 서로 다른 종류의 정보를 갖고 있을 것이므로, 이들을 모두 활용한다는 점**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CcQ-55k-1-Nl"
      },
      "source": [
        "* ELMo가 임베딩 벡터를 얻는 과정\n",
        "#### 1) 각 층의 출력값을 concatenate\n",
        "#### 2) 각 층의 출력값 별로 가중치를 둠..s1, s2, s3\n",
        "#### 3) 각 층의 출력값을 모두 더함..가중합\n",
        "#### 4) 벡터의 크기를 결정하는 스칼라 매개변수 곱함\n",
        "  * 지금까지는 ELMo 표현(representation)을 얻음\n",
        "\n",
        "\n",
        "* 이제 ELMo를 입력으로 사용하고 수행하고 싶은 텍스트 분류, 질의 응답 시스템 등의 자연어 처리 작업이 있을 것\n",
        "* ex. 텍스트 분류 작업\n",
        "  * ELMo 표현의 기존의 임베딩 벡터와 함께 사용 가능.\n",
        "  1. 텍스트 분류 위해 GloVE와 같은 기존의 방법론을 사용한 임베딩 벡터 준비함.\n",
        "  2. GloVe를 사용한 임베딩 벡터만 텍분 작업에 사용하는게 아니라, **이렇게 준비된 ELMo 표현을 GloVe 임베딩 벡터와 concatenate해서 입력으로 사용할 수 있음**\n",
        "  3. ELMo 표현을 만드는데 사용되는 사전 훈련된 언어의 가중치는 고정, 위에서 사용한 s1, s2, s3와 스칼라 매개변수는 훈련 과정에서 학습됨"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjyovIYX3I85"
      },
      "source": [
        "### 4. ELMo 표현을 이용해서 스팸 메일 분류\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JlCX_ZsT5CcS",
        "outputId": "96957e15-1853-44bb-c2ca-c35092a6251c"
      },
      "source": [
        "%tensorflow_version 1.x "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow 1.x selected.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q1f6quyx3POP",
        "outputId": "9dc22e6d-6f0c-4d92-9a2d-1e2bc3763279"
      },
      "source": [
        "# 텐서플로우 허브로부터 다양한 pre-trained model들을 사용 가능\n",
        "!pip install tensorflow-hub"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow-hub in /usr/local/lib/python3.7/dist-packages (0.12.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-hub) (3.17.3)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-hub) (1.19.5)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorflow-hub) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8ZLX3AY3WRU",
        "outputId": "2a59b71e-9f03-4124-fb77-38dad2d076cd"
      },
      "source": [
        "import tensorflow_hub as hub\n",
        "import tensorflow as tf\n",
        "from keras import backend as K\n",
        "import urllib.request\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using TensorFlow backend.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33XauqXW3Y2Y"
      },
      "source": [
        "# 텐서플로우 허브로부터 ELMo를 다운로드\n",
        "elmo = hub.Module(\"https://tfhub.dev/google/elmo/1\", trainable=True)\n",
        "\n",
        "sess = tf.Session()\n",
        "K.set_session(sess)\n",
        "sess.run(tf.global_variables_initializer())\n",
        "sess.run(tf.tables_initializer())"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "id": "e3trd6qT3hWO",
        "outputId": "bd6d481e-ab00-49b9-9cc5-a5621d627ac2"
      },
      "source": [
        "# 스팸 메일 분류하기 데이터 다운로드\n",
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/mohitgupta-omg/Kaggle-SMS-Spam-Collection-Dataset-/master/spam.csv\", filename=\"spam.csv\")\n",
        "data = pd.read_csv('spam.csv', encoding='latin-1')\n",
        "data[:5]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>v1</th>\n",
              "      <th>v2</th>\n",
              "      <th>Unnamed: 2</th>\n",
              "      <th>Unnamed: 3</th>\n",
              "      <th>Unnamed: 4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     v1  ... Unnamed: 4\n",
              "0   ham  ...        NaN\n",
              "1   ham  ...        NaN\n",
              "2  spam  ...        NaN\n",
              "3   ham  ...        NaN\n",
              "4   ham  ...        NaN\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJeWjagE6i-J"
      },
      "source": [
        "# v1열을 숫자 레이블로 바꿈\n",
        "data['v1'] = data['v1'].replace(['ham','spam'],[0,1])\n",
        "y_data = list(data['v1'])\n",
        "X_data = list(data['v2'])"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQJPo9jM6s76",
        "outputId": "849c5dce-bc86-4861-f026-d8c8e75fc59b"
      },
      "source": [
        "X_data[:5]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...',\n",
              " 'Ok lar... Joking wif u oni...',\n",
              " \"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\",\n",
              " 'U dun say so early hor... U c already then say...',\n",
              " \"Nah I don't think he goes to usf, he lives around here though\"]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0X3HXEwBlbMB",
        "outputId": "c82983ec-f825-4a1d-f049-b2933d21210e"
      },
      "source": [
        "# 훈련 데이터와 테스트 데이터를 8:2로 분할\n",
        "print(len(X_data))\n",
        "n_of_train = int(len(X_data) * 0.8)\n",
        "n_of_test = int(len(X_data) - n_of_train)\n",
        "print(n_of_train)\n",
        "print(n_of_test)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5572\n",
            "4457\n",
            "1115\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6cQcTd_653R"
      },
      "source": [
        "# 데이터 분할\n",
        "X_train = np.asarray(X_data[:n_of_train]) #X_data 데이터 중에서 앞의 4457개의 데이터만 저장\n",
        "y_train = np.asarray(y_data[:n_of_train]) #y_data 데이터 중에서 앞의 4457개의 데이터만 저장\n",
        "X_test = np.asarray(X_data[n_of_train:]) #X_data 데이터 중에서 뒤의 1115개의 데이터만 저장\n",
        "y_test = np.asarray(y_data[n_of_train:]) #y_data 데이터 중에서 뒤의 1115개의 데이터만 저장"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8mBuV8QV6-Jl"
      },
      "source": [
        "# ELMo는 텐서플로우 허브에서 가져온 것. 케라스에서 사용하기 위해서는 케사르에서 사용할 수 있도록 변환해주는 작업 필요\n",
        "def ELMoEmbedding(x):\n",
        "    return elmo(tf.squeeze(tf.cast(x, tf.string)), as_dict=True, signature=\"default\")[\"default\"]\n",
        "# 데이터의 이동이 케라스 → 텐서플로우 → 케라스가 되도록 하는 함수"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mKqruPha7IS-",
        "outputId": "bf46cc3d-3815-45d3-f83c-cc8f80394585"
      },
      "source": [
        "# 모델 설계\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, Lambda, Input\n",
        "\n",
        "input_text = Input(shape=(1,), dtype=tf.string)\n",
        "embedding_layer = Lambda(ELMoEmbedding, output_shape=(1024, ))(input_text)\n",
        "hidden_layer = Dense(256, activation='relu')(embedding_layer)\n",
        "output_layer = Dense(1, activation='sigmoid')(hidden_layer)\n",
        "model = Model(inputs=[input_text], outputs=output_layer)\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# ELMo를 이용한 임베딩 층을 거쳐서 256개의 뉴런이 있는 은닉층을 거친 후 마지막 1개의 뉴런을 통해 이진 분류 수ㅐㅇ\n",
        "# 활성화 함수 sigmoid, 모델 loss func는 binary_crossentropy"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XBdJt_W_7ZOO",
        "outputId": "bb17da59-f482-4c3d-f759-3a872feaa401"
      },
      "source": [
        "history = model.fit(X_train, y_train, epochs=1, batch_size=60)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/keras/backend/tensorflow_backend.py:431: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/keras/backend/tensorflow_backend.py:431: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1\n",
            "4457/4457 [==============================] - 773s 173ms/step - loss: 0.1314 - accuracy: 0.9509\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bt2jI9607dwl",
        "outputId": "53ce008f-9d13-4226-a278-0e596f028883"
      },
      "source": [
        "print(\"\\n 테스트 정확도: %.4f\" % (model.evaluate(X_test, y_test)[1]))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1115/1115 [==============================] - 168s 151ms/step\n",
            "\n",
            " 테스트 정확도: 0.9713\n"
          ]
        }
      ]
    }
  ]
}