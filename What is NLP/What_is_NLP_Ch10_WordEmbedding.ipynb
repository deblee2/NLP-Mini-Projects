{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "What_is_NLP_Ch10_WordEmbedding.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-r5A2ttu8j5k"
      },
      "source": [
        "## 01) Word Embedding\n",
        "* 단어를 벡터로 표현하는 방법\n",
        "* 단어를 밀집 표현으로 변환\n",
        "\n",
        "### 1. 희소 표현 Sparse Representation\n",
        "* 정의: 벡터/행렬의 값이 대부분이 0으로 표현되는 방법\n",
        "  * 원-핫 벡터는 희소 벡터\n",
        "* 문제점: 단어 개수 늘어나면 벡터의 차원이 한없이 커짐.. 공간적 낭비 + 단어의 의미 담지 못함\n",
        "\n",
        "### 2. 밀집 표현 Dense Representation\n",
        "  * 벡터의 차원을 단어 집합의 크기로 상정하지 않음\n",
        "  * 사용자가 설정한 값으로 모든 단어의 벡터 표현의 차원을 맞춤\n",
        "  * 0과 1만이 아니라 실수값 가짐\n",
        "\n",
        "### 3. 워드 임베딩\n",
        "  * 단어를 dense vector의 형태로 표현하는 방법\n",
        "  * Aka Embedding vector (밀집 벡터를 워드 임베딩 과정을 통해 나온 결과)\n",
        "  * ex) LSA, Word2Vec, FastText, Glove"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEayKtDL9a6J"
      },
      "source": [
        "## 02) Word2Vec\n",
        "* 단어 간 유사도를 반영할 수 있도록 단어의 의미 벡터화 해야됨\n",
        "### 1. 희소 표현 Sparse Representation\n",
        "  * 희소 벡터(ex. 원-핫 벡터)는 각 단어 간 유사성 표현 못함\n",
        "    * 단어의 '의미'를 다차원 공간에 벡터화. 이러한 표현 방법을 **분산 표현(Distributed represntation)**\n",
        "\n",
        "### 2. 분산 표현 Distributed Representation\n",
        "  * Based on 분포 가설 Distributional hypothesis: 비슷한 위치에서 등장하는 단어들은 비슷한 의미를 가짐\n",
        "  * 요약: 희소 차원은 고차원에 각 차원이 분리된 표현 방법, 분산 표현은 저차원에 단어의 의미를 여러 차원에다가 분산하여 표현. ==> 단어 간 유사도 계싼 가능\n",
        "\n",
        "### 3. CBOW(Continuous Bag of Words)\n",
        "  * Word2Vec은 CBOW와 Skip-Gram 두 가지\n",
        "  * CBOW: 주변에 있는 단어(context word)들을 가지고 중간에 있는 단어(center word) 예측\n",
        "  * Skip-Gram: 중간에 있는 단어로 주변 단어들을 예측  \n",
        "\n",
        "  * Window: 중심 단어를 예측하기 위해서 앞, 뒤로 몇 개의 단어를 볼지 범위\n",
        "  * Sliding window: Window를 계속 움직여서 주변 단어와 중심 단어 선택을 바꿔가며 학습을 위한 데이터셋 만들기\n",
        "\n",
        "  * 과정 설명: \n",
        "  1. 입력층의 입력으로서 앞, 뒤로 사용자가 정한 윈도우 크기 범위 안 주변 단어들의 원-핫 벡터가 들어감\n",
        "  2. 출력층에서 예측하고자 하는 '중간 단어의 원-핫 벡터'가 필요\n",
        "    * 이는 Word2Vec의 학습을 위해서 필요\n",
        "  * 추가: Word2Vec은 Deep Learning Model이 아님.\n",
        "    * 딥러닝: 입력층과 출력층 사이의 은닉층의 개수가 충분히 쌓인 신경망 학습\n",
        "    * Word2vec: 입력층과 출력층 사이 하나의 은닉층만 존재. Shallow Neural Network\n",
        "      * (+) 은닉층에 활성화 함수 존재하지 않으며, 룩업 테이블이라는 연산을 담당하는 층.. 그래서 투사층 projection layer이라고도 부름  \n",
        "\n",
        "  * 동작 메커니즘:\n",
        "  1. 투사층의 크기가 M(입력층은 VxM)\n",
        "  2. 입력층과 투사층 사이의 가중치 W는 VxM, 투사층과 출력층 사이의 가중치 W'는 MxV\n",
        "  * 투사층에서 평균을 구하는 부분은 **CBOW가 Skip-Gram과 다른 차이점**\n",
        "    * Skip-Gram은 입력이 중심 단어 하나. 투사층에서 벡터의 평균을 구하지 않음\n",
        "\n",
        "### 4. Skip-Gram\n",
        "  * 전반적으로 성능 더 좋음  \n",
        "\n",
        "### 5. NNLM vs. Word2Vec\n",
        "#### 차이점\n",
        "1. \n",
        "  * 신경망 언어 모델(NNLM): 언어 모델로 **다음 대상 예측**\n",
        "  * Word2Vec: 워드 임베딩 자체가 목적. **다음 단어가 아닌 중심 단어를 예측하여 학습**\n",
        "    * => NNLM은 예측 단어의 이전 단어들만 참고, Word2Vec은 예측 단어의 전, 후 단어들을 모두 참고\n",
        "2. Word2Vec은 NNLM에 존재하던 활성화 함수가 있는 은닉층 제거\n",
        "  * 투사층 다음에 바로 출력층으로 연결\n",
        "* (+) 속도에 차이나는 이유\n",
        "  * 계층적 소프트맥스(Hierarchical softmax)와 네거티브 샘플링(Negative sampling)\n",
        "    * Word2Vec은 출력층에서의 연산에서 V를 log(V)로 바꿔 배는 빠른 학습속도 가짐"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2SwSNDQGG96"
      },
      "source": [
        "## 03) 영어/한국어 Word2Vec 실습\n",
        "* gensim 패키지 이용\n",
        "\n",
        "### 1. 영어 Word2Vec 만들기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4PyTn_t99UHs"
      },
      "source": [
        "import urllib.request\n",
        "import zipfile\n",
        "from lxml import etree\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CKAkGr8lGVDw",
        "outputId": "d4c0cb30-5099-4845-e1a7-bbedac91a7e3"
      },
      "source": [
        "# 데이터 다운로드\n",
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/GaoleMeng/RNN-and-FFNN-textClassification/master/ted_en-20160408.xml\", filename=\"ted_en-20160408.xml\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('ted_en-20160408.xml', <http.client.HTTPMessage at 0x7fa4eebbe550>)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IdmVsTcwKMH_",
        "outputId": "6c37479a-185b-417f-d335-3bab546fff36"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_HDkD36GYxW"
      },
      "source": [
        "* 훈련 데이터 파일은 xml 문법으로 작성되어 있어 자연어 얻기 위해서는 전처리 필요\n",
        "  * <content>와 </content> 사이의 내용만\n",
        "* 배경음 나타내는 (Laughter), (Applause)도 제거"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PlUhejaEGX6Y"
      },
      "source": [
        "# 전처리 코드\n",
        "targetXML = open('ted_en-20160408.xml', 'r', encoding='UTF8')\n",
        "target_text = etree.parse(targetXML)\n",
        "\n",
        "# xml 파일로부터 <content>와 </content> 사이의 내용만 가져옴\n",
        "parse_text = '\\n'.join(target_text.xpath('//content/text()'))\n",
        "\n",
        "# 정규표현식의 sub 모듈로 content 중간의 (Audio), (Laughter) 등의 배경음 부분 제거\n",
        "# 해당 코드는 괄호로 구성된 내용 제거\n",
        "content_text = re.sub(r'\\([^)]*\\)', '', parse_text)\n",
        "\n",
        "# 입력 코퍼스에 대해서 NLTK를 이용하여 문장 토큰화 수행\n",
        "sent_text = sent_tokenize(content_text)\n",
        "\n",
        "# 각 문장에 대해서 구두점 제거, 대문자를 소문자로 변환\n",
        "normalized_text = []\n",
        "for string in sent_text:\n",
        "  tokens = re.sub(r\"[^a-z0-9]+\", \" \", string.lower())\n",
        "  normalized_text.append(tokens)\n",
        "\n",
        "# 각 문장에 대해서 NLTK를 이용하여 단어 토큰화를 수행\n",
        "result = [word_tokenize(sentence) for sentence in normalized_text]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73ed2shtKG9c",
        "outputId": "e5a75052-8aed-4b6f-ffd5-0a7d5c43647e"
      },
      "source": [
        "print('총 샘플의 개수 : {}'.format(len(result)))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "총 샘플의 개수 : 273424\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "unUoUMzFKJWz",
        "outputId": "b998490d-a101-4640-f39f-160a49ec4f8c"
      },
      "source": [
        "# 샘플 3개만 출력\n",
        "for line in result[:3]:\n",
        "    print(line)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['here', 'are', 'two', 'reasons', 'companies', 'fail', 'they', 'only', 'do', 'more', 'of', 'the', 'same', 'or', 'they', 'only', 'do', 'what', 's', 'new']\n",
            "['to', 'me', 'the', 'real', 'real', 'solution', 'to', 'quality', 'growth', 'is', 'figuring', 'out', 'the', 'balance', 'between', 'two', 'activities', 'exploration', 'and', 'exploitation']\n",
            "['both', 'are', 'necessary', 'but', 'it', 'can', 'be', 'too', 'much', 'of', 'a', 'good', 'thing']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2Mhec_DKbjK"
      },
      "source": [
        "### 3) Word2Vec 훈련시키기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZpxsw_aKTu1"
      },
      "source": [
        "from gensim.models import Word2Vec\n",
        "from gensim.models import KeyedVectors"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7DbTUIhKonw"
      },
      "source": [
        "Word2Vec의 하이퍼 파라미터값\n",
        "* size: 워드 벡터의 특징 값. 임베딩된 벡터의 차원\n",
        "* window: context window 크기\n",
        "* min_count: 단어 최소 빈도 수 제한\n",
        "* workers: 학습을 위한 프로세스 수\n",
        "* sg: 0은 cbow, 1은 skip-gram"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UuGAyd7eKg78"
      },
      "source": [
        "model = Word2Vec(sentences=result, size=100, window=5, min_count=5, workers=4, sg=0)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bE7URzgOKn4w",
        "outputId": "830d61c3-835f-4387-f2a7-454288c46cda"
      },
      "source": [
        "# Word2Vec에 대해서 학습 진행\n",
        "# 입력 단어에 대해서 가장 유사한 단어 출력\n",
        "\n",
        "model_result = model.wv.most_similar(\"man\")\n",
        "print(model_result)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('woman', 0.8435317277908325), ('guy', 0.7985468506813049), ('lady', 0.7817904949188232), ('boy', 0.7606765031814575), ('gentleman', 0.7366722822189331), ('girl', 0.7296925187110901), ('soldier', 0.7029783725738525), ('kid', 0.7020815014839172), ('poet', 0.6805630326271057), ('king', 0.66712486743927)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udMayI8QK8pP"
      },
      "source": [
        "# Word2Vec 모델 저장하고 로드\n",
        "model.wv.save_word2vec_format('eng_w2v') # 모델 저장\n",
        "loaded_model = KeyedVectors.load_word2vec_format('eng_w2v') #모델 로드"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TVJ44x8CLJWo",
        "outputId": "9b645270-54fd-428d-d42c-ce1386797f42"
      },
      "source": [
        "# 로드한 모델에 대해서 다시 man과 유사한 단어 출력\n",
        "model_result = loaded_model.most_similar('man')\n",
        "print(model_result)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('woman', 0.8435317277908325), ('guy', 0.7985468506813049), ('lady', 0.7817904949188232), ('boy', 0.7606765031814575), ('gentleman', 0.7366722822189331), ('girl', 0.7296925187110901), ('soldier', 0.7029783725738525), ('kid', 0.7020815014839172), ('poet', 0.6805630326271057), ('king', 0.66712486743927)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cv4GS4gWQAyK"
      },
      "source": [
        "### 2. 한국어 Word2Vec 만들기 (네이버 영화 리뷰)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uu6d8naLQb77",
        "outputId": "f945d64f-7806-4787-ea3d-c171caace028"
      },
      "source": [
        "!pip install konlpy"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting konlpy\n",
            "  Downloading konlpy-0.5.2-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4 MB 5.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.19.5)\n",
            "Collecting beautifulsoup4==4.6.0\n",
            "  Downloading beautifulsoup4-4.6.0-py3-none-any.whl (86 kB)\n",
            "\u001b[K     |████████████████████████████████| 86 kB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tweepy>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (3.10.0)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n",
            "Collecting JPype1>=0.7.0\n",
            "  Downloading JPype1-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (448 kB)\n",
            "\u001b[K     |████████████████████████████████| 448 kB 65.5 MB/s \n",
            "\u001b[?25hCollecting colorama\n",
            "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "Installing collected packages: JPype1, colorama, beautifulsoup4, konlpy\n",
            "  Attempting uninstall: beautifulsoup4\n",
            "    Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "Successfully installed JPype1-1.3.0 beautifulsoup4-4.6.0 colorama-0.4.4 konlpy-0.5.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SzGnno4HLOWF"
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import urllib.request\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "from konlpy.tag import Okt"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UtFrEV1iQK6j",
        "outputId": "2bcd2a5d-857f-455a-fbc4-2f6e6e9842eb"
      },
      "source": [
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings.txt\", filename=\"ratings.txt\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('ratings.txt', <http.client.HTTPMessage at 0x7fa4cb26d110>)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLmekUtiQMS2"
      },
      "source": [
        "train_data = pd.read_table('ratings.txt')"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "id": "YsdhDeH2QN1J",
        "outputId": "6edf8bb5-12d9-4f5d-9c4d-3f0754ff5e82"
      },
      "source": [
        "train_data[:5]"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>document</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>8112052</td>\n",
              "      <td>어릴때보고 지금다시봐도 재밌어요ㅋㅋ</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>8132799</td>\n",
              "      <td>디자인을 배우는 학생으로, 외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4655635</td>\n",
              "      <td>폴리스스토리 시리즈는 1부터 뉴까지 버릴께 하나도 없음.. 최고.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9251303</td>\n",
              "      <td>와.. 연기가 진짜 개쩔구나.. 지루할거라고 생각했는데 몰입해서 봤다.. 그래 이런...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>10067386</td>\n",
              "      <td>안개 자욱한 밤하늘에 떠 있는 초승달 같은 영화.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         id                                           document  label\n",
              "0   8112052                                어릴때보고 지금다시봐도 재밌어요ㅋㅋ      1\n",
              "1   8132799  디자인을 배우는 학생으로, 외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산...      1\n",
              "2   4655635               폴리스스토리 시리즈는 1부터 뉴까지 버릴께 하나도 없음.. 최고.      1\n",
              "3   9251303  와.. 연기가 진짜 개쩔구나.. 지루할거라고 생각했는데 몰입해서 봤다.. 그래 이런...      1\n",
              "4  10067386                        안개 자욱한 밤하늘에 떠 있는 초승달 같은 영화.      1"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j6_mWHXrQPaz",
        "outputId": "3b7a70ad-4572-4ce0-ca63-a019cdcd7876"
      },
      "source": [
        "print(len(train_data))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "200000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_feQbe5MQQ3x"
      },
      "source": [
        "# null 값 존재 유무 확인\n",
        "print(train_data.isnull().values.any())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q5EgrVLwQV7u",
        "outputId": "37894153-8ee3-4ae7-8e29-1647766cba3d"
      },
      "source": [
        "train_data = train_data.dropna(how = 'any') #null 값이 존재하는 행 제거\n",
        "print(train_data.isnull().values.any())"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4P6k3bm4Qj_x",
        "outputId": "26f24fd7-c3c2-4404-bdae-020ecac2c676"
      },
      "source": [
        "print(len(train_data))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "199992\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qI4drT7QnjB"
      },
      "source": [
        "train_data['document'] = train_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "id": "D0G9bEaYQ0K2",
        "outputId": "57d1c242-bbeb-49d5-bfda-fede9dfb28ea"
      },
      "source": [
        "train_data[:5]"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>document</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>8112052</td>\n",
              "      <td>어릴때보고 지금다시봐도 재밌어요ㅋㅋ</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>8132799</td>\n",
              "      <td>디자인을 배우는 학생으로 외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산업...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4655635</td>\n",
              "      <td>폴리스스토리 시리즈는 부터 뉴까지 버릴께 하나도 없음 최고</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9251303</td>\n",
              "      <td>와 연기가 진짜 개쩔구나 지루할거라고 생각했는데 몰입해서 봤다 그래 이런게 진짜 영화지</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>10067386</td>\n",
              "      <td>안개 자욱한 밤하늘에 떠 있는 초승달 같은 영화</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         id                                           document  label\n",
              "0   8112052                                어릴때보고 지금다시봐도 재밌어요ㅋㅋ      1\n",
              "1   8132799  디자인을 배우는 학생으로 외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산업...      1\n",
              "2   4655635                   폴리스스토리 시리즈는 부터 뉴까지 버릴께 하나도 없음 최고      1\n",
              "3   9251303   와 연기가 진짜 개쩔구나 지루할거라고 생각했는데 몰입해서 봤다 그래 이런게 진짜 영화지      1\n",
              "4  10067386                         안개 자욱한 밤하늘에 떠 있는 초승달 같은 영화      1"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wan0HLs0Q2fu"
      },
      "source": [
        "# 불용어 정의\n",
        "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LhWfiN6FRTY4"
      },
      "source": [
        "# import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aS_pcCZvRFZ2"
      },
      "source": [
        "from tqdm import tqdm"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jM4qJpXXQ6Pi",
        "outputId": "999e3286-e25a-468b-d86b-97aff507d23d"
      },
      "source": [
        "# 형태소 분석기 okt 사용한 토큰화 작업\n",
        "okt = Okt()\n",
        "\n",
        "tokenized_data = []\n",
        "for sentence in tqdm(train_data['document']):\n",
        "    tokenized_sentence = okt.morphs(sentence, stem=True) # 형태소분석.. 토큰화\n",
        "    stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords] # 불용어 제거\n",
        "    tokenized_data.append(stopwords_removed_sentence)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 199992/199992 [20:18<00:00, 164.09it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "MqEelcchRBt2",
        "outputId": "368b399d-c031-4850-8b21-70c83dd4c6a2"
      },
      "source": [
        "# 리뷰 길이 분포 확인\n",
        "print('리뷰의 최대 길이 :',max(len(l) for l in tokenized_data))\n",
        "print('리뷰의 평균 길이 :',sum(map(len, tokenized_data))/len(tokenized_data))\n",
        "plt.hist([len(s) for s in tokenized_data], bins=50)\n",
        "plt.xlabel('length of samples')\n",
        "plt.ylabel('number of samples')\n",
        "plt.show()"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "리뷰의 최대 길이 : 72\n",
            "리뷰의 평균 길이 : 10.716703668146726\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fr7WtMMPRbrq"
      },
      "source": [
        "from gensim.models import Word2Vec\n",
        "model = Word2Vec(sentences = tokenized_data, size = 100, window = 5, min_count = 5, workers = 4, sg = 0)"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qanJsGQeRn9j",
        "outputId": "2ed0f26b-72a1-42da-8c92-44268c1164b2"
      },
      "source": [
        "# 완성된 임베딩 매트릭스의 크기 확인\n",
        "model.wv.vectors.shape"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(16477, 100)"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vhO9H2zcRq5Q",
        "outputId": "053ffa0d-23af-473a-9f00-6cbe745812b2"
      },
      "source": [
        "print(model.wv.most_similar(\"최민식\"))"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('한석규', 0.8778828382492065), ('송강호', 0.8502885103225708), ('박중훈', 0.850069522857666), ('이정재', 0.8448969721794128), ('최민수', 0.844124972820282), ('엄정화', 0.8440042734146118), ('강지환', 0.842502236366272), ('김명민', 0.837706446647644), ('안성기', 0.8365885019302368), ('주진모', 0.833758533000946)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E0OubXb3Ruy6",
        "outputId": "2fe3833c-d82d-48d7-edb8-7785d2463570"
      },
      "source": [
        "print(model.wv.most_similar(\"히어로\"))"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('무협', 0.8730586767196655), ('호러', 0.8645143508911133), ('슬래셔', 0.8605858087539673), ('느와르', 0.8421926498413086), ('물', 0.8336173295974731), ('무비', 0.8279067277908325), ('멜로', 0.811174213886261), ('물의', 0.8094125390052795), ('헐리우드', 0.7883111238479614), ('교과서', 0.7843461036682129)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xc0kCzzNRzO6"
      },
      "source": [
        "### 3. 사전 훈련된 Word2Vec 임베딩(Pretrained Word2Vec embedding) 소개\n",
        "* 구글이 제공하는 사전 훈련된 Word2Vec 모델 사용\n",
        "* 사전훈련된 3백만 개의 단어 벡터 제공\n",
        "* 각 임베딩 벡터의 차원은 300"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SWYeBzerSP8U"
      },
      "source": [
        "import gensim"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wAoUo0qjYzvi",
        "outputId": "ad3f8147-5739-4214-c06a-f7804a278e92"
      },
      "source": [
        "!git clone https://github.com/mmihaltz/word2vec-GoogleNews-vectors.git"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'word2vec-GoogleNews-vectors'...\n",
            "remote: Enumerating objects: 20, done.\u001b[K\n",
            "remote: Total 20 (delta 0), reused 0 (delta 0), pack-reused 20\u001b[K\n",
            "Unpacking objects: 100% (20/20), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n9YF7uNyY3mP",
        "outputId": "8d2d1f0c-032a-409f-dc2f-7c9c6dc62a01"
      },
      "source": [
        "import os\n",
        "\n",
        "os.listdir('word2vec-GoogleNews-vectors')"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['README.md',\n",
              " 'GoogleNews-vectors-negative300.bin.gz',\n",
              " '.gitattributes',\n",
              " '.git']"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tiVV0YkHZOGO",
        "outputId": "ff9bee7e-bf07-4e2c-af6f-7bef8fe458e1"
      },
      "source": [
        "#a = os.system('GoogleNews-vectors-negative300.bin.gz')\n",
        "#path = get_file(MODEL + '.gz', 'https://deeplearning4jblob.blob.core.windows.net/resources/wordvectors/%s.gz' % MODEL)\n",
        "!wget -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\"\n",
        " \n",
        "model = gensim.models.KeyedVectors.load_word2vec_format(\"GoogleNews-vectors-negative300.bin.gz\",binary = True)\n"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-10-22 14:31:18--  https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.133.48\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.133.48|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1647046227 (1.5G) [application/x-gzip]\n",
            "Saving to: ‘GoogleNews-vectors-negative300.bin.gz’\n",
            "\n",
            "GoogleNews-vectors- 100%[===================>]   1.53G  15.4MB/s    in 1m 46s  \n",
            "\n",
            "2021-10-22 14:33:05 (14.8 MB/s) - ‘GoogleNews-vectors-negative300.bin.gz’ saved [1647046227/1647046227]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uyKw0OzrZSBF",
        "outputId": "6ab881d5-8d42-4ada-ef97-3cecf0a95d4f"
      },
      "source": [
        "# import zipfile\n",
        "\n",
        "# zip_file=zipfile.ZipFile(\"test.zip\")\n",
        "# zip_file.extractall()"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "32512"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_IaYcHbS8Kw"
      },
      "source": [
        "# model = gensim.models.KeyedVectors.load_word2vec_format('', binary=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LjOZlgn2TA_I",
        "outputId": "562f56ea-3ae4-4281-8579-4e7f15cf296e"
      },
      "source": [
        "print(model.vectors.shape)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3000000, 300)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I5iJCkHpTCvB",
        "outputId": "2c2630b0-3ad7-4cea-b990-45ee7c5d08ef"
      },
      "source": [
        "print (model.similarity('this', 'is'))\n",
        "print (model.similarity('post', 'book'))"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.40797037\n",
            "0.057204384\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sUrf6BeUTFTJ",
        "outputId": "824dc9e2-da72-4d42-bcee-310b150e50fc"
      },
      "source": [
        "print(model['book'])"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 0.11279297 -0.02612305 -0.04492188  0.06982422  0.140625    0.03039551\n",
            " -0.04370117  0.24511719  0.08740234 -0.05053711  0.23144531 -0.07470703\n",
            "  0.21875     0.03466797 -0.14550781  0.05761719  0.00671387 -0.00701904\n",
            "  0.13183594 -0.25390625  0.14355469 -0.140625   -0.03564453 -0.21289062\n",
            " -0.24804688  0.04980469 -0.09082031  0.14453125  0.05712891 -0.10400391\n",
            " -0.19628906 -0.20507812 -0.27539062  0.03063965  0.20117188  0.17382812\n",
            "  0.09130859 -0.10107422  0.22851562 -0.04077148  0.02709961 -0.00106049\n",
            "  0.02709961  0.34179688 -0.13183594 -0.078125    0.02197266 -0.18847656\n",
            " -0.17480469 -0.05566406 -0.20898438  0.04858398 -0.07617188 -0.15625\n",
            " -0.05419922  0.01672363 -0.02722168 -0.11132812 -0.03588867 -0.18359375\n",
            "  0.28710938  0.01757812  0.02185059 -0.05664062 -0.01251221  0.01708984\n",
            " -0.21777344 -0.06787109  0.04711914 -0.00668335  0.08544922 -0.02209473\n",
            "  0.31835938  0.01794434 -0.02246094 -0.03051758 -0.09570312  0.24414062\n",
            "  0.20507812  0.05419922  0.29101562  0.03637695  0.04956055 -0.06689453\n",
            "  0.09277344 -0.10595703 -0.04370117  0.19726562 -0.03015137  0.05615234\n",
            "  0.08544922 -0.09863281 -0.02392578 -0.08691406 -0.22460938 -0.16894531\n",
            "  0.09521484 -0.0612793  -0.03015137 -0.265625   -0.13378906  0.00139618\n",
            "  0.01794434  0.10107422  0.13964844  0.06445312 -0.09765625 -0.11376953\n",
            " -0.24511719 -0.15722656  0.00457764  0.12988281 -0.03540039 -0.08105469\n",
            "  0.18652344  0.03125    -0.09326172 -0.04760742  0.23730469  0.11083984\n",
            "  0.08691406  0.01916504  0.21386719 -0.0065918  -0.08984375 -0.02502441\n",
            " -0.09863281 -0.05639648 -0.26757812  0.19335938 -0.08886719 -0.25976562\n",
            "  0.05957031 -0.10742188  0.09863281  0.1484375   0.04101562  0.00340271\n",
            " -0.06591797 -0.02941895  0.20019531 -0.00521851  0.02355957 -0.13671875\n",
            " -0.12597656 -0.10791016  0.0067749   0.15917969  0.0145874  -0.15136719\n",
            "  0.07519531 -0.02905273  0.01843262  0.20800781  0.25195312 -0.11523438\n",
            " -0.23535156  0.04101562 -0.11035156  0.02905273  0.22460938 -0.04272461\n",
            "  0.09667969  0.11865234  0.08007812  0.07958984  0.3125     -0.14941406\n",
            " -0.234375    0.06079102  0.06982422 -0.14355469 -0.05834961 -0.36914062\n",
            " -0.10595703  0.00738525  0.24023438 -0.10400391 -0.02124023  0.05712891\n",
            " -0.11621094 -0.16894531 -0.06396484 -0.12060547  0.08105469 -0.13769531\n",
            " -0.08447266  0.12792969 -0.15429688  0.17871094  0.2421875  -0.06884766\n",
            "  0.03320312  0.04394531 -0.04589844  0.03686523 -0.07421875 -0.01635742\n",
            " -0.24121094 -0.08203125 -0.01733398  0.0291748   0.10742188  0.11279297\n",
            "  0.12890625  0.01416016 -0.28710938  0.16503906 -0.25585938  0.2109375\n",
            " -0.19238281  0.22363281  0.04541016  0.00872803  0.11376953  0.375\n",
            "  0.09765625  0.06201172  0.12109375 -0.24316406  0.203125    0.12158203\n",
            "  0.08642578  0.01782227  0.17382812  0.01855469  0.03613281 -0.02124023\n",
            " -0.02905273 -0.04541016  0.1796875   0.06494141 -0.13378906 -0.09228516\n",
            "  0.02172852  0.02099609  0.07226562  0.3046875  -0.27539062 -0.30078125\n",
            "  0.08691406 -0.22949219  0.0546875  -0.34179688 -0.00680542 -0.0291748\n",
            " -0.03222656  0.16210938  0.01141357  0.23339844 -0.0859375  -0.06494141\n",
            "  0.15039062  0.17675781  0.08251953 -0.26757812 -0.11669922  0.01330566\n",
            "  0.01818848  0.10009766 -0.09570312  0.109375   -0.16992188 -0.23046875\n",
            " -0.22070312  0.0625      0.03662109 -0.125       0.05151367 -0.18847656\n",
            "  0.22949219  0.26367188 -0.09814453  0.06176758  0.11669922  0.23046875\n",
            "  0.32617188  0.02038574 -0.03735352 -0.12255859  0.296875   -0.25\n",
            " -0.08544922 -0.03149414  0.38085938  0.02929688 -0.265625    0.42382812\n",
            " -0.1484375   0.14355469 -0.03125     0.00717163 -0.16601562 -0.15820312\n",
            "  0.03637695 -0.16796875 -0.01483154  0.09667969 -0.05761719 -0.00515747]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDsgCWhBSNT5"
      },
      "source": [
        "최근 Word2Vec은 추천시스템에도 사용됨"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nESy-jBiTspH"
      },
      "source": [
        "## 04) 네거티브 샘플링을 이용한 Word2Vec 구현 Skip-Gram with Negative Sampling\n",
        "### 1. 네거티브 샘플링\n",
        "* Word2Vec의 출력층: 소프트맥스 함수를 지난 단어 집합 크기의 벡터와 실제값인 원-핫 벡터와의 오차를 구하고 이로부터 임베딩 테이블에 있는 모든 단어에 대한 임베딩 벡터 값을 업데이트\n",
        "* 네거티브 샘플링: Word2Vec이 학습 과정에서 전체 단어 집합이 아니라 일부 단어 집합에만 집중할 수 있도록 함\n",
        "  * 하나의 중심 단어에 대해서 작은 단어 집합 만든 후 마지막 단계를 이진 분류 문제로 변환\n",
        "  * 기존의 단어 집합의 크기만큼 선택지를 두고 다중 클래스 분류 문제를 풀던 Word2Vec보다 훨씬 연산량에서 효율적\n",
        "\n",
        "\n",
        "### 2. 네거티브 샘플링 Skip-Gram (SGNS)\n",
        "* Skip-Gram: 입력은 중심 단어, 모델의 예측은 주변 단어\n",
        "* SGNS: 중심 단어와 주변 단어 **모두**가 입력, 이 **두 단어가 실제로 윈도우 크기 내에 존재하는 이웃 관계**인지 그 확률 예측  \n",
        "  > 주변 단어 관계까 아닌 단어들을 입력2로 삼기 위하여 레이블을 0으로  \n",
        "\n",
        "* 두 테이블 중 하나는 입력 1인 중심단어의 테이블 룩업을 위한 임베딩 테이블, 하나는 입력 2인 주변 단어의 테이블 룩업을 위한 임베딩 테이블\n",
        "* 각 임베딩 테이블을 통해 테이블 룩업하여 임베딩 벡터로 변환\n",
        "* 중심 단어와 주변 단어의 내적값을 이 모델의 예측값, 레이블과의 오차로부터 역전파하여 중심 단어와 주변 단어의 임베딩 벡터값을 업데이트"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sY2hwXJSV9ka"
      },
      "source": [
        "### 3. 20 뉴스그룹 데이터 전처리하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36Y1EAzsRyjy"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1SRXSN6WWCGd",
        "outputId": "f81495f2-b703-4abc-8716-65411cb93838"
      },
      "source": [
        "# 하나의 샘플에 최소 단어 2개는 있어야 함\n",
        "dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))\n",
        "documents = dataset.data\n",
        "print('총 샘플 수 : ', len(documents))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading 20news dataset. This may take a few minutes.\n",
            "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "총 샘플 수 :  11314\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ss9YE1iTWPfp"
      },
      "source": [
        "news_df = pd.DataFrame({'document':documents})\n",
        "# 특수 문자 제거\n",
        "news_df['clean_doc'] = news_df['document'].str.replace(\"[^a-zA-Z]\", \" \")\n",
        "# 길이가 3 이하인 단어는 제거\n",
        "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: ' '.join([w for w in x.split() if len(w) > 3]))\n",
        "# 전체 단어에 대한 소문자 변환\n",
        "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: x.lower())"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GX8JgPMyWsly",
        "outputId": "68625283-44d7-428e-b9b3-87ae563f7fff"
      },
      "source": [
        "# null값 있는지 확인\n",
        "news_df.isnull().values.any()"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lAZVbIQZW97T",
        "outputId": "4332c3e7-dd2d-4db3-df76-752b4575c606"
      },
      "source": [
        "# 빈 값 유무 확인\n",
        "# 모든 빈 값을 Null 값으로 변환, 다시 확인\n",
        "news_df.replace(\"\", float(\"NaN\"), inplace=True)\n",
        "news_df.isnull().values.any()"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fw1vZ7t4XMAE",
        "outputId": "e084ffa1-964e-4904-d069-b10527544d4c"
      },
      "source": [
        "# Null값 제거\n",
        "news_df.dropna(inplace=True)\n",
        "print('총 샘플 수 :',len(news_df))"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "총 샘플 수 : 10995\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ff0Ht8anXmFd",
        "outputId": "b3c02604-5441-434e-a4f2-e0e7dd647105"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZwq2_YrXRLd"
      },
      "source": [
        "# 불용어를 제거\n",
        "stop_words = stopwords.words('english')\n",
        "tokenized_doc = news_df['clean_doc'].apply(lambda x: x.split())\n",
        "tokenized_doc = tokenized_doc.apply(lambda x: [item for item in x if item not in stop_words])\n",
        "tokenized_doc = tokenized_doc.to_list()"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0iuiCURHXUx1",
        "outputId": "90fa1faa-6c7a-44b1-d615-51b03918e7a2"
      },
      "source": [
        "# 불용어 제거하여 단어의 수 줄어듦. 모든 샘플 중 단어가 1개 이하인 경우 모두 찾아 제거\n",
        "drop_train = [index for index, sentence in enumerate(tokenized_doc) if len(sentence) <= 1]\n",
        "tokenized_doc = np.delete(tokenized_doc, drop_train, axis=0)\n",
        "print('총 샘플 수 :',len(tokenized_doc))"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "총 샘플 수 : 10940\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return array(a, dtype, copy=False, order=order)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfhaoI2OaqI2"
      },
      "source": [
        "# 단어 집합 생성, 정수 인코딩 진행\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(tokenized_doc)\n",
        "\n",
        "word2idx = tokenizer.word_index\n",
        "idx2word = {value : key for key, value in word2idx.items()}\n",
        "encoded = tokenizer.texts_to_sequences(tokenized_doc)"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EfPjWMSNavWN",
        "outputId": "0214948a-1d57-4b7e-8611-98036e8ed0b6"
      },
      "source": [
        "print(encoded[:2])"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[9, 59, 603, 207, 3278, 1495, 474, 702, 9470, 13686, 5533, 15227, 702, 442, 702, 70, 1148, 1095, 1036, 20294, 984, 705, 4294, 702, 217, 207, 1979, 15228, 13686, 4865, 4520, 87, 1530, 6, 52, 149, 581, 661, 4406, 4988, 4866, 1920, 755, 10668, 1102, 7837, 442, 957, 10669, 634, 51, 228, 2669, 4989, 178, 66, 222, 4521, 6066, 68, 4295], [1026, 532, 2, 60, 98, 582, 107, 800, 23, 79, 4522, 333, 7838, 864, 421, 3825, 458, 6488, 458, 2700, 4730, 333, 23, 9, 4731, 7262, 186, 310, 146, 170, 642, 1260, 107, 33568, 13, 985, 33569, 33570, 9471, 11491]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IhgTxwaRaxcY",
        "outputId": "6bd0acb6-e8b2-46ef-c6c2-7d6f1eb2a8ad"
      },
      "source": [
        "# 단어 집합 크기 확인\n",
        "vocab_size = len(word2idx) + 1 \n",
        "print('단어 집합의 크기 :', vocab_size)"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "단어 집합의 크기 : 64277\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPB7vdj-a047"
      },
      "source": [
        "### 4. 네거티브 샘플링 통한 데이터셋 구성\n",
        "* 토큰화, 정제, 정규화, 불용어 제거, 정수 인코딩까지 일반적인 전처리 과정 거침\n",
        "* 네거티브 샘플링 위해서 keras의 skipgrams 사용\n",
        "* 상위 10개의 뉴스그룹 샘플에 대해서만 수행"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-eRey7XqbDjV"
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import skipgrams\n",
        "# 네거티브 샘플링\n",
        "skip_grams = [skipgrams(sample, vocabulary_size=vocab_size, window_size=10) for sample in encoded[:10]]"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "URUgTd7HbNZI",
        "outputId": "6b2d747b-30f2-4529-bc7b-a34e77c13dfb"
      },
      "source": [
        "# 첫번째 샘플인 skip_grams[0] 내 skipgrams로 형성된 데이터셋 확인\n",
        "pairs, labels = skip_grams[0][0], skip_grams[0][1]\n",
        "for i in range(5):\n",
        "      print(\"({:s} ({:d}), {:s} ({:d})) -> {:d}\".format(\n",
        "          idx2word[pairs[i][0]], pairs[i][0], \n",
        "          idx2word[pairs[i][1]], pairs[i][1], \n",
        "          labels[i]))"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(guilt (4989), chow (20699)) -> 0\n",
            "(existance (4865), achtung (26602)) -> 0\n",
            "(israeli (442), thermocouple (63915)) -> 0\n",
            "(subsidizing (15228), degree (1530)) -> 1\n",
            "(think (6), sodom (13039)) -> 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lWZ6SxwFbgo7",
        "outputId": "472ae5ab-beb0-4202-8041-8161cc8dd3e0"
      },
      "source": [
        "print('전체 샘플 수 :',len(skip_grams))"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "전체 샘플 수 : 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-LVFWhDcbjmc",
        "outputId": "da609f16-821b-46ce-9d67-9d6e4a466219"
      },
      "source": [
        "# 첫번째 뉴스그룹 샘플에 대해서 생긴 pairs와 labels의 개수\n",
        "print(len(pairs))\n",
        "print(len(labels))"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2220\n",
            "2220\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sL_hyoPgbocZ"
      },
      "source": [
        "# 모든 뉴스그룹 샘플에 대해서 수행\n",
        "skip_grams = [skipgrams(sample, vocabulary_size=vocab_size, window_size=10) for sample in encoded]"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIqIO5sDbsTb"
      },
      "source": [
        "### 5. Skip-Gram with Negative Sampling SGNS 구현"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCZoj3eBbv3n"
      },
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Embedding, Reshape, Activation, Input\n",
        "from tensorflow.keras.layers import Dot\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from IPython.display import SVG"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xqd3Xk9XbyNq"
      },
      "source": [
        "# 임베딩 벡터 차원\n",
        "embed_size = 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHeBShjAb06r"
      },
      "source": [
        "# 모델 설계\n",
        "# 두 개의 임베딩 테이블 생성\n",
        "\n",
        "# 중심 단어를 위한 임베딩 테이블\n",
        "w_inputs = Input(shape=(1, ), dtype='int32')\n",
        "word_embedding = Embedding(vocab_size, embed_size)(w_inputs)\n",
        "\n",
        "# 주변 단어를 위한 임베딩 테이블\n",
        "c_inputs = Input(shape=(1, ), dtype='int32')\n",
        "context_embedding  = Embedding(vocab_size, embed_size)(c_inputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmP7QGEAcBt_"
      },
      "source": [
        "# 각 단어는 임베딩 테이블을 거쳐 내적 수행, 내적의 결과는 1 또는 0으로 예측하기 위해서 시그모이드 함수를 활성화 함수로 거쳐 최종 예측값 얻음\n",
        "dot_product = Dot(axes=2)([word_embedding, context_embedding])\n",
        "dot_product = Reshape((1,), input_shape=(1, 1))(dot_product)\n",
        "output = Activation('sigmoid')(dot_product)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LrmPusgcHAq"
      },
      "source": [
        "model = Model(inputs=[w_inputs, c_inputs], outputs=output)\n",
        "model.summary()\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam')\n",
        "plot_model(model, to_file='model3.png', show_shapes=True, show_layer_names=True, rankdir='TB')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lmdr2wmUcJAZ"
      },
      "source": [
        "# 모델의 학습\n",
        "for epoch in range(1, 6):\n",
        "    loss = 0\n",
        "    for _, elem in enumerate(skip_grams):\n",
        "        first_elem = np.array(list(zip(*elem[0]))[0], dtype='int32')\n",
        "        second_elem = np.array(list(zip(*elem[0]))[1], dtype='int32')\n",
        "        labels = np.array(elem[1], dtype='int32')\n",
        "        X = [first_elem, second_elem]\n",
        "        Y = labels\n",
        "        loss += model.train_on_batch(X,Y)  \n",
        "    print('Epoch :',epoch, 'Loss :',loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzpoVM1UcMlk"
      },
      "source": [
        "### 6. 결과 확인"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVv0wV8PcLNm"
      },
      "source": [
        "import gensim\n",
        "\n",
        "# 학습된 임베딩 벡터들은 vector.txt에 저장\n",
        "f = open('vectors.txt' ,'w')\n",
        "f.write('{} {}\\n'.format(vocab_size-1, embed_size))\n",
        "vectors = model.get_weights()[0]\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    f.write('{} {}\\n'.format(word, ' '.join(map(str, list(vectors[i, :])))))\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-QnEaBh_cSpp"
      },
      "source": [
        "# 쉽게 유사도 구할 수 있음\n",
        "w2v = gensim.models.KeyedVectors.load_word2vec_format('./vectors.txt', binary=False)\n",
        "w2v.most_similar(positive=['soldiers'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Zrn7T-YcYS8"
      },
      "source": [
        "w2v.most_similar(positive=['doctor'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LkpVUbtdcZ5e"
      },
      "source": [
        "w2v.most_similar(positive=['police'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wF6kT9kIcaCz"
      },
      "source": [
        "w2v.most_similar(positive=['knife'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NG9pO9WvcaLq"
      },
      "source": [
        "w2v.most_similar(positive=['engine'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hneh9Qdck-W"
      },
      "source": [
        "## 05) 글로브 GloVe\n",
        "* 카운트 기반과 예측 기반을 모두 사용하는 방법론\n",
        "* 2014년에 미국 스탠포드 대학에서 개발한 단어 임베딩 방법론\n",
        "* 앞서 카운트 기반의 LSA(Latent Semantic Analysis)와 예측 기반 Word2Vec의 단점 지적하며 이를 보완\n",
        "\n",
        "### 1. 기존 방법론에 대한 비판\n",
        "* LSA: DTM이나 TD-IDF 행렬과 같이 **각 문서에서의 각 단어의 빈도수를 카운트 한 행렬이라는 전체적인 통계 정보를 입력으로 받아 차원을 축소(Truncated SVD)하여 잠재된 의미 끌어내느 방법론**\n",
        "  * 카운트 기반으로 코퍼스의 전체적인 통계 정보 고려. 하지만 **단어 의미의 유추 작업 Analogy task에는 성능이 떨어짐**  \n",
        "\n",
        "* Word2Vec: 예측 기반. 단어 간 유추 작업에는 LSA보다 뛰어남. **임베딩 벡터가 윈도우 크기 내에서만 주변 단어를 고려하기 때문에 코퍼스의 전체적인 통계 정보 반영 못함**\n",
        "* 이러한 점을 비판하여 카운트 기반과 예측 기반 방법론 두 가지 모두 사용"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96dj5aiadTeH"
      },
      "source": [
        "### 2. 윈도우 기반 동시 등장 행렬 Window based Co-occurrence Matrix\n",
        "* 단어의 동시 등장 행렬 Co-occurrence Matrix: 행과 열을 전체 단어 집합의 단어들로 구성. \n",
        "  * i 단어의 Window Size 내에서 k 단어가 등장한 횟수를 i행 k열에 기재한 행렬\n",
        "  * Transpose해도 동일한 행렬이 된다는 특징\n",
        "\n",
        "\n",
        "### 3. 동시 등장 확률 Co-occurence Probability\n",
        "* P(k|i): 동시 등장 행렬로부터 특정 단어 i의 전체 등장 횟수를 카운트하고, 특정 단어 i가 등장했을 때 어떤 단어 k가 등장한 횟수를 카운트하여 계산한 조건부 확률\n",
        "  * i: Center Word, k: Context Word\n",
        "  * 중심 단어 i의 행의 모든 값을 더한 값을 분모, i행 k열의 값을 분자로 한 값\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5dnTVvteQhk"
      },
      "source": [
        "### 4. 손실 함수 Loss Function\n",
        "* X: 동시 등장 행렬 Co-occurence Matrix\n",
        "* Xij: 중심 단어 i가 등장했을 때 윈도우 내 주변 단어 j가 등장하는 횟수\n",
        "* sigma j Xij: 동시 등장 행렬에서 i행의 값을 모두 더한 값\n",
        "* Pik = P(k|i) = Xik/Xi: 중심 단어 i가 등장했을 때 윈도우 내 주변 단어 k가 등장할 확률\n",
        "  * ex) P(solid|ice) =  ice가 등장했을 때 solid가 등장할 확률\n",
        "* Pik/Pjk\n",
        "  ex) P(solid|ice)/P(solid|steam) = 8.9\n",
        "* wi: 중심 단어 i의 임베딩 벡터\n",
        "* ὦk: 주변 단어 k의 임베딩 벡터\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZuT8Mol5e7oO"
      },
      "source": [
        "* GloVe in one sentence\n",
        "  > 임베딩 된 중심 단어와 주변 단어 벡터의 내적이 전체 코퍼스에서 동시 등장 확률이 되도록 만드는 것\n",
        "  > dot product(wi ὦk) ≈ P(k|i) = Pik\n",
        "\n",
        "  > 더 정확하게는,\n",
        "  > dot product(wi ὦk) ≈ logP(k|i) = logPik "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NiLZPCNXfX68"
      },
      "source": [
        "* 손실 함수: 단어 간의 관계 잘 표현해야 함\n",
        "  > F(wi, wj, ὦk) = Pik/Pjk\n",
        "* F의 목적: 두 단어 사이의 동시 등장 확률의 크기 관계 비(ratio) 정보를 벡터 공간에 인코딩 하는 것이 목적\n",
        "* F의 입력: \n",
        "  > F(wi - wj, ὦk) = Pik/Pjk\n",
        "* 우변은 스칼라, 좌변은 벡터. Dot product 수행\n",
        "  > F((wi - wj)Tὦk) = Pik/Pjk\n",
        "  * Linear space에서의 단어 의미 관계 표현 위해 뺄셈과 내적 택함  \n",
        "\n",
        "\n",
        "* F가 만족해야 할 필수 조건: 중심 단어 w와 주변 단어 ὦ라는 선택 기준은 실제로는 무작위 선택이므로 둘의 관계는 자유롭게 교환될 수 있도록 해야됨\n",
        "  * **F가 실수의 덧셈과 양수의 곱셈에 대해서 *준동형 Homomorphism*을 만족하도록 함**\n",
        "    > F(a+b) = F(a)F(b)\n",
        "* F에 적용\n",
        "  > F(v1Tv2 + v3Tv4) = F(v1Tv2)F(v3Tv4)\n",
        "* 뺄셈에 대한 준동형식으로 변경\n",
        "  > F(v1Tv2 - v3Tv4) = F(v1Tv2) / F(v3Tv4)\n",
        "* 이를 Glove씩에 적용\n",
        "  > F((wi - wj)Tὦk) = F(wiTὦk) / F(wjTὦk)\n",
        "* 우변\n",
        "  > Pik/Pjk = F(wiTὦk) / F(wjTὦk)\n",
        "  > F(wiTὦk) = Pik = Xik/Xi\n",
        "* 좌변\n",
        "  > F(wiTὦk - wjTὦk) = F(wiTὦk) / F(wjTὦk)\n",
        "\n",
        "* F 함수에 지수 함수\n",
        "  > expF(wiTὦk - wjTὦk) = expF(wiTὦk) / expF(wjTὦk)\n",
        "\n",
        "  > wiTὦk = logPik = log(Xik/Xi) = logXik - logXi\n",
        "\n",
        "* wi와 ὦk는 값 바뀌어도 식 성립\n",
        "* logXi항을 wi에 대한 편향 bi로 대체\n",
        "  > wiTὦk + bi + b tilde - logXik\n",
        "* 손실함수 일반화\n",
        "  * Loss func = V sigma m, n = (wmTὦn + bm + b tilde n - logXmn)^2\n",
        "    * V는 단어 집합의 크기. \n",
        "    * Still 문제.. \n",
        "        * logXik에서 Xik 값이 0이 될 수 있으므로 log(1+Xik)로 변경\n",
        "        * 동시 등장 행렬 X는 DTM처럼 희소 행렬 Sparse Matrix일 가능성 다분.\n",
        "          * Xik 갑에 영향을 받는 가중치 함수 Weighting function f(Xik)를 손실 함수에 도임\n",
        "            * Xik 값이 작으면 상대적으로 함수의 값은 작은데, 크면 최대값이 정해져 있음\n",
        "              > f(x) = min(1, (x/xmax)^3/4)\n",
        "* **최종**\n",
        "  > Loss func = V sigma m, n = f(Xmn)(wmTὦn + bm + b tilde n - logXmn)^2        \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QUxHL5tjhGO"
      },
      "source": [
        "### 5. GloVe 훈련시키기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JtnPu70ajW5N",
        "outputId": "101378c6-087c-4c25-9721-1e51e0363567"
      },
      "source": [
        "!python -m pip install glove_python_binary"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: glove_python_binary in /usr/local/lib/python3.7/dist-packages (0.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from glove_python_binary) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from glove_python_binary) (1.4.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KcVMOrCkmhb2",
        "outputId": "49d4e2d1-752f-4339-d7cc-8a9cb83e5f8b"
      },
      "source": [
        "# glov_python_library가 계속 import error가 나서 대신 mittens를 사용했음\n",
        "# 근데 mittens에는 Corpus가 없어 오류가 남..\n",
        "!pip install -U mittens\n",
        "from mittens import GloVe as Glove"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: mittens in /usr/local/lib/python3.7/dist-packages (0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from mittens) (1.19.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AmERgEMidPYV"
      },
      "source": [
        "# from glove_pybind import *\n",
        "# from .Glove import Corpus, Glove\n",
        "\n",
        "corpus = Corpus() \n",
        "\n",
        "# 훈련 데이터로부터 GloVe에서 사용할 동시 등장 행렬 생성\n",
        "corpus.fit(result, window=5)\n",
        "glove = Glove(no_components=100, learning_rate=0.05)\n",
        "\n",
        "# 학습에 이용할 쓰레드의 개수는 4로 설정, 에포크는 20.\n",
        "glove.fit(corpus.matrix, epochs=20, no_threads=4, verbose=True)\n",
        "glove.add_dictionary(corpus.dictionary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rA5m068jjkku"
      },
      "source": [
        "print(glove.most_similar(\"man\"))\n",
        "print(glove.most_similar(\"boy\"))\n",
        "print(glove.most_similar(\"university\"))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}