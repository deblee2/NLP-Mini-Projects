{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "What_is_NLP_Ch16_Attention.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVnHHepAJXCa"
      },
      "source": [
        "# 16. Attention Mechanism\n",
        "## 1. Attention Mechanism\n",
        "* **seq2seq 모델**: 인코더에서 입력 시퀀스를 **컨텍스트 벡터**라는 하나의 고정된 크기의 벡터 표현으로 압축하고, 디코더는 이 컨텍스트 벡터를 통해서 출력 시퀀스를 만들어냄\n",
        "* 하지만, 이러한 RNN 기반 seq2seq 모델은 크게 두 가지 문제가 있음\n",
        "  1. 하나의 **고정**된 크기의 벡터에 **모든 정보를 압축**하려고 하니까 정보 손실 발생\n",
        "  2. RNN의 고질적인 문제인 **기울기 소실(Vanishing Gratient) 문제**가 존재\n",
        "* 결국 이는 기계 번역 분야에서 입력 문장이 길면 번역 품질이 떨어지는 현상으로 나타남\n",
        "* 이를 위한 대안으로 입력 시퀀스가 길어지면 출력 시퀀스의 정혹도가 떨어지는 것을 보정해주기 위해 등장한 기법인 **어텐션(attention)** 소갸\n",
        "\n",
        "### 1. The Idea of Attention\n",
        "* 어텐션의 기본 아이디어: **디코더에서 출력 단어를 예측하는 매 시점(timestep)마다, 인코더에서의 전체 입력 문장을 다시 한 번 참고**한다는 점\n",
        "* 단, 전체 입력 문장을 전부 다 동일한 비율로 참고하는 것이 아니라, **해당 시점에서 예측해야 할 단어와 연관이 있는 입력 단어 부분을 좀 더 집중(attention)**해서 보게 됨\n",
        "\n",
        "### 2. Attention Function\n",
        "* 우선 Key-Value로 구성되는 자료형에 대해서 잠깐 언급\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ixYHAJBvJRDy"
      },
      "source": [
        "# 파이썬의 딕셔너리 자료형을 선언\n",
        "# 키(Key): 값(Value)의 형식으로 키와 값의 쌍(Pair)를 선언\n",
        "dict = {\"2017\": \"Transformer\", \"2018\":\"BERT\"}"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uu2hmQOwK09i",
        "outputId": "bab70984-3012-4898-a693-77113e1e413e"
      },
      "source": [
        "print(dict[\"2017\"])"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transformer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NY7KAnQbK3HU",
        "outputId": "07c1a93f-27c2-47b8-9954-ec15020bbc76"
      },
      "source": [
        "print(dict[\"2018\"])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BERT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkeiaFo4LTLK"
      },
      "source": [
        "* 어텐션을 함수로 표현하면 주로 다음과 같이 표현됨\n",
        "> Attention(Q, K, V) = Attention Value\n",
        "* 어텐션 함수는 주어진 '쿼리(Query)'에 대해서 모든 '키(Key)'와의 유사도를 각각 구함\n",
        "* 그리고 구해낸 이 유사도를 키와 맵핑되어 있는 각각의 '값(Value)'에 반영해줌\n",
        "* 유사도가 반영된 '값(Value)'를 모두 더해서 리턴\n",
        "* 이를 **어텐션 값(Attention Value)**라고 함\n",
        "  1. Q = Query : t 시점의 디코더 셀에서의 은닉 상태 \n",
        "  2. K = Keys : 모든 시점의 인코더 셀의 은닉 상태들 \n",
        "  3. V = Values: 모든 시점의 인코더 셀의 은닉 상태들\n",
        "\n",
        "### 3. Dot-Product Attention\n",
        "* 소프트맥스 함수를 통해 나온 결과 값은 I, am, a, student 단어 각각이 출력 단어를 예측할 때 얼마나 도움이 되는지의 정도를 수치화한 값"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDP_OaHZLpdX"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gH2SEIwWK4Lp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}