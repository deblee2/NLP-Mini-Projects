{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "What_is_NLP_Ch9_RNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__Qy2J3JunlL"
      },
      "source": [
        "## 09) 순환 신경망(Recurrent Neural Network)\n",
        "* 피드 포워드 신경망은 입력의 길이가 고정되어 있어 한계 있음\n",
        "* 다양한 길이의 입력 시퀀스를 처리할 수 있는 인공 신경망 필요  \n",
        "\n",
        "\n",
        "### 1) RNN\n",
        "* RNN은 시퀀스 Sequence 모델. 입력과 출력을 시퀀스 단위로 처리하는 모델.\n",
        "* 재귀 신경망(Recursive Neural Network)는 전혀 다름\n",
        "\n",
        "#### 1. RNN\n",
        "* 피드 포워드 신경망: 전부 은닉층에서 활성화 함수를 지난 값은 오직 출력층 방향만 향함\n",
        "* BUT, RNN은 은닉층의 노드에서 활성화 함수를 통해 나온 결과값을 출력층 방향으로도 보내면서, **다시 은닉층 노드의 다음 계산의 입력으로 보내는 특징 가짐**  \n",
        "\n",
        "\n",
        "* 메모리 셀, RNN 셀: 은닉층에서 활성화 함수를 통해 결과를 내보내는 역할을 하는 노드. 이전의 값을 기억하려고 하는 일종의 메모리 역할.\n",
        "* 은닉층의 메모리 셀: 각각의 시점 time step에서 바로 이전 시점에서의 은닉층 메모리 셀에서 나온 값을 자신의 입력으로 하는 재귀적 활동\n",
        "* **은닉 상태 hidden state**: 메모리 셀이 출력층 방향으로 또는 다음 시점 t+1의 자신에게 보내는 값\n",
        "* 피드 포워드 신경망: 뉴런, RNN: 뉴런보다는 입력 벡터, 출력 벡터, 은닉 상태  \n",
        "\n",
        "\n",
        "* 입출력 길이 다르게 가능\n",
        "  * 가장 보편적 입출력 단위는 단어 벡터\n",
        "  * ex) one-to-many: 이미지 캡셔닝(하나의 이미지 입력에 대해서 사진의 제목 출력)\n",
        "  * ex) many-to-one: 감성 분류, 스팸 메일 분류\n",
        "  * ex) many-to-many: 입력 문장으로부터 대답 문장 출력하는 챗봇. 입력 문장으로부터 번역된 문장을 출력하는 번역기. 개체명 인식. 품사 태깅.\n",
        "* ht를 계산하기 위한 활성화 함수: tanh, ReLU\n",
        "\n",
        "#### 2. Keras로 RNN 구현\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJUH5aNrxpLl"
      },
      "source": [
        "# RNN층을 추가하는 코드\n",
        "model.add(SimpleRNN(hidden_size))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqD_vzwEyQtI"
      },
      "source": [
        "* hidden_size: 은닉 상태의 크기 정의. output_dim과 동일. RNN의 용량을 늘린다.. 128, 256, 512, 1024\n",
        "* output_dim: 메모리 셀이 다음 시점의 메모리 셀과 출력층으로 보내는 값의 크기\n",
        "* timesteps: 입력 시퀀스의 길이(input_length). 시점의 수.\n",
        "* input_dim: 입력의 크기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltD_fZ_Txvr2"
      },
      "source": [
        "# 추가 인자 사용\n",
        "model.add(SimpleRNN(hidden_size, input_shape=(timesteps, input_dim)))\n",
        "\n",
        "# 다른 표기\n",
        "model.add(SimpleRNN(hidden_size, input_length=M, input_dim=N))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NebhvyTEywbD"
      },
      "source": [
        "* RNN 층은 (batch_size, timesteps, input_dim) 크기의 3D 텐서를 입력으로\n",
        "* 위 코드의 결과는 출력층이 아니라 하나의 은닉 상태  \n",
        "\n",
        "* if 은닉 상태만 리턴, (batch_size, output_dim) 크기의 2D 텐서\n",
        "* if 전체 시퀀스 리턴, (batch_size, timesteps, output_dim) 크기의 3D 텐서를 리턴\n",
        "  * 이는 RNN 층의 return_sequences 매개 변수에 True를 설정하여 설정 가능  \n",
        "\n",
        "* if 마지막 은닉 상태만 전달, many-to-one 문제 해결\n",
        "* if 모든 시점의 은닉 상태 전달, 다음 층에 은닉층이 하나 더 있거나 many-to-many 해결 가능\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JA4ptlMDz6ze",
        "outputId": "64d0abdb-47be-4486-97ba-61dd1f4518b6"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import SimpleRNN\n",
        "\n",
        "model = Sequential()\n",
        "model.add(SimpleRNN(3, input_shape=(2,10))) #3은 은닉 상태 크기\n",
        "# model.add(SimpleRNN(3, input_length=2, input_dim=10))와 동일함.\n",
        "model.summary()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "simple_rnn (SimpleRNN)       (None, 3)                 42        \n",
            "=================================================================\n",
            "Total params: 42\n",
            "Trainable params: 42\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GewBjqTE0I9q",
        "outputId": "ddf13e62-d1be-4a2c-b153-fbb3482d53a2"
      },
      "source": [
        "# batch size 정의\n",
        "model = Sequential()\n",
        "model.add(SimpleRNN(3, batch_input_shape=(8,2,10)))\n",
        "model.summary()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "simple_rnn_1 (SimpleRNN)     (8, 3)                    42        \n",
            "=================================================================\n",
            "Total params: 42\n",
            "Trainable params: 42\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-NWKewZF0Qmf",
        "outputId": "3a499d3e-5251-437c-e3ef-b14c08a9afe8"
      },
      "source": [
        "# return_sequences 매개 변수에 True를 기재\n",
        "# 출력값으로 (batch_size, timesteps, output_dim) 크기의 3D 텐서를 리턴\n",
        "\n",
        "model = Sequential()\n",
        "model.add(SimpleRNN(3, batch_input_shape=(8,2,10), return_sequences=True))\n",
        "model.summary()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "simple_rnn_2 (SimpleRNN)     (8, 2, 3)                 42        \n",
            "=================================================================\n",
            "Total params: 42\n",
            "Trainable params: 42\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqizFVg40e6t"
      },
      "source": [
        "#### 3. 파이썬으로 RNN 구현\n",
        "* 직접 Numpy로 RNN 층 구현"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKX0sQOf0Zai"
      },
      "source": [
        "# 아래의 코드는 의사 코드(pseudocode)로 실제 동작하는 코드가 아님\n",
        "\n",
        "hidden_state_t = 0 # 초기 은닉 상태를 0(벡터)로 초기화\n",
        "for input_t in input_length: # 각 시점마다 입력을 받음\n",
        "    output_t = tanh(input_t, hidden_state_t) # 각 시점에 대해서 입력과 은닉 상태를 가지고 연산\n",
        "    hidden_state_t = output_t # 계산 결과는 현재 시점의 은닉 상태"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZaZbEIJD1kWn"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "timesteps = 10   # 시점의 수. 문장의 길이.\n",
        "input_dim = 4   # 입력의 차원. 단어 벡터의 차원.\n",
        "hidden_size = 8   # 은닉 상태의 크기. 메모리 셀의 용량.\n",
        "\n",
        "# 입력에 해당되는 2D 텐서\n",
        "inputs = np.random.random((timesteps, input_dim))\n",
        "\n",
        "# 초기 은닉 상태는 0(벡터)로 초기화\n",
        "hidden_state_t = np.zeros((hidden_size,)) "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q5NdIE2s2RMc",
        "outputId": "4d8be2dc-4653-4178-ca14-a7ee3cedd800"
      },
      "source": [
        "print(hidden_state_t)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8SVdPJf2u-R"
      },
      "source": [
        "* **Wx: 은닉 상태의 크기 x 입력의 차원**\n",
        "* ** Wh: 은닉 상태의 크기 x 은닉 상태의 크기**\n",
        "* ** b: 은닉 상태의 크기**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4psarP62Vu9"
      },
      "source": [
        "# 가중치와 편향\n",
        "\n",
        "Wx = np.random.random((hidden_size, input_dim))  # (8, 4)크기의 2D 텐서 생성. 입력에 대한 가중치.\n",
        "Wh = np.random.random((hidden_size, hidden_size)) # (8, 8)크기의 2D 텐서 생성. 은닉 상태에 대한 가중치.\n",
        "b = np.random.random((hidden_size,)) # (8,)크기의 1D 텐서 생성. 이 값은 편향(bias)."
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wZGq2C8A2cAR",
        "outputId": "30129d2d-cada-4739-de9d-cebcfa3e7a08"
      },
      "source": [
        "print(np.shape(Wx))\n",
        "print(np.shape(Wh))\n",
        "print(np.shape(b))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(8, 4)\n",
            "(8, 8)\n",
            "(8,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F2bDb18o2jdN",
        "outputId": "588fc7b3-5bc8-40b9-cb93-c93b61a56600"
      },
      "source": [
        "total_hidden_states = []\n",
        "\n",
        "# 메모리 셀 동작\n",
        "for input_t in inputs: # 각 시점에 따라서 입력값이 입력됨.\n",
        "  output_t = np.tanh(np.dot(Wx,input_t) + np.dot(Wh,hidden_state_t) + b) # Wx * Xt + Wh * Ht-1 + b(bias)\n",
        "  total_hidden_states.append(list(output_t)) # 각 시점의 은닉 상태의 값을 계속해서 축적\n",
        "  print(np.shape(total_hidden_states)) # 각 시점 t별 메모리 셀의 출력의 크기는 (timestep, output_dim)\n",
        "  hidden_state_t = output_t\n",
        "\n",
        "# 출력 시 값을 깔끔하게 해주는 용도.\n",
        "total_hidden_states = np.stack(total_hidden_states, axis = 0) \n",
        "\n",
        "# (timesteps, output_dim)\n",
        "print(total_hidden_states)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 8)\n",
            "(2, 8)\n",
            "(3, 8)\n",
            "(4, 8)\n",
            "(5, 8)\n",
            "(6, 8)\n",
            "(7, 8)\n",
            "(8, 8)\n",
            "(9, 8)\n",
            "(10, 8)\n",
            "[[0.83400137 0.93416315 0.95592632 0.74883609 0.75823877 0.90855195\n",
            "  0.96356391 0.98224485]\n",
            " [0.99950465 0.99984241 0.99998116 0.99912536 0.99995231 0.99891907\n",
            "  0.99998284 0.99997251]\n",
            " [0.99991506 0.99997479 0.99999777 0.99991291 0.99998981 0.99944575\n",
            "  0.99999753 0.99999711]\n",
            " [0.99974104 0.99992473 0.99999279 0.99978429 0.99998499 0.99908333\n",
            "  0.9999891  0.99998378]\n",
            " [0.99989641 0.99993846 0.99999275 0.99950951 0.99997006 0.99943216\n",
            "  0.99999598 0.99999356]\n",
            " [0.99978581 0.99987362 0.9999771  0.99945846 0.99995102 0.9987945\n",
            "  0.9999861  0.99997351]\n",
            " [0.99986078 0.9999563  0.99999427 0.99979328 0.99998206 0.99922245\n",
            "  0.99999638 0.99999173]\n",
            " [0.99993001 0.99996484 0.99999765 0.99982538 0.99998651 0.99960933\n",
            "  0.99999686 0.99999779]\n",
            " [0.99984847 0.99991956 0.99999109 0.99962776 0.99997296 0.99925958\n",
            "  0.99999168 0.99998955]\n",
            " [0.99985708 0.99994578 0.99999322 0.99968647 0.99997846 0.99928214\n",
            "  0.99999581 0.999991  ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-S7IuIY3gOd"
      },
      "source": [
        "#### 4. Deep RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n041YBXI3Ri-"
      },
      "source": [
        "# 은닉층 2개 추가\n",
        "model = Sequential()\n",
        "model.add(SimpleRNN(hidden_size, input_length=10, input_dim=5, return_sequences=True))\n",
        "model.add(SimpleRNN(hidden_size, return_sequences=True))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fyyTwGO3jO1"
      },
      "source": [
        "#### 5. 양방향 RNN\n",
        "* 시점 t에서의 출력값을 예측할 때 이전 시점의 데이터뿐만 아니라, 이후 데이터로도 예측할 수 있다는 아이디어에 기반\n",
        "* 하나의 출력값을 예측하기 위해 기본적으로 두 개의 메모리 셀을 사용\n",
        "  * **앞 시점의 은닉 상태(Forward States), 뒤 시점의 은닉 상태(Backward States)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GEPPOft93pO-"
      },
      "source": [
        "from tensorflow.keras.models import Bidirectional\n",
        "\n",
        "timesteps = 10\n",
        "input_dim = 5\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Bidirectional(SimpleRNN(hidden_size, return_sequences = True), input_shape=(timesteps, input_dim)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4kljrteO4jyY"
      },
      "source": [
        "### 2) 장단기 메모리 Long Short-Term Memory, LSTM\n",
        "* ** 장기 의존성 문제 The problem of Long-Term Dependencies ** \n",
        "  * 바닐라 RNN의 단점으로 비교적 짧은 sequence에 대해서만 효과 보임. 시점 time step이 길어질 수록 앞의 정보가 뒤로 충분히 전달되지 못함.  \n",
        "\n",
        "\n",
        "* LSTM은 은닉층의 메모리 셀에 **입력 게이트, 망각 게이트, 출력 게이트**를 추가하여 불필요한 기억을 지우고, 기억해야 할 것들을 정함.\n",
        "  * **은닉 상태 hidden state**를 계산하는 식이 전통 RNN보다 복잡\n",
        "  * **셀 상태 cell state**라는 값을 추가\n",
        "\n",
        "1. 입력 게이트: 현재 정보를 기억하기 위한 게이트\n",
        "2. 삭제 게이트: 기억을 삭제하기 위한 게이트\n",
        "3. 셀 상태(장기 상태)\n",
        "4. 출력 게이트와 은닉 상태(단기 상태)\n",
        "\n",
        "구체적인 설명은 https://wikidocs.net/22888 를 참고하면 됨\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xbZeYjiLRGH"
      },
      "source": [
        "### 3) 게이트 순환 유닛 Gated Recurrent Unit, GRU\n",
        "* 은닉 상태를 업데이트 하는 계산을 줄임\n",
        "* LSTM에는 출력, 입력, 삭제 게이트가 존재하지만, GRU에서는 **업데이트 게이트와 리셋 게이트** 두 가지 게이트만 존재\n",
        "* 데이터 양이 적을 때는 매개 변수의 양이 적은 GRU\n",
        "* 데이터 양이 더 많으면 LSTM이 더 나음"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wymzWUF5LM4y"
      },
      "source": [
        "model.add(GRU(hidden_size, input_shape=(timesteps, input_dim)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4M_lqN2LrGS"
      },
      "source": [
        "### 4) Keras의 SimpleRNN과 LSTM 이해하기\n",
        "#### 1. 임의의 입력 생성하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AHt2IZvbLqFW",
        "outputId": "27569868-e0ab-4123-9bd1-2fafb4ff0300"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import SimpleRNN, LSTM, Bidirectional\n",
        "\n",
        "# 단어 벡터 차원 5, 문장의 길이 4\n",
        "train_X = [[0.1, 4.2, 1.5, 1.1, 2.8], [1.0, 3.1, 2.5, 0.7, 1.1], [0.3, 2.1, 1.5, 2.1, 0.1], [2.2, 1.4, 0.5, 0.9, 1.1]]\n",
        "print(np.shape(train_X))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4, 5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EcebEukVMBdr"
      },
      "source": [
        "* 4번의 시점 timesteps이 존재하고, 각 시점마다 5차원의 단어 벡터가 입력으로 사용됨"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h0vyXqZHLzx1",
        "outputId": "db2722ec-d8f4-4cfc-e049-b30bc16f5c12"
      },
      "source": [
        "#  RNN은 2D 텐서가 아니라 3D 텐서를 입력을 받음\n",
        "# 위에서 만든 2D 텐서를 3D 텐서로 변경합니다. 이는 배치 크기 1을 추가하여 해결\n",
        "\n",
        "train_X = [[[0.1, 4.2, 1.5, 1.1, 2.8], [1.0, 3.1, 2.5, 0.7, 1.1], [0.3, 2.1, 1.5, 2.1, 0.1], [2.2, 1.4, 0.5, 0.9, 1.1]]]\n",
        "train_X = np.array(train_X, dtype=np.float32)\n",
        "print(train_X.shape)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 4, 5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNx3cP6eMWId"
      },
      "source": [
        "####  2. SimpleRNN 이해하기\n",
        "* 대표적인 인자로 return_sequences와 return_state가 있음"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fgBrh4cJMPKR",
        "outputId": "5b9bf0e0-1f61-4fb1-eb87-553bb99915b7"
      },
      "source": [
        "rnn = SimpleRNN(3)\n",
        "# rnn = SimpleRNN(3, return_sequences=False, return_state=False)와 동일.\n",
        "hidden_state = rnn(train_X)\n",
        "\n",
        "print('hidden state : {}, shape: {}'.format(hidden_state, hidden_state.shape))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hidden state : [[-0.61090666 -0.7983958  -0.61283493]], shape: (1, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "StBZ3pMKMnoi"
      },
      "source": [
        "* (1,3)은 마지막 시점의 은닉 상태. 은닉 상태의 크기를 3으로 지정함.\n",
        "* return_sequenes가 False면 마지막 시점의 은닉 상태만 출력\n",
        "* True면 모든 시점의 은닉 상태만 출력"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vyrhTP72MkvU",
        "outputId": "5eb4fb44-17d1-4f28-8555-7dad0cb01093"
      },
      "source": [
        "rnn = SimpleRNN(3, return_sequences=True)\n",
        "hidden_states = rnn(train_X)\n",
        "\n",
        "print('hidden states : {}, shape: {}'.format(hidden_states, hidden_states.shape))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hidden states : [[[ 0.9649639   0.78921616 -0.9214914 ]\n",
            "  [ 0.92051166  0.90270686 -0.90345895]\n",
            "  [ 0.89259624  0.14309247 -0.8523793 ]\n",
            "  [-0.06274243  0.38294312 -0.9980742 ]]], shape: (1, 4, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxGIaMzXM2Im"
      },
      "source": [
        "* (1, 4, 3) 크기의 텐서가 출력됨\n",
        "* 입력 데이터는 (1, 4, 5)의 크기를 가지는 3D 텐서. \n",
        "  * 4가 시점 timesteps에 해당하는 값이므로 모든 시점에 대해서 은닉 상태의 값을 출력하여 (1, 4, 3) 크기의 텐서를 출력하는 것\n",
        "* return_state가 True면 return_sequences의 여부와 상관 없이 *마지막 시점의 은닉 상태 출력*\n",
        "* return_sequences = True, return_state = True ==> 두 개의 출력 리턴"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mxLu_UVpNwOd",
        "outputId": "f3591ce6-4cb2-4d2d-eca7-bd740ab2729f"
      },
      "source": [
        "rnn = SimpleRNN(3, return_sequences=True, return_state=True)\n",
        "hidden_states, last_state = rnn(train_X)\n",
        "\n",
        "print('hidden states : {}, shape: {}'.format(hidden_states, hidden_states.shape))\n",
        "print('last hidden state : {}, shape: {}'.format(last_state, last_state.shape))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hidden states : [[[-0.9633086   0.999996    0.21033907]\n",
            "  [ 0.5332108   0.9999877  -0.83054066]\n",
            "  [-0.8734952   0.99850875 -0.34791538]\n",
            "  [ 0.47813833  0.99915427  0.52087396]]], shape: (1, 4, 3)\n",
            "last hidden state : [[0.47813833 0.99915427 0.52087396]], shape: (1, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwBITp4aN767"
      },
      "source": [
        "* 첫번째는 return_sequences = True로 인한 모든 시점의 은닉 상태\n",
        "* 두번째는 return_state = True로 인한 마지막 시점의 은닉 상태"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VFIiO5xSNyY6",
        "outputId": "265b863f-9302-4781-c124-09be992c472a"
      },
      "source": [
        "# return_sequences = False, return_state = True인 경우?\n",
        "rnn = SimpleRNN(3, return_sequences=False, return_state=True)\n",
        "hidden_state, last_state = rnn(train_X)\n",
        "\n",
        "print('hidden state : {}, shape: {}'.format(hidden_state, hidden_state.shape))\n",
        "print('last hidden state : {}, shape: {}'.format(last_state, last_state.shape))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hidden state : [[-0.770695   -0.990965    0.65312725]], shape: (1, 3)\n",
            "last hidden state : [[-0.770695   -0.990965    0.65312725]], shape: (1, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i36HqDVHOVVJ"
      },
      "source": [
        "* 두 개의 출력 모두 마지막 시점의 은닉 상태 출력"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16-3f_LHOkg6"
      },
      "source": [
        "### 3. LSTM 이해하기\n",
        "* return_sequences = False, return_state = True"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j5_KRjRrOQ4P",
        "outputId": "b898c533-34ee-47e0-f7d7-dd23d8c81b71"
      },
      "source": [
        "lstm = LSTM(3, return_sequences=False, return_state=True)\n",
        "hidden_state, last_state, last_cell_state = lstm(train_X)\n",
        "\n",
        "print('hidden state : {}, shape: {}'.format(hidden_state, hidden_state.shape))\n",
        "print('last hidden state : {}, shape: {}'.format(last_state, last_state.shape))\n",
        "print('last cell state : {}, shape: {}'.format(last_cell_state, last_cell_state.shape))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hidden state : [[ 0.057607    0.07817724 -0.24318294]], shape: (1, 3)\n",
            "last hidden state : [[ 0.057607    0.07817724 -0.24318294]], shape: (1, 3)\n",
            "last cell state : [[ 0.13276935  0.6909851  -1.2508996 ]], shape: (1, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bewiaGb-Oqev"
      },
      "source": [
        "* SimpleRNN과 달리, 세 개의 결과 반환\n",
        "* 첫 번째 결과: return_sequences = False이므로 마지막 시점의 은닉 상태\n",
        "* **그런데 LSTM이 SimpleRNN과 다른 점은 return_state = True인 경우에는 마지막 시점의 은닉 상태 뿐만 아니라 셀 상태도 반환한다는 점**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SRq03dhMOoFz",
        "outputId": "8974295f-ae87-408d-ed01-d462f64cd64e"
      },
      "source": [
        "lstm = LSTM(3, return_sequences=True, return_state=True)\n",
        "hidden_states, last_hidden_state, last_cell_state = lstm(train_X)\n",
        "\n",
        "print('hidden states : {}, shape: {}'.format(hidden_states, hidden_states.shape))\n",
        "print('last hidden state : {}, shape: {}'.format(last_hidden_state, last_hidden_state.shape))\n",
        "print('last cell state : {}, shape: {}'.format(last_cell_state, last_cell_state.shape))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hidden states : [[[ 0.00396451 -0.00716264 -0.00886327]\n",
            "  [-0.02562587 -0.08779231 -0.0194691 ]\n",
            "  [-0.0313523  -0.20313898 -0.02970505]\n",
            "  [-0.06266455 -0.15132275 -0.04431514]]], shape: (1, 4, 3)\n",
            "last hidden state : [[-0.06266455 -0.15132275 -0.04431514]], shape: (1, 3)\n",
            "last cell state : [[-0.22602531 -0.54194105 -0.0599919 ]], shape: (1, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OeQhnlswPDeb"
      },
      "source": [
        "* return_state =  True이므로 두 번째 출력값이 마지막 은닉 상태, 세 번째 출력값이 마지막 셀 상태인 것은 변함 없음\n",
        "* 그런데 return_sequences = True이므로 첫번째 출력 값은 *모든 시점*의 은닉 상태 출력"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jF6F7AbQPkGb"
      },
      "source": [
        "### 4. Bidirectional(LSTM) 이해하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMyhv-wfO-hZ"
      },
      "source": [
        "k_init = tf.keras.initializers.Constant(value=0.1)\n",
        "b_init = tf.keras.initializers.Constant(value=0)\n",
        "r_init = tf.keras.initializers.Constant(value=0.1)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-E2NTkzwPyvs",
        "outputId": "6d09196a-09d1-455e-f500-1cebd53c7546"
      },
      "source": [
        "# return_sequences = False, return_state = True\n",
        "bilstm = Bidirectional(LSTM(3, return_sequences=False, return_state=True, \\\n",
        "                            kernel_initializer=k_init, bias_initializer=b_init, recurrent_initializer=r_init))\n",
        "hidden_states, forward_h, forward_c, backward_h, backward_c = bilstm(train_X)\n",
        "\n",
        "print('hidden states : {}, shape: {}'.format(hidden_states, hidden_states.shape))\n",
        "print('forward state : {}, shape: {}'.format(forward_h, forward_h.shape))\n",
        "print('backward state : {}, shape: {}'.format(backward_h, backward_h.shape))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hidden states : [[0.63031393 0.63031393 0.63031393 0.7038734  0.7038734  0.7038734 ]], shape: (1, 6)\n",
            "forward state : [[0.63031393 0.63031393 0.63031393]], shape: (1, 3)\n",
            "backward state : [[0.7038734 0.7038734 0.7038734]], shape: (1, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6h9PtsOYP5kQ"
      },
      "source": [
        "* 5개의 값을 반환\n",
        "* return_state = True인 경우,\n",
        "  * 정방향 LSTM의 은닉 상태와 셀 상태\n",
        "  * 역방향 LSTM의 은닉 상태와 셀 상태 4가지를 반환\n",
        "* 셀 상태는 forward_c와 backward_c에 저장하고 출력하지 않음  \n",
        "\n",
        "* return_sequences = False인 경우, 정방향 LSTM의 마지막 시점인 은닉 상태와 역방향 LSTM의 첫번째 시점의 은닉 상태가 연결된 채 반환되기 때문  \n",
        "\n",
        "* return_state = True인 경우, 반환한 은닉 상태의 값인 forward_h와 backward_h는 각각 정방향 LSTM의 마지막 시점인 은닉 상태와 역방향 LSTM의 첫번째 시점의 은닉 상태값. 이 두 값을 연결한 값이 hidden_states에 출력되는 값"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IyuONkacP4mq"
      },
      "source": [
        "# 은닉 상태의 값을 고정시켜두었기 때문에 return_sequences를 True로 할 경우, 출력이 어떻게 바뀌는지 비교가 가능\n",
        "\n",
        "bilstm = Bidirectional(LSTM(3, return_sequences=True, return_state=True, \\\n",
        "                            kernel_initializer=k_init, bias_initializer=b_init, recurrent_initializer=r_init))\n",
        "hidden_states, forward_h, forward_c, backward_h, backward_c = bilstm(train_X)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3TlK6fsGQ6J8",
        "outputId": "43ee8b6a-eb5e-436b-b50f-41756f7cc54f"
      },
      "source": [
        "print('hidden states : {}, shape: {}'.format(hidden_states, hidden_states.shape))\n",
        "print('forward state : {}, shape: {}'.format(forward_h, forward_h.shape))\n",
        "print('backward state : {}, shape: {}'.format(backward_h, backward_h.shape))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hidden states : [[[0.35906473 0.35906473 0.35906473 0.7038734  0.7038734  0.7038734 ]\n",
            "  [0.5511133  0.5511133  0.5511133  0.58863586 0.58863586 0.58863586]\n",
            "  [0.59115744 0.59115744 0.59115744 0.3951699  0.3951699  0.3951699 ]\n",
            "  [0.63031393 0.63031393 0.63031393 0.21942244 0.21942244 0.21942244]]], shape: (1, 4, 6)\n",
            "forward state : [[0.63031393 0.63031393 0.63031393]], shape: (1, 3)\n",
            "backward state : [[0.7038734 0.7038734 0.7038734]], shape: (1, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mL8ZypVbRBdC"
      },
      "source": [
        "* hidden states의 출력값에서는 이제 모든 시점의 은닉 상태가 출력됨\n",
        "* 역방향 LSTM의 첫 번째 시점의 은닉 상태는 더 이상 정방향 LSTM의 마지막 시점의 은닉 상태와 연결되는 것이 아니라 정방향 LSTM의 첫번째 시점의 은닉 상태와 연결"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s41z4dK8RRDa"
      },
      "source": [
        "### 5) RNN 언어 모델 (RNNLM)\n",
        "* RNN으로 입력의 길이를 고정하지 않을 수 있음\n",
        "* 훈련 기법: **교사 강요(teacher forcing)**\n",
        "  * 훈련 과정에서는 A 훈련 샘플이 있다면, A 시퀀스를 모델의 입력으로 넣어 A를 예측하도록 훈련\n",
        "  * 모델이 t 시점에서 예측한 값을 t+1 시점에 입력으로 넣지 않고, t 시점의 레이블, 즉 실제로 알고있는 '정답'을 t+1 시점의 입력으로 넣음\n",
        "* 훈련 시 출력층에서 사용하는 활성화 함수: Softmax funciton\n",
        "* 손실함수: Cross entropy function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNA33sJKSrM3"
      },
      "source": [
        "### 6) Text Generation using RNN\n",
        "* many-to-one 구조의 RNN으로 문맥을 반영하여 텍스트 생성\n",
        "1. 데이터에 대한 이해와 전처리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPxxQyLtQ7cw"
      },
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4L0LsczmS4WT"
      },
      "source": [
        "text = \"\"\"경마장에 있는 말이 뛰고 있다\\n\n",
        "그의 말이 법이다\\n\n",
        "가는 말이 고와야 오는 말이 곱다\\n\"\"\""
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1j2uU4BbS5Z_",
        "outputId": "d04ade96-a64f-437b-880e-8e270b0f9608"
      },
      "source": [
        "# 단어 집합 생성하고 크기 확인\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([text])\n",
        "# 단어 집합의 크기를 저장할 때는 케라스 토크나이저의 정수 인코딩은 인덱스가 1부터 시작하지만, 패딩을 위한 0을 고려하여 +1\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print('단어 집합의 크기 : %d' % vocab_size)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "단어 집합의 크기 : 12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UysP8av5TDdy",
        "outputId": "0dabf266-32cd-4bd4-8796-4fce95f4eae3"
      },
      "source": [
        "print(tokenizer.word_index)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'말이': 1, '경마장에': 2, '있는': 3, '뛰고': 4, '있다': 5, '그의': 6, '법이다': 7, '가는': 8, '고와야': 9, '오는': 10, '곱다': 11}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ubI-1EoPTFVs",
        "outputId": "431845a2-4e9e-4700-c903-a203138218e0"
      },
      "source": [
        "# 훈련 데이터 만들기\n",
        "sequences = list()\n",
        "for line in text.split('\\n'): # Wn을 기준으로 문장 토큰화\n",
        "    encoded = tokenizer.texts_to_sequences([line])[0]\n",
        "    for i in range(1, len(encoded)):\n",
        "        sequence = encoded[:i+1]\n",
        "        sequences.append(sequence)\n",
        "\n",
        "print('학습에 사용할 샘플의 개수: %d' % len(sequences))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "학습에 사용할 샘플의 개수: 11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6i8Z9CeFTIhh",
        "outputId": "2421c016-d683-41b9-98c4-084597327ef9"
      },
      "source": [
        "print(sequences)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[2, 3], [2, 3, 1], [2, 3, 1, 4], [2, 3, 1, 4, 5], [6, 1], [6, 1, 7], [8, 1], [8, 1, 9], [8, 1, 9, 10], [8, 1, 9, 10, 1], [8, 1, 9, 10, 1, 11]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l6QEAWsETP_u",
        "outputId": "8758b9d4-fb34-4f44-d662-403772726a77"
      },
      "source": [
        "# 전체 샘플에 대해서 길이를 일치\n",
        "max_len = max(len(l) for l in sequences) # 모든 샘플에서 길이가 가장 긴 샘플의 길이 출력\n",
        "print('샘플의 최대 길이 : {}'.format(max_len))\n",
        "# 전체 샘플의 길이를 6으로 패딩\n",
        "sequences = pad_sequences(sequences, maxlen=max_len, padding='pre')\n",
        "print(sequences)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "샘플의 최대 길이 : 6\n",
            "[[ 0  0  0  0  2  3]\n",
            " [ 0  0  0  2  3  1]\n",
            " [ 0  0  2  3  1  4]\n",
            " [ 0  2  3  1  4  5]\n",
            " [ 0  0  0  0  6  1]\n",
            " [ 0  0  0  6  1  7]\n",
            " [ 0  0  0  0  8  1]\n",
            " [ 0  0  0  8  1  9]\n",
            " [ 0  0  8  1  9 10]\n",
            " [ 0  8  1  9 10  1]\n",
            " [ 8  1  9 10  1 11]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGukhz5VTakW"
      },
      "source": [
        "# 각 샘플의 마지막 단어를 레이블로 분리\n",
        "sequences = np.array(sequences)\n",
        "X = sequences[:,:-1] # 마지막 값을 제외하고 저장\n",
        "y = sequences[:,-1] # 마지막 값만 저장"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nXIlMdk_Tmje",
        "outputId": "67f43fe1-0c2e-40cd-9ded-e2364d070fd8"
      },
      "source": [
        "print(X)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0  0  0  0  2]\n",
            " [ 0  0  0  2  3]\n",
            " [ 0  0  2  3  1]\n",
            " [ 0  2  3  1  4]\n",
            " [ 0  0  0  0  6]\n",
            " [ 0  0  0  6  1]\n",
            " [ 0  0  0  0  8]\n",
            " [ 0  0  0  8  1]\n",
            " [ 0  0  8  1  9]\n",
            " [ 0  8  1  9 10]\n",
            " [ 8  1  9 10  1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QYpgVQ6bToZO",
        "outputId": "cc5dd6cf-dcb2-4265-8bf0-f76ea68f8d4f"
      },
      "source": [
        "print(y)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 3  1  4  5  1  7  1  9 10  1 11]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Osi0OhvFTpgq",
        "outputId": "9b05c3e9-7cbc-4894-a080-ee80c811d608"
      },
      "source": [
        "# 원-핫 인코딩\n",
        "y = to_categorical(y, num_classes=vocab_size)\n",
        "print(y)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOG77zvSTwrv"
      },
      "source": [
        "2. 모델 설계하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_78dLG-5Ts7S",
        "outputId": "8721847c-02e2-4cf3-e7b7-9b0463c2f06d"
      },
      "source": [
        "# RNN 모델에 데이터 훈련\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Dense, SimpleRNN\n",
        "\n",
        "embedding_dim = 10\n",
        "hidden_units = 32\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, embedding_dim))\n",
        "model.add(SimpleRNN(hidden_units))\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(X, y, epochs=200, verbose=2)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "1/1 - 1s - loss: 2.5001 - accuracy: 0.0909\n",
            "Epoch 2/200\n",
            "1/1 - 0s - loss: 2.4835 - accuracy: 0.0909\n",
            "Epoch 3/200\n",
            "1/1 - 0s - loss: 2.4671 - accuracy: 0.0909\n",
            "Epoch 4/200\n",
            "1/1 - 0s - loss: 2.4508 - accuracy: 0.0909\n",
            "Epoch 5/200\n",
            "1/1 - 0s - loss: 2.4343 - accuracy: 0.0909\n",
            "Epoch 6/200\n",
            "1/1 - 0s - loss: 2.4177 - accuracy: 0.1818\n",
            "Epoch 7/200\n",
            "1/1 - 0s - loss: 2.4007 - accuracy: 0.1818\n",
            "Epoch 8/200\n",
            "1/1 - 0s - loss: 2.3832 - accuracy: 0.1818\n",
            "Epoch 9/200\n",
            "1/1 - 0s - loss: 2.3652 - accuracy: 0.2727\n",
            "Epoch 10/200\n",
            "1/1 - 0s - loss: 2.3466 - accuracy: 0.2727\n",
            "Epoch 11/200\n",
            "1/1 - 0s - loss: 2.3273 - accuracy: 0.4545\n",
            "Epoch 12/200\n",
            "1/1 - 0s - loss: 2.3073 - accuracy: 0.5455\n",
            "Epoch 13/200\n",
            "1/1 - 0s - loss: 2.2865 - accuracy: 0.5455\n",
            "Epoch 14/200\n",
            "1/1 - 0s - loss: 2.2649 - accuracy: 0.5455\n",
            "Epoch 15/200\n",
            "1/1 - 0s - loss: 2.2426 - accuracy: 0.5455\n",
            "Epoch 16/200\n",
            "1/1 - 0s - loss: 2.2195 - accuracy: 0.4545\n",
            "Epoch 17/200\n",
            "1/1 - 0s - loss: 2.1958 - accuracy: 0.4545\n",
            "Epoch 18/200\n",
            "1/1 - 0s - loss: 2.1715 - accuracy: 0.4545\n",
            "Epoch 19/200\n",
            "1/1 - 0s - loss: 2.1467 - accuracy: 0.4545\n",
            "Epoch 20/200\n",
            "1/1 - 0s - loss: 2.1215 - accuracy: 0.4545\n",
            "Epoch 21/200\n",
            "1/1 - 0s - loss: 2.0961 - accuracy: 0.4545\n",
            "Epoch 22/200\n",
            "1/1 - 0s - loss: 2.0707 - accuracy: 0.3636\n",
            "Epoch 23/200\n",
            "1/1 - 0s - loss: 2.0453 - accuracy: 0.3636\n",
            "Epoch 24/200\n",
            "1/1 - 0s - loss: 2.0202 - accuracy: 0.3636\n",
            "Epoch 25/200\n",
            "1/1 - 0s - loss: 1.9956 - accuracy: 0.3636\n",
            "Epoch 26/200\n",
            "1/1 - 0s - loss: 1.9716 - accuracy: 0.3636\n",
            "Epoch 27/200\n",
            "1/1 - 0s - loss: 1.9485 - accuracy: 0.3636\n",
            "Epoch 28/200\n",
            "1/1 - 0s - loss: 1.9262 - accuracy: 0.3636\n",
            "Epoch 29/200\n",
            "1/1 - 0s - loss: 1.9051 - accuracy: 0.3636\n",
            "Epoch 30/200\n",
            "1/1 - 0s - loss: 1.8851 - accuracy: 0.3636\n",
            "Epoch 31/200\n",
            "1/1 - 0s - loss: 1.8664 - accuracy: 0.3636\n",
            "Epoch 32/200\n",
            "1/1 - 0s - loss: 1.8489 - accuracy: 0.3636\n",
            "Epoch 33/200\n",
            "1/1 - 0s - loss: 1.8325 - accuracy: 0.3636\n",
            "Epoch 34/200\n",
            "1/1 - 0s - loss: 1.8172 - accuracy: 0.3636\n",
            "Epoch 35/200\n",
            "1/1 - 0s - loss: 1.8028 - accuracy: 0.3636\n",
            "Epoch 36/200\n",
            "1/1 - 0s - loss: 1.7891 - accuracy: 0.3636\n",
            "Epoch 37/200\n",
            "1/1 - 0s - loss: 1.7759 - accuracy: 0.3636\n",
            "Epoch 38/200\n",
            "1/1 - 0s - loss: 1.7629 - accuracy: 0.3636\n",
            "Epoch 39/200\n",
            "1/1 - 0s - loss: 1.7499 - accuracy: 0.3636\n",
            "Epoch 40/200\n",
            "1/1 - 0s - loss: 1.7367 - accuracy: 0.3636\n",
            "Epoch 41/200\n",
            "1/1 - 0s - loss: 1.7232 - accuracy: 0.3636\n",
            "Epoch 42/200\n",
            "1/1 - 0s - loss: 1.7093 - accuracy: 0.3636\n",
            "Epoch 43/200\n",
            "1/1 - 0s - loss: 1.6949 - accuracy: 0.3636\n",
            "Epoch 44/200\n",
            "1/1 - 0s - loss: 1.6799 - accuracy: 0.3636\n",
            "Epoch 45/200\n",
            "1/1 - 0s - loss: 1.6645 - accuracy: 0.3636\n",
            "Epoch 46/200\n",
            "1/1 - 0s - loss: 1.6486 - accuracy: 0.3636\n",
            "Epoch 47/200\n",
            "1/1 - 0s - loss: 1.6323 - accuracy: 0.3636\n",
            "Epoch 48/200\n",
            "1/1 - 0s - loss: 1.6156 - accuracy: 0.3636\n",
            "Epoch 49/200\n",
            "1/1 - 0s - loss: 1.5984 - accuracy: 0.3636\n",
            "Epoch 50/200\n",
            "1/1 - 0s - loss: 1.5808 - accuracy: 0.4545\n",
            "Epoch 51/200\n",
            "1/1 - 0s - loss: 1.5628 - accuracy: 0.4545\n",
            "Epoch 52/200\n",
            "1/1 - 0s - loss: 1.5443 - accuracy: 0.5455\n",
            "Epoch 53/200\n",
            "1/1 - 0s - loss: 1.5253 - accuracy: 0.5455\n",
            "Epoch 54/200\n",
            "1/1 - 0s - loss: 1.5057 - accuracy: 0.5455\n",
            "Epoch 55/200\n",
            "1/1 - 0s - loss: 1.4857 - accuracy: 0.5455\n",
            "Epoch 56/200\n",
            "1/1 - 0s - loss: 1.4651 - accuracy: 0.5455\n",
            "Epoch 57/200\n",
            "1/1 - 0s - loss: 1.4440 - accuracy: 0.5455\n",
            "Epoch 58/200\n",
            "1/1 - 0s - loss: 1.4225 - accuracy: 0.5455\n",
            "Epoch 59/200\n",
            "1/1 - 0s - loss: 1.4006 - accuracy: 0.5455\n",
            "Epoch 60/200\n",
            "1/1 - 0s - loss: 1.3783 - accuracy: 0.5455\n",
            "Epoch 61/200\n",
            "1/1 - 0s - loss: 1.3558 - accuracy: 0.5455\n",
            "Epoch 62/200\n",
            "1/1 - 0s - loss: 1.3330 - accuracy: 0.5455\n",
            "Epoch 63/200\n",
            "1/1 - 0s - loss: 1.3102 - accuracy: 0.5455\n",
            "Epoch 64/200\n",
            "1/1 - 0s - loss: 1.2874 - accuracy: 0.6364\n",
            "Epoch 65/200\n",
            "1/1 - 0s - loss: 1.2645 - accuracy: 0.6364\n",
            "Epoch 66/200\n",
            "1/1 - 0s - loss: 1.2418 - accuracy: 0.6364\n",
            "Epoch 67/200\n",
            "1/1 - 0s - loss: 1.2193 - accuracy: 0.6364\n",
            "Epoch 68/200\n",
            "1/1 - 0s - loss: 1.1970 - accuracy: 0.6364\n",
            "Epoch 69/200\n",
            "1/1 - 0s - loss: 1.1750 - accuracy: 0.6364\n",
            "Epoch 70/200\n",
            "1/1 - 0s - loss: 1.1533 - accuracy: 0.6364\n",
            "Epoch 71/200\n",
            "1/1 - 0s - loss: 1.1319 - accuracy: 0.6364\n",
            "Epoch 72/200\n",
            "1/1 - 0s - loss: 1.1110 - accuracy: 0.6364\n",
            "Epoch 73/200\n",
            "1/1 - 0s - loss: 1.0905 - accuracy: 0.6364\n",
            "Epoch 74/200\n",
            "1/1 - 0s - loss: 1.0704 - accuracy: 0.6364\n",
            "Epoch 75/200\n",
            "1/1 - 0s - loss: 1.0508 - accuracy: 0.6364\n",
            "Epoch 76/200\n",
            "1/1 - 0s - loss: 1.0317 - accuracy: 0.6364\n",
            "Epoch 77/200\n",
            "1/1 - 0s - loss: 1.0130 - accuracy: 0.6364\n",
            "Epoch 78/200\n",
            "1/1 - 0s - loss: 0.9948 - accuracy: 0.6364\n",
            "Epoch 79/200\n",
            "1/1 - 0s - loss: 0.9771 - accuracy: 0.6364\n",
            "Epoch 80/200\n",
            "1/1 - 0s - loss: 0.9599 - accuracy: 0.6364\n",
            "Epoch 81/200\n",
            "1/1 - 0s - loss: 0.9431 - accuracy: 0.6364\n",
            "Epoch 82/200\n",
            "1/1 - 0s - loss: 0.9267 - accuracy: 0.6364\n",
            "Epoch 83/200\n",
            "1/1 - 0s - loss: 0.9108 - accuracy: 0.6364\n",
            "Epoch 84/200\n",
            "1/1 - 0s - loss: 0.8953 - accuracy: 0.7273\n",
            "Epoch 85/200\n",
            "1/1 - 0s - loss: 0.8803 - accuracy: 0.7273\n",
            "Epoch 86/200\n",
            "1/1 - 0s - loss: 0.8656 - accuracy: 0.7273\n",
            "Epoch 87/200\n",
            "1/1 - 0s - loss: 0.8513 - accuracy: 0.7273\n",
            "Epoch 88/200\n",
            "1/1 - 0s - loss: 0.8373 - accuracy: 0.7273\n",
            "Epoch 89/200\n",
            "1/1 - 0s - loss: 0.8236 - accuracy: 0.7273\n",
            "Epoch 90/200\n",
            "1/1 - 0s - loss: 0.8103 - accuracy: 0.7273\n",
            "Epoch 91/200\n",
            "1/1 - 0s - loss: 0.7973 - accuracy: 0.7273\n",
            "Epoch 92/200\n",
            "1/1 - 0s - loss: 0.7845 - accuracy: 0.7273\n",
            "Epoch 93/200\n",
            "1/1 - 0s - loss: 0.7720 - accuracy: 0.7273\n",
            "Epoch 94/200\n",
            "1/1 - 0s - loss: 0.7598 - accuracy: 0.8182\n",
            "Epoch 95/200\n",
            "1/1 - 0s - loss: 0.7478 - accuracy: 0.8182\n",
            "Epoch 96/200\n",
            "1/1 - 0s - loss: 0.7360 - accuracy: 0.8182\n",
            "Epoch 97/200\n",
            "1/1 - 0s - loss: 0.7244 - accuracy: 0.8182\n",
            "Epoch 98/200\n",
            "1/1 - 0s - loss: 0.7130 - accuracy: 0.9091\n",
            "Epoch 99/200\n",
            "1/1 - 0s - loss: 0.7018 - accuracy: 0.9091\n",
            "Epoch 100/200\n",
            "1/1 - 0s - loss: 0.6908 - accuracy: 0.9091\n",
            "Epoch 101/200\n",
            "1/1 - 0s - loss: 0.6799 - accuracy: 0.9091\n",
            "Epoch 102/200\n",
            "1/1 - 0s - loss: 0.6692 - accuracy: 0.9091\n",
            "Epoch 103/200\n",
            "1/1 - 0s - loss: 0.6587 - accuracy: 0.9091\n",
            "Epoch 104/200\n",
            "1/1 - 0s - loss: 0.6483 - accuracy: 0.9091\n",
            "Epoch 105/200\n",
            "1/1 - 0s - loss: 0.6381 - accuracy: 0.9091\n",
            "Epoch 106/200\n",
            "1/1 - 0s - loss: 0.6280 - accuracy: 0.9091\n",
            "Epoch 107/200\n",
            "1/1 - 0s - loss: 0.6181 - accuracy: 0.9091\n",
            "Epoch 108/200\n",
            "1/1 - 0s - loss: 0.6084 - accuracy: 0.9091\n",
            "Epoch 109/200\n",
            "1/1 - 0s - loss: 0.5988 - accuracy: 0.9091\n",
            "Epoch 110/200\n",
            "1/1 - 0s - loss: 0.5894 - accuracy: 0.9091\n",
            "Epoch 111/200\n",
            "1/1 - 0s - loss: 0.5801 - accuracy: 0.9091\n",
            "Epoch 112/200\n",
            "1/1 - 0s - loss: 0.5710 - accuracy: 0.9091\n",
            "Epoch 113/200\n",
            "1/1 - 0s - loss: 0.5621 - accuracy: 0.9091\n",
            "Epoch 114/200\n",
            "1/1 - 0s - loss: 0.5533 - accuracy: 0.9091\n",
            "Epoch 115/200\n",
            "1/1 - 0s - loss: 0.5446 - accuracy: 0.9091\n",
            "Epoch 116/200\n",
            "1/1 - 0s - loss: 0.5362 - accuracy: 0.9091\n",
            "Epoch 117/200\n",
            "1/1 - 0s - loss: 0.5278 - accuracy: 0.9091\n",
            "Epoch 118/200\n",
            "1/1 - 0s - loss: 0.5197 - accuracy: 0.9091\n",
            "Epoch 119/200\n",
            "1/1 - 0s - loss: 0.5116 - accuracy: 0.9091\n",
            "Epoch 120/200\n",
            "1/1 - 0s - loss: 0.5037 - accuracy: 0.9091\n",
            "Epoch 121/200\n",
            "1/1 - 0s - loss: 0.4960 - accuracy: 0.9091\n",
            "Epoch 122/200\n",
            "1/1 - 0s - loss: 0.4884 - accuracy: 0.9091\n",
            "Epoch 123/200\n",
            "1/1 - 0s - loss: 0.4809 - accuracy: 0.9091\n",
            "Epoch 124/200\n",
            "1/1 - 0s - loss: 0.4736 - accuracy: 0.9091\n",
            "Epoch 125/200\n",
            "1/1 - 0s - loss: 0.4664 - accuracy: 0.9091\n",
            "Epoch 126/200\n",
            "1/1 - 0s - loss: 0.4593 - accuracy: 0.9091\n",
            "Epoch 127/200\n",
            "1/1 - 0s - loss: 0.4524 - accuracy: 0.9091\n",
            "Epoch 128/200\n",
            "1/1 - 0s - loss: 0.4456 - accuracy: 0.9091\n",
            "Epoch 129/200\n",
            "1/1 - 0s - loss: 0.4389 - accuracy: 0.9091\n",
            "Epoch 130/200\n",
            "1/1 - 0s - loss: 0.4323 - accuracy: 0.9091\n",
            "Epoch 131/200\n",
            "1/1 - 0s - loss: 0.4258 - accuracy: 0.9091\n",
            "Epoch 132/200\n",
            "1/1 - 0s - loss: 0.4195 - accuracy: 0.9091\n",
            "Epoch 133/200\n",
            "1/1 - 0s - loss: 0.4133 - accuracy: 0.9091\n",
            "Epoch 134/200\n",
            "1/1 - 0s - loss: 0.4071 - accuracy: 0.9091\n",
            "Epoch 135/200\n",
            "1/1 - 0s - loss: 0.4011 - accuracy: 0.9091\n",
            "Epoch 136/200\n",
            "1/1 - 0s - loss: 0.3952 - accuracy: 0.9091\n",
            "Epoch 137/200\n",
            "1/1 - 0s - loss: 0.3893 - accuracy: 0.9091\n",
            "Epoch 138/200\n",
            "1/1 - 0s - loss: 0.3836 - accuracy: 0.9091\n",
            "Epoch 139/200\n",
            "1/1 - 0s - loss: 0.3780 - accuracy: 0.9091\n",
            "Epoch 140/200\n",
            "1/1 - 0s - loss: 0.3724 - accuracy: 0.9091\n",
            "Epoch 141/200\n",
            "1/1 - 0s - loss: 0.3670 - accuracy: 0.9091\n",
            "Epoch 142/200\n",
            "1/1 - 0s - loss: 0.3616 - accuracy: 0.9091\n",
            "Epoch 143/200\n",
            "1/1 - 0s - loss: 0.3563 - accuracy: 0.9091\n",
            "Epoch 144/200\n",
            "1/1 - 0s - loss: 0.3511 - accuracy: 0.9091\n",
            "Epoch 145/200\n",
            "1/1 - 0s - loss: 0.3460 - accuracy: 0.9091\n",
            "Epoch 146/200\n",
            "1/1 - 0s - loss: 0.3409 - accuracy: 0.9091\n",
            "Epoch 147/200\n",
            "1/1 - 0s - loss: 0.3360 - accuracy: 0.9091\n",
            "Epoch 148/200\n",
            "1/1 - 0s - loss: 0.3310 - accuracy: 0.9091\n",
            "Epoch 149/200\n",
            "1/1 - 0s - loss: 0.3262 - accuracy: 0.9091\n",
            "Epoch 150/200\n",
            "1/1 - 0s - loss: 0.3214 - accuracy: 0.9091\n",
            "Epoch 151/200\n",
            "1/1 - 0s - loss: 0.3167 - accuracy: 1.0000\n",
            "Epoch 152/200\n",
            "1/1 - 0s - loss: 0.3121 - accuracy: 1.0000\n",
            "Epoch 153/200\n",
            "1/1 - 0s - loss: 0.3075 - accuracy: 1.0000\n",
            "Epoch 154/200\n",
            "1/1 - 0s - loss: 0.3029 - accuracy: 1.0000\n",
            "Epoch 155/200\n",
            "1/1 - 0s - loss: 0.2985 - accuracy: 1.0000\n",
            "Epoch 156/200\n",
            "1/1 - 0s - loss: 0.2941 - accuracy: 1.0000\n",
            "Epoch 157/200\n",
            "1/1 - 0s - loss: 0.2897 - accuracy: 1.0000\n",
            "Epoch 158/200\n",
            "1/1 - 0s - loss: 0.2854 - accuracy: 1.0000\n",
            "Epoch 159/200\n",
            "1/1 - 0s - loss: 0.2811 - accuracy: 1.0000\n",
            "Epoch 160/200\n",
            "1/1 - 0s - loss: 0.2769 - accuracy: 1.0000\n",
            "Epoch 161/200\n",
            "1/1 - 0s - loss: 0.2728 - accuracy: 1.0000\n",
            "Epoch 162/200\n",
            "1/1 - 0s - loss: 0.2687 - accuracy: 1.0000\n",
            "Epoch 163/200\n",
            "1/1 - 0s - loss: 0.2646 - accuracy: 1.0000\n",
            "Epoch 164/200\n",
            "1/1 - 0s - loss: 0.2606 - accuracy: 1.0000\n",
            "Epoch 165/200\n",
            "1/1 - 0s - loss: 0.2566 - accuracy: 1.0000\n",
            "Epoch 166/200\n",
            "1/1 - 0s - loss: 0.2527 - accuracy: 1.0000\n",
            "Epoch 167/200\n",
            "1/1 - 0s - loss: 0.2489 - accuracy: 1.0000\n",
            "Epoch 168/200\n",
            "1/1 - 0s - loss: 0.2450 - accuracy: 1.0000\n",
            "Epoch 169/200\n",
            "1/1 - 0s - loss: 0.2413 - accuracy: 1.0000\n",
            "Epoch 170/200\n",
            "1/1 - 0s - loss: 0.2375 - accuracy: 1.0000\n",
            "Epoch 171/200\n",
            "1/1 - 0s - loss: 0.2339 - accuracy: 1.0000\n",
            "Epoch 172/200\n",
            "1/1 - 0s - loss: 0.2302 - accuracy: 1.0000\n",
            "Epoch 173/200\n",
            "1/1 - 0s - loss: 0.2266 - accuracy: 1.0000\n",
            "Epoch 174/200\n",
            "1/1 - 0s - loss: 0.2231 - accuracy: 1.0000\n",
            "Epoch 175/200\n",
            "1/1 - 0s - loss: 0.2196 - accuracy: 1.0000\n",
            "Epoch 176/200\n",
            "1/1 - 0s - loss: 0.2161 - accuracy: 1.0000\n",
            "Epoch 177/200\n",
            "1/1 - 0s - loss: 0.2127 - accuracy: 1.0000\n",
            "Epoch 178/200\n",
            "1/1 - 0s - loss: 0.2094 - accuracy: 1.0000\n",
            "Epoch 179/200\n",
            "1/1 - 0s - loss: 0.2060 - accuracy: 1.0000\n",
            "Epoch 180/200\n",
            "1/1 - 0s - loss: 0.2028 - accuracy: 1.0000\n",
            "Epoch 181/200\n",
            "1/1 - 0s - loss: 0.1996 - accuracy: 1.0000\n",
            "Epoch 182/200\n",
            "1/1 - 0s - loss: 0.1964 - accuracy: 1.0000\n",
            "Epoch 183/200\n",
            "1/1 - 0s - loss: 0.1932 - accuracy: 1.0000\n",
            "Epoch 184/200\n",
            "1/1 - 0s - loss: 0.1902 - accuracy: 1.0000\n",
            "Epoch 185/200\n",
            "1/1 - 0s - loss: 0.1871 - accuracy: 1.0000\n",
            "Epoch 186/200\n",
            "1/1 - 0s - loss: 0.1841 - accuracy: 1.0000\n",
            "Epoch 187/200\n",
            "1/1 - 0s - loss: 0.1812 - accuracy: 1.0000\n",
            "Epoch 188/200\n",
            "1/1 - 0s - loss: 0.1783 - accuracy: 1.0000\n",
            "Epoch 189/200\n",
            "1/1 - 0s - loss: 0.1754 - accuracy: 1.0000\n",
            "Epoch 190/200\n",
            "1/1 - 0s - loss: 0.1726 - accuracy: 1.0000\n",
            "Epoch 191/200\n",
            "1/1 - 0s - loss: 0.1698 - accuracy: 1.0000\n",
            "Epoch 192/200\n",
            "1/1 - 0s - loss: 0.1671 - accuracy: 1.0000\n",
            "Epoch 193/200\n",
            "1/1 - 0s - loss: 0.1644 - accuracy: 1.0000\n",
            "Epoch 194/200\n",
            "1/1 - 0s - loss: 0.1618 - accuracy: 1.0000\n",
            "Epoch 195/200\n",
            "1/1 - 0s - loss: 0.1592 - accuracy: 1.0000\n",
            "Epoch 196/200\n",
            "1/1 - 0s - loss: 0.1567 - accuracy: 1.0000\n",
            "Epoch 197/200\n",
            "1/1 - 0s - loss: 0.1542 - accuracy: 1.0000\n",
            "Epoch 198/200\n",
            "1/1 - 0s - loss: 0.1517 - accuracy: 1.0000\n",
            "Epoch 199/200\n",
            "1/1 - 0s - loss: 0.1493 - accuracy: 1.0000\n",
            "Epoch 200/200\n",
            "1/1 - 0s - loss: 0.1469 - accuracy: 1.0000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f4373a22e50>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SYg0u4aMT6UT"
      },
      "source": [
        "# 모델이 정확하게 예측하고 있는지 문장을 생성하는 함수를 만들어서 출력\n",
        "def sentence_generation(model, tokenizer, current_word, n): # 모델, 토크나이저, 현재 단어, 반복할 횟수\n",
        "    init_word = current_word\n",
        "    sentence = ''\n",
        "\n",
        "    # n번 반복\n",
        "    for _ in range(n):\n",
        "        # 현재 단어에 대한 정수 인코딩과 패딩\n",
        "        encoded = tokenizer.texts_to_sequences([current_word])[0]\n",
        "        encoded = pad_sequences([encoded], maxlen=5, padding='pre')\n",
        "        # 입력한 X(현재 단어)에 대해서 Y를 예측하고 Y(예측한 단어)를 result에 저장.\n",
        "        result = model.predict(encoded, verbose=0)\n",
        "        result = np.argmax(result, axis=1)\n",
        "\n",
        "        for word, index in tokenizer.word_index.items(): \n",
        "            # 만약 예측한 단어와 인덱스와 동일한 단어가 있다면 break\n",
        "            if index == result:\n",
        "                break\n",
        "\n",
        "        # 현재 단어 + ' ' + 예측 단어를 현재 단어로 변경\n",
        "        current_word = current_word + ' '  + word\n",
        "\n",
        "        # 예측 단어를 문장에 저장\n",
        "        sentence = sentence + ' ' + word\n",
        "\n",
        "    sentence = init_word + sentence\n",
        "    return sentence"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iYJExycKUAl4",
        "outputId": "b22b360b-8bce-4a7e-abcb-94eacae381e1"
      },
      "source": [
        "# 입력된 단어로부터 다음 단어를 예측해서 문장을 생성하는 함수\n",
        "print(sentence_generation(model, tokenizer, '경마장에', 4))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "경마장에 있는 말이 뛰고 있다\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AqYeClv6UGYd",
        "outputId": "2427fef0-7949-4cd5-e64d-6853d5cd852a"
      },
      "source": [
        "print(sentence_generation(model, tokenizer, '그의', 2))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "그의 말이 법이다\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DEhbFQhVUH06",
        "outputId": "d2b819e1-14f7-4e09-e538-ab520e514a66"
      },
      "source": [
        "print(sentence_generation(model, tokenizer, '가는', 5))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "가는 말이 고와야 오는 말이 곱다\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8y25S5MLUL18"
      },
      "source": [
        "* 앞의 문맥을 기준으로 '말이'라는 단어 다음에 나올 단어를 기존의 훈련 데이터와 일치하게 예측함을 보여줌"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HeoRJL86URZ2"
      },
      "source": [
        "#### LSTM으로 텍스트 생성\n",
        "1. 데이터에 대한 이해와 전처리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QeSR45D-WA-1"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from string import punctuation\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "6bu5hpi9XFaL",
        "outputId": "53caca5d-eb1f-4310-9627-6da06bd6548f"
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-9ae30c5a-7c7c-4fea-837d-acf90288d3af\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-9ae30c5a-7c7c-4fea-837d-acf90288d3af\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving ArticlesApril2017.csv to ArticlesApril2017.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 609
        },
        "id": "_0xWoeRsUJ8Z",
        "outputId": "65af05f1-ec84-4f1c-f8ec-41848a5dd774"
      },
      "source": [
        "df = pd.read_csv('/content/ArticlesApril2017.csv')\n",
        "df.head()"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>abstract</th>\n",
              "      <th>articleID</th>\n",
              "      <th>articleWordCount</th>\n",
              "      <th>byline</th>\n",
              "      <th>documentType</th>\n",
              "      <th>headline</th>\n",
              "      <th>keywords</th>\n",
              "      <th>multimedia</th>\n",
              "      <th>newDesk</th>\n",
              "      <th>printPage</th>\n",
              "      <th>pubDate</th>\n",
              "      <th>sectionName</th>\n",
              "      <th>snippet</th>\n",
              "      <th>source</th>\n",
              "      <th>typeOfMaterial</th>\n",
              "      <th>webURL</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>58def1347c459f24986d7c80</td>\n",
              "      <td>716</td>\n",
              "      <td>By STEPHEN HILTNER and SUSAN LEHMAN</td>\n",
              "      <td>article</td>\n",
              "      <td>Finding an Expansive View  of a Forgotten Peop...</td>\n",
              "      <td>['Photography', 'New York Times', 'Niger', 'Fe...</td>\n",
              "      <td>3</td>\n",
              "      <td>Insider</td>\n",
              "      <td>2</td>\n",
              "      <td>2017-04-01 00:15:41</td>\n",
              "      <td>Unknown</td>\n",
              "      <td>One of the largest photo displays in Times his...</td>\n",
              "      <td>The New York Times</td>\n",
              "      <td>News</td>\n",
              "      <td>https://www.nytimes.com/2017/03/31/insider/nig...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>NaN</td>\n",
              "      <td>58def3237c459f24986d7c84</td>\n",
              "      <td>823</td>\n",
              "      <td>By GAIL COLLINS</td>\n",
              "      <td>article</td>\n",
              "      <td>And Now,  the Dreaded Trump Curse</td>\n",
              "      <td>['United States Politics and Government', 'Tru...</td>\n",
              "      <td>3</td>\n",
              "      <td>OpEd</td>\n",
              "      <td>23</td>\n",
              "      <td>2017-04-01 00:23:58</td>\n",
              "      <td>Unknown</td>\n",
              "      <td>Meet the gang from under the bus.</td>\n",
              "      <td>The New York Times</td>\n",
              "      <td>Op-Ed</td>\n",
              "      <td>https://www.nytimes.com/2017/03/31/opinion/and...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>NaN</td>\n",
              "      <td>58def9f57c459f24986d7c90</td>\n",
              "      <td>575</td>\n",
              "      <td>By THE EDITORIAL BOARD</td>\n",
              "      <td>article</td>\n",
              "      <td>Venezuela’s Descent Into Dictatorship</td>\n",
              "      <td>['Venezuela', 'Politics and Government', 'Madu...</td>\n",
              "      <td>3</td>\n",
              "      <td>Editorial</td>\n",
              "      <td>22</td>\n",
              "      <td>2017-04-01 00:53:06</td>\n",
              "      <td>Unknown</td>\n",
              "      <td>A court ruling annulling the legislature’s aut...</td>\n",
              "      <td>The New York Times</td>\n",
              "      <td>Editorial</td>\n",
              "      <td>https://www.nytimes.com/2017/03/31/opinion/ven...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>NaN</td>\n",
              "      <td>58defd317c459f24986d7c95</td>\n",
              "      <td>1374</td>\n",
              "      <td>By MICHAEL POWELL</td>\n",
              "      <td>article</td>\n",
              "      <td>Stain Permeates Basketball Blue Blood</td>\n",
              "      <td>['Basketball (College)', 'University of North ...</td>\n",
              "      <td>3</td>\n",
              "      <td>Sports</td>\n",
              "      <td>1</td>\n",
              "      <td>2017-04-01 01:06:52</td>\n",
              "      <td>College Basketball</td>\n",
              "      <td>For two decades, until 2013, North Carolina en...</td>\n",
              "      <td>The New York Times</td>\n",
              "      <td>News</td>\n",
              "      <td>https://www.nytimes.com/2017/03/31/sports/ncaa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>NaN</td>\n",
              "      <td>58df09b77c459f24986d7ca7</td>\n",
              "      <td>708</td>\n",
              "      <td>By DEB AMLEN</td>\n",
              "      <td>article</td>\n",
              "      <td>Taking Things for Granted</td>\n",
              "      <td>['Crossword Puzzles']</td>\n",
              "      <td>3</td>\n",
              "      <td>Games</td>\n",
              "      <td>0</td>\n",
              "      <td>2017-04-01 02:00:14</td>\n",
              "      <td>Unknown</td>\n",
              "      <td>In which Howard Barkin and Will Shortz teach u...</td>\n",
              "      <td>The New York Times</td>\n",
              "      <td>News</td>\n",
              "      <td>https://www.nytimes.com/2017/03/31/crosswords/...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  abstract  ...                                             webURL\n",
              "0      NaN  ...  https://www.nytimes.com/2017/03/31/insider/nig...\n",
              "1      NaN  ...  https://www.nytimes.com/2017/03/31/opinion/and...\n",
              "2      NaN  ...  https://www.nytimes.com/2017/03/31/opinion/ven...\n",
              "3      NaN  ...  https://www.nytimes.com/2017/03/31/sports/ncaa...\n",
              "4      NaN  ...  https://www.nytimes.com/2017/03/31/crosswords/...\n",
              "\n",
              "[5 rows x 16 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "znzEv9tVWDE0",
        "outputId": "38c6413d-7438-4c24-bb10-e4ee8a59265b"
      },
      "source": [
        "print('열의 개수: ',len(df.columns))\n",
        "print(df.columns)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "열의 개수:  16\n",
            "Index(['abstract', 'articleID', 'articleWordCount', 'byline', 'documentType',\n",
            "       'headline', 'keywords', 'multimedia', 'newDesk', 'printPage', 'pubDate',\n",
            "       'sectionName', 'snippet', 'source', 'typeOfMaterial', 'webURL'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BVCBcGL5XRFl",
        "outputId": "9f564be7-05fc-49f1-a71e-9293fc1b61ac"
      },
      "source": [
        "# Null값 있는지 확인\n",
        "print(df['headline'].isnull().values.any())"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jXSDcDEEXWLS",
        "outputId": "b53ddb32-d1e1-4deb-dc91-807267ece2ab"
      },
      "source": [
        "headline = []\n",
        "# 헤드라인의 값들을 리스트로 저장\n",
        "headline.extend(list(df.headline.values)) \n",
        "headline[:5]"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Finding an Expansive View  of a Forgotten People in Niger',\n",
              " 'And Now,  the Dreaded Trump Curse',\n",
              " 'Venezuela’s Descent Into Dictatorship',\n",
              " 'Stain Permeates Basketball Blue Blood',\n",
              " 'Taking Things for Granted']"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0q-8areuXdME",
        "outputId": "92152567-1535-4f7c-c026-3458e3b52b3d"
      },
      "source": [
        "# 노이즈 제거\n",
        "print('총 샘플의 개수 : {}'.format(len(headline)))\n",
        "\n",
        "headline = [word for word in headline if word != \"Unknown\"]\n",
        "print('노이즈값 제거 후 샘플의 개수 : {}'.format(len(headline)))"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "총 샘플의 개수 : 886\n",
            "노이즈값 제거 후 샘플의 개수 : 831\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DfKpIEWmXlaS",
        "outputId": "011a3e11-2816-411c-acbb-fd0f1322cd75"
      },
      "source": [
        "headline[:5]"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Finding an Expansive View  of a Forgotten People in Niger',\n",
              " 'And Now,  the Dreaded Trump Curse',\n",
              " 'Venezuela’s Descent Into Dictatorship',\n",
              " 'Stain Permeates Basketball Blue Blood',\n",
              " 'Taking Things for Granted']"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oqoHyf6jXoBF",
        "outputId": "c85ebb2e-f6b3-409e-cbf8-b450197785de"
      },
      "source": [
        "# 데이터 전처리\n",
        "def repreprocessing(raw_sentence):\n",
        "    preproceseed_sentence = raw_sentence.encode(\"utf8\").decode(\"ascii\",'ignore')\n",
        "    # 구두점 제거와 동시에 소문자화\n",
        "    return ''.join(word for word in preproceseed_sentence if word not in punctuation).lower()\n",
        "\n",
        "preporcessed_headline = [repreprocessing(x) for x in headline]\n",
        "preporcessed_headline[:5]"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['finding an expansive view  of a forgotten people in niger',\n",
              " 'and now  the dreaded trump curse',\n",
              " 'venezuelas descent into dictatorship',\n",
              " 'stain permeates basketball blue blood',\n",
              " 'taking things for granted']"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gXsiF3uhXsIo",
        "outputId": "d29e0691-1ac5-49ed-a9e3-b32c38141054"
      },
      "source": [
        "# 단어 집합 만들고 크기 확인\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(preporcessed_headline)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print('단어 집합의 크기 : %d' % vocab_size)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "단어 집합의 크기 : 2422\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XGGK4TjiXwyC",
        "outputId": "f3630ad2-9a7c-4809-e480-fcaa2c8e5709"
      },
      "source": [
        "# 정수 인코딩과 하나의 문장을 분해하여 훈련 데이터 구성\n",
        "sequences = list()\n",
        "\n",
        "for sentence in preporcessed_headline:\n",
        "\n",
        "    # 각 샘플에 대한 정수 인코딩\n",
        "    encoded = tokenizer.texts_to_sequences([sentence])[0] \n",
        "    for i in range(1, len(encoded)):\n",
        "        sequence = encoded[:i+1]\n",
        "        sequences.append(sequence)\n",
        "\n",
        "sequences[:11]"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[169, 17],\n",
              " [169, 17, 665],\n",
              " [169, 17, 665, 367],\n",
              " [169, 17, 665, 367, 4],\n",
              " [169, 17, 665, 367, 4, 2],\n",
              " [169, 17, 665, 367, 4, 2, 666],\n",
              " [169, 17, 665, 367, 4, 2, 666, 170],\n",
              " [169, 17, 665, 367, 4, 2, 666, 170, 5],\n",
              " [169, 17, 665, 367, 4, 2, 666, 170, 5, 667],\n",
              " [6, 80],\n",
              " [6, 80, 1]]"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HW94-zkjX2Cy",
        "outputId": "16d9bd76-0a8e-490f-dd04-1dfb76496243"
      },
      "source": [
        "# 어떤 정수가 어떤 단어를 의미하는지 알아보기 위해 인덱스로부터 단어를 찾는 index_to_word 만들기\n",
        "index_to_word = {}\n",
        "for key, value in tokenizer.word_index.items(): # 인덱스를 단어로 바꾸기 위해 index_to_word를 생성\n",
        "    index_to_word[value] = key\n",
        "\n",
        "print('빈도수 상위 582번 단어 : {}'.format(index_to_word[582]))"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "빈도수 상위 582번 단어 : exist\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rg26DTD4X-HH",
        "outputId": "1e305f50-82d5-4cf9-c2bb-d11af4eb01d2"
      },
      "source": [
        "# 샘플 길이 패딩\n",
        "max_len = max(len(l) for l in sequences)\n",
        "print('샘플의 최대 길이 : {}'.format(max_len))\n",
        "\n",
        "sequences = pad_sequences(sequences, maxlen=max_len, padding='pre')\n",
        "print(sequences[:3])"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "샘플의 최대 길이 : 19\n",
            "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 169\n",
            "   17]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 169  17\n",
            "  665]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 169  17 665\n",
            "  367]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JuFp8go2YE3g",
        "outputId": "fa9c61bf-4297-4531-b883-499d1b4e43c3"
      },
      "source": [
        "# 맨 우측 단어로만 레이블로 분리\n",
        "sequences = np.array(sequences)\n",
        "X = sequences[:,:-1]\n",
        "y = sequences[:,-1]\n",
        "\n",
        "print(X[:3])"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 169]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 169  17]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 169  17 665]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7TB80sh-YLMv",
        "outputId": "b6dc9393-88cf-4a7b-c9c7-9e97481d322a"
      },
      "source": [
        "print(y[:3])"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 17 665 367]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RG2o9d6AYPJN"
      },
      "source": [
        "# 레이블 데이터 y에 대해서 원-핫 인코딩 수ㅐㅇ\n",
        "y = to_categorical(y, num_classes=vocab_size)"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPOuzTrqYUIH"
      },
      "source": [
        "2. 모델 설계하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmD7rRMPYSS3"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Dense, LSTM"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3JV8oSzKYXFR",
        "outputId": "18252cf4-72b2-462b-dc3a-07b43976068c"
      },
      "source": [
        "embedding_dim = 10\n",
        "hidden_units = 128\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, embedding_dim))\n",
        "model.add(LSTM(hidden_units))\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(X, y, epochs=200, verbose=2)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "151/151 - 5s - loss: 7.4316 - accuracy: 0.0279\n",
            "Epoch 2/200\n",
            "151/151 - 4s - loss: 6.9209 - accuracy: 0.0323\n",
            "Epoch 3/200\n",
            "151/151 - 4s - loss: 6.7968 - accuracy: 0.0318\n",
            "Epoch 4/200\n",
            "151/151 - 4s - loss: 6.7105 - accuracy: 0.0343\n",
            "Epoch 5/200\n",
            "151/151 - 3s - loss: 6.6251 - accuracy: 0.0422\n",
            "Epoch 6/200\n",
            "151/151 - 3s - loss: 6.5215 - accuracy: 0.0470\n",
            "Epoch 7/200\n",
            "151/151 - 3s - loss: 6.3951 - accuracy: 0.0497\n",
            "Epoch 8/200\n",
            "151/151 - 4s - loss: 6.2533 - accuracy: 0.0497\n",
            "Epoch 9/200\n",
            "151/151 - 4s - loss: 6.1000 - accuracy: 0.0516\n",
            "Epoch 10/200\n",
            "151/151 - 4s - loss: 5.9394 - accuracy: 0.0556\n",
            "Epoch 11/200\n",
            "151/151 - 3s - loss: 5.7809 - accuracy: 0.0583\n",
            "Epoch 12/200\n",
            "151/151 - 3s - loss: 5.6288 - accuracy: 0.0676\n",
            "Epoch 13/200\n",
            "151/151 - 3s - loss: 5.4870 - accuracy: 0.0680\n",
            "Epoch 14/200\n",
            "151/151 - 4s - loss: 5.3530 - accuracy: 0.0699\n",
            "Epoch 15/200\n",
            "151/151 - 3s - loss: 5.2256 - accuracy: 0.0747\n",
            "Epoch 16/200\n",
            "151/151 - 3s - loss: 5.1029 - accuracy: 0.0789\n",
            "Epoch 17/200\n",
            "151/151 - 3s - loss: 4.9818 - accuracy: 0.0874\n",
            "Epoch 18/200\n",
            "151/151 - 3s - loss: 4.8688 - accuracy: 0.0924\n",
            "Epoch 19/200\n",
            "151/151 - 4s - loss: 4.7540 - accuracy: 0.0984\n",
            "Epoch 20/200\n",
            "151/151 - 3s - loss: 4.6462 - accuracy: 0.1117\n",
            "Epoch 21/200\n",
            "151/151 - 3s - loss: 4.5372 - accuracy: 0.1265\n",
            "Epoch 22/200\n",
            "151/151 - 4s - loss: 4.4334 - accuracy: 0.1404\n",
            "Epoch 23/200\n",
            "151/151 - 4s - loss: 4.3298 - accuracy: 0.1494\n",
            "Epoch 24/200\n",
            "151/151 - 3s - loss: 4.2277 - accuracy: 0.1663\n",
            "Epoch 25/200\n",
            "151/151 - 4s - loss: 4.1285 - accuracy: 0.1902\n",
            "Epoch 26/200\n",
            "151/151 - 4s - loss: 4.0328 - accuracy: 0.2066\n",
            "Epoch 27/200\n",
            "151/151 - 4s - loss: 3.9381 - accuracy: 0.2224\n",
            "Epoch 28/200\n",
            "151/151 - 4s - loss: 3.8470 - accuracy: 0.2291\n",
            "Epoch 29/200\n",
            "151/151 - 3s - loss: 3.7558 - accuracy: 0.2499\n",
            "Epoch 30/200\n",
            "151/151 - 4s - loss: 3.6679 - accuracy: 0.2655\n",
            "Epoch 31/200\n",
            "151/151 - 4s - loss: 3.5809 - accuracy: 0.2799\n",
            "Epoch 32/200\n",
            "151/151 - 4s - loss: 3.5016 - accuracy: 0.2940\n",
            "Epoch 33/200\n",
            "151/151 - 4s - loss: 3.4180 - accuracy: 0.3079\n",
            "Epoch 34/200\n",
            "151/151 - 3s - loss: 3.3375 - accuracy: 0.3261\n",
            "Epoch 35/200\n",
            "151/151 - 4s - loss: 3.2630 - accuracy: 0.3367\n",
            "Epoch 36/200\n",
            "151/151 - 3s - loss: 3.1885 - accuracy: 0.3527\n",
            "Epoch 37/200\n",
            "151/151 - 4s - loss: 3.1185 - accuracy: 0.3631\n",
            "Epoch 38/200\n",
            "151/151 - 4s - loss: 3.0503 - accuracy: 0.3743\n",
            "Epoch 39/200\n",
            "151/151 - 4s - loss: 2.9790 - accuracy: 0.3870\n",
            "Epoch 40/200\n",
            "151/151 - 4s - loss: 2.9153 - accuracy: 0.3947\n",
            "Epoch 41/200\n",
            "151/151 - 4s - loss: 2.8512 - accuracy: 0.4087\n",
            "Epoch 42/200\n",
            "151/151 - 4s - loss: 2.7924 - accuracy: 0.4176\n",
            "Epoch 43/200\n",
            "151/151 - 4s - loss: 2.7322 - accuracy: 0.4282\n",
            "Epoch 44/200\n",
            "151/151 - 4s - loss: 2.6734 - accuracy: 0.4419\n",
            "Epoch 45/200\n",
            "151/151 - 4s - loss: 2.6185 - accuracy: 0.4528\n",
            "Epoch 46/200\n",
            "151/151 - 4s - loss: 2.5673 - accuracy: 0.4671\n",
            "Epoch 47/200\n",
            "151/151 - 4s - loss: 2.5151 - accuracy: 0.4690\n",
            "Epoch 48/200\n",
            "151/151 - 4s - loss: 2.4653 - accuracy: 0.4859\n",
            "Epoch 49/200\n",
            "151/151 - 4s - loss: 2.4156 - accuracy: 0.4852\n",
            "Epoch 50/200\n",
            "151/151 - 4s - loss: 2.3664 - accuracy: 0.5089\n",
            "Epoch 51/200\n",
            "151/151 - 4s - loss: 2.3240 - accuracy: 0.5131\n",
            "Epoch 52/200\n",
            "151/151 - 4s - loss: 2.2747 - accuracy: 0.5268\n",
            "Epoch 53/200\n",
            "151/151 - 4s - loss: 2.2322 - accuracy: 0.5339\n",
            "Epoch 54/200\n",
            "151/151 - 4s - loss: 2.1914 - accuracy: 0.5439\n",
            "Epoch 55/200\n",
            "151/151 - 4s - loss: 2.1510 - accuracy: 0.5516\n",
            "Epoch 56/200\n",
            "151/151 - 4s - loss: 2.1099 - accuracy: 0.5612\n",
            "Epoch 57/200\n",
            "151/151 - 4s - loss: 2.0691 - accuracy: 0.5726\n",
            "Epoch 58/200\n",
            "151/151 - 4s - loss: 2.0312 - accuracy: 0.5818\n",
            "Epoch 59/200\n",
            "151/151 - 4s - loss: 1.9914 - accuracy: 0.5836\n",
            "Epoch 60/200\n",
            "151/151 - 4s - loss: 1.9564 - accuracy: 0.5938\n",
            "Epoch 61/200\n",
            "151/151 - 4s - loss: 1.9207 - accuracy: 0.5968\n",
            "Epoch 62/200\n",
            "151/151 - 4s - loss: 1.8866 - accuracy: 0.6124\n",
            "Epoch 63/200\n",
            "151/151 - 4s - loss: 1.8509 - accuracy: 0.6124\n",
            "Epoch 64/200\n",
            "151/151 - 4s - loss: 1.8158 - accuracy: 0.6221\n",
            "Epoch 65/200\n",
            "151/151 - 4s - loss: 1.7808 - accuracy: 0.6267\n",
            "Epoch 66/200\n",
            "151/151 - 4s - loss: 1.7500 - accuracy: 0.6380\n",
            "Epoch 67/200\n",
            "151/151 - 4s - loss: 1.7174 - accuracy: 0.6452\n",
            "Epoch 68/200\n",
            "151/151 - 4s - loss: 1.6840 - accuracy: 0.6481\n",
            "Epoch 69/200\n",
            "151/151 - 4s - loss: 1.6523 - accuracy: 0.6567\n",
            "Epoch 70/200\n",
            "151/151 - 4s - loss: 1.6221 - accuracy: 0.6642\n",
            "Epoch 71/200\n",
            "151/151 - 4s - loss: 1.5910 - accuracy: 0.6696\n",
            "Epoch 72/200\n",
            "151/151 - 4s - loss: 1.5612 - accuracy: 0.6808\n",
            "Epoch 73/200\n",
            "151/151 - 4s - loss: 1.5326 - accuracy: 0.6848\n",
            "Epoch 74/200\n",
            "151/151 - 4s - loss: 1.5038 - accuracy: 0.6893\n",
            "Epoch 75/200\n",
            "151/151 - 4s - loss: 1.4755 - accuracy: 0.6923\n",
            "Epoch 76/200\n",
            "151/151 - 4s - loss: 1.4488 - accuracy: 0.7031\n",
            "Epoch 77/200\n",
            "151/151 - 4s - loss: 1.4203 - accuracy: 0.7114\n",
            "Epoch 78/200\n",
            "151/151 - 4s - loss: 1.3950 - accuracy: 0.7172\n",
            "Epoch 79/200\n",
            "151/151 - 4s - loss: 1.3683 - accuracy: 0.7185\n",
            "Epoch 80/200\n",
            "151/151 - 4s - loss: 1.3397 - accuracy: 0.7274\n",
            "Epoch 81/200\n",
            "151/151 - 4s - loss: 1.3140 - accuracy: 0.7324\n",
            "Epoch 82/200\n",
            "151/151 - 4s - loss: 1.2963 - accuracy: 0.7293\n",
            "Epoch 83/200\n",
            "151/151 - 3s - loss: 1.2680 - accuracy: 0.7418\n",
            "Epoch 84/200\n",
            "151/151 - 4s - loss: 1.2458 - accuracy: 0.7459\n",
            "Epoch 85/200\n",
            "151/151 - 4s - loss: 1.2208 - accuracy: 0.7514\n",
            "Epoch 86/200\n",
            "151/151 - 3s - loss: 1.1984 - accuracy: 0.7538\n",
            "Epoch 87/200\n",
            "151/151 - 4s - loss: 1.1772 - accuracy: 0.7574\n",
            "Epoch 88/200\n",
            "151/151 - 4s - loss: 1.1532 - accuracy: 0.7649\n",
            "Epoch 89/200\n",
            "151/151 - 4s - loss: 1.1349 - accuracy: 0.7659\n",
            "Epoch 90/200\n",
            "151/151 - 4s - loss: 1.1131 - accuracy: 0.7705\n",
            "Epoch 91/200\n",
            "151/151 - 4s - loss: 1.0892 - accuracy: 0.7717\n",
            "Epoch 92/200\n",
            "151/151 - 3s - loss: 1.0702 - accuracy: 0.7786\n",
            "Epoch 93/200\n",
            "151/151 - 4s - loss: 1.0482 - accuracy: 0.7836\n",
            "Epoch 94/200\n",
            "151/151 - 3s - loss: 1.0311 - accuracy: 0.7873\n",
            "Epoch 95/200\n",
            "151/151 - 4s - loss: 1.0108 - accuracy: 0.7905\n",
            "Epoch 96/200\n",
            "151/151 - 3s - loss: 0.9956 - accuracy: 0.7957\n",
            "Epoch 97/200\n",
            "151/151 - 3s - loss: 0.9739 - accuracy: 0.7940\n",
            "Epoch 98/200\n",
            "151/151 - 4s - loss: 0.9557 - accuracy: 0.8007\n",
            "Epoch 99/200\n",
            "151/151 - 4s - loss: 0.9390 - accuracy: 0.8042\n",
            "Epoch 100/200\n",
            "151/151 - 4s - loss: 0.9213 - accuracy: 0.8077\n",
            "Epoch 101/200\n",
            "151/151 - 4s - loss: 0.9042 - accuracy: 0.8121\n",
            "Epoch 102/200\n",
            "151/151 - 4s - loss: 0.8872 - accuracy: 0.8163\n",
            "Epoch 103/200\n",
            "151/151 - 4s - loss: 0.8711 - accuracy: 0.8177\n",
            "Epoch 104/200\n",
            "151/151 - 4s - loss: 0.8532 - accuracy: 0.8221\n",
            "Epoch 105/200\n",
            "151/151 - 4s - loss: 0.8401 - accuracy: 0.8244\n",
            "Epoch 106/200\n",
            "151/151 - 4s - loss: 0.8227 - accuracy: 0.8256\n",
            "Epoch 107/200\n",
            "151/151 - 4s - loss: 0.8169 - accuracy: 0.8263\n",
            "Epoch 108/200\n",
            "151/151 - 4s - loss: 0.7987 - accuracy: 0.8331\n",
            "Epoch 109/200\n",
            "151/151 - 4s - loss: 0.7813 - accuracy: 0.8331\n",
            "Epoch 110/200\n",
            "151/151 - 4s - loss: 0.7650 - accuracy: 0.8385\n",
            "Epoch 111/200\n",
            "151/151 - 4s - loss: 0.7525 - accuracy: 0.8417\n",
            "Epoch 112/200\n",
            "151/151 - 4s - loss: 0.7389 - accuracy: 0.8433\n",
            "Epoch 113/200\n",
            "151/151 - 4s - loss: 0.7235 - accuracy: 0.8437\n",
            "Epoch 114/200\n",
            "151/151 - 4s - loss: 0.7130 - accuracy: 0.8498\n",
            "Epoch 115/200\n",
            "151/151 - 4s - loss: 0.6981 - accuracy: 0.8525\n",
            "Epoch 116/200\n",
            "151/151 - 4s - loss: 0.6876 - accuracy: 0.8543\n",
            "Epoch 117/200\n",
            "151/151 - 4s - loss: 0.6747 - accuracy: 0.8548\n",
            "Epoch 118/200\n",
            "151/151 - 4s - loss: 0.6631 - accuracy: 0.8612\n",
            "Epoch 119/200\n",
            "151/151 - 4s - loss: 0.6510 - accuracy: 0.8598\n",
            "Epoch 120/200\n",
            "151/151 - 4s - loss: 0.6402 - accuracy: 0.8618\n",
            "Epoch 121/200\n",
            "151/151 - 4s - loss: 0.6278 - accuracy: 0.8666\n",
            "Epoch 122/200\n",
            "151/151 - 4s - loss: 0.6187 - accuracy: 0.8672\n",
            "Epoch 123/200\n",
            "151/151 - 4s - loss: 0.6052 - accuracy: 0.8672\n",
            "Epoch 124/200\n",
            "151/151 - 4s - loss: 0.5967 - accuracy: 0.8720\n",
            "Epoch 125/200\n",
            "151/151 - 4s - loss: 0.5839 - accuracy: 0.8735\n",
            "Epoch 126/200\n",
            "151/151 - 4s - loss: 0.5759 - accuracy: 0.8737\n",
            "Epoch 127/200\n",
            "151/151 - 4s - loss: 0.5663 - accuracy: 0.8752\n",
            "Epoch 128/200\n",
            "151/151 - 4s - loss: 0.5558 - accuracy: 0.8774\n",
            "Epoch 129/200\n",
            "151/151 - 4s - loss: 0.5483 - accuracy: 0.8795\n",
            "Epoch 130/200\n",
            "151/151 - 4s - loss: 0.5406 - accuracy: 0.8806\n",
            "Epoch 131/200\n",
            "151/151 - 4s - loss: 0.5288 - accuracy: 0.8826\n",
            "Epoch 132/200\n",
            "151/151 - 4s - loss: 0.5197 - accuracy: 0.8841\n",
            "Epoch 133/200\n",
            "151/151 - 4s - loss: 0.5118 - accuracy: 0.8849\n",
            "Epoch 134/200\n",
            "151/151 - 4s - loss: 0.5031 - accuracy: 0.8885\n",
            "Epoch 135/200\n",
            "151/151 - 4s - loss: 0.4976 - accuracy: 0.8874\n",
            "Epoch 136/200\n",
            "151/151 - 4s - loss: 0.4875 - accuracy: 0.8887\n",
            "Epoch 137/200\n",
            "151/151 - 4s - loss: 0.4810 - accuracy: 0.8883\n",
            "Epoch 138/200\n",
            "151/151 - 4s - loss: 0.4737 - accuracy: 0.8906\n",
            "Epoch 139/200\n",
            "151/151 - 4s - loss: 0.4687 - accuracy: 0.8943\n",
            "Epoch 140/200\n",
            "151/151 - 4s - loss: 0.4656 - accuracy: 0.8937\n",
            "Epoch 141/200\n",
            "151/151 - 4s - loss: 0.4594 - accuracy: 0.8928\n",
            "Epoch 142/200\n",
            "151/151 - 4s - loss: 0.4515 - accuracy: 0.8943\n",
            "Epoch 143/200\n",
            "151/151 - 4s - loss: 0.4395 - accuracy: 0.8980\n",
            "Epoch 144/200\n",
            "151/151 - 4s - loss: 0.4333 - accuracy: 0.8980\n",
            "Epoch 145/200\n",
            "151/151 - 4s - loss: 0.4275 - accuracy: 0.9001\n",
            "Epoch 146/200\n",
            "151/151 - 4s - loss: 0.4240 - accuracy: 0.8991\n",
            "Epoch 147/200\n",
            "151/151 - 3s - loss: 0.4171 - accuracy: 0.8991\n",
            "Epoch 148/200\n",
            "151/151 - 4s - loss: 0.4110 - accuracy: 0.9007\n",
            "Epoch 149/200\n",
            "151/151 - 4s - loss: 0.4057 - accuracy: 0.9003\n",
            "Epoch 150/200\n",
            "151/151 - 4s - loss: 0.4009 - accuracy: 0.9010\n",
            "Epoch 151/200\n",
            "151/151 - 4s - loss: 0.3968 - accuracy: 0.9016\n",
            "Epoch 152/200\n",
            "151/151 - 4s - loss: 0.3914 - accuracy: 0.9026\n",
            "Epoch 153/200\n",
            "151/151 - 4s - loss: 0.3868 - accuracy: 0.9051\n",
            "Epoch 154/200\n",
            "151/151 - 4s - loss: 0.3828 - accuracy: 0.9037\n",
            "Epoch 155/200\n",
            "151/151 - 4s - loss: 0.3774 - accuracy: 0.9051\n",
            "Epoch 156/200\n",
            "151/151 - 4s - loss: 0.3746 - accuracy: 0.9028\n",
            "Epoch 157/200\n",
            "151/151 - 4s - loss: 0.3681 - accuracy: 0.9043\n",
            "Epoch 158/200\n",
            "151/151 - 4s - loss: 0.3673 - accuracy: 0.9012\n",
            "Epoch 159/200\n",
            "151/151 - 4s - loss: 0.3608 - accuracy: 0.9064\n",
            "Epoch 160/200\n",
            "151/151 - 4s - loss: 0.3576 - accuracy: 0.9053\n",
            "Epoch 161/200\n",
            "151/151 - 4s - loss: 0.3541 - accuracy: 0.9070\n",
            "Epoch 162/200\n",
            "151/151 - 3s - loss: 0.3515 - accuracy: 0.9039\n",
            "Epoch 163/200\n",
            "151/151 - 4s - loss: 0.3476 - accuracy: 0.9057\n",
            "Epoch 164/200\n",
            "151/151 - 4s - loss: 0.3437 - accuracy: 0.9053\n",
            "Epoch 165/200\n",
            "151/151 - 3s - loss: 0.3454 - accuracy: 0.9039\n",
            "Epoch 166/200\n",
            "151/151 - 4s - loss: 0.3442 - accuracy: 0.9060\n",
            "Epoch 167/200\n",
            "151/151 - 3s - loss: 0.3474 - accuracy: 0.9047\n",
            "Epoch 168/200\n",
            "151/151 - 3s - loss: 0.3373 - accuracy: 0.9078\n",
            "Epoch 169/200\n",
            "151/151 - 4s - loss: 0.3316 - accuracy: 0.9080\n",
            "Epoch 170/200\n",
            "151/151 - 4s - loss: 0.3276 - accuracy: 0.9070\n",
            "Epoch 171/200\n",
            "151/151 - 4s - loss: 0.3255 - accuracy: 0.9068\n",
            "Epoch 172/200\n",
            "151/151 - 3s - loss: 0.3237 - accuracy: 0.9072\n",
            "Epoch 173/200\n",
            "151/151 - 4s - loss: 0.3218 - accuracy: 0.9080\n",
            "Epoch 174/200\n",
            "151/151 - 3s - loss: 0.3190 - accuracy: 0.9082\n",
            "Epoch 175/200\n",
            "151/151 - 3s - loss: 0.3173 - accuracy: 0.9070\n",
            "Epoch 176/200\n",
            "151/151 - 4s - loss: 0.3152 - accuracy: 0.9062\n",
            "Epoch 177/200\n",
            "151/151 - 3s - loss: 0.3134 - accuracy: 0.9068\n",
            "Epoch 178/200\n",
            "151/151 - 4s - loss: 0.3106 - accuracy: 0.9093\n",
            "Epoch 179/200\n",
            "151/151 - 4s - loss: 0.3120 - accuracy: 0.9068\n",
            "Epoch 180/200\n",
            "151/151 - 4s - loss: 0.3098 - accuracy: 0.9082\n",
            "Epoch 181/200\n",
            "151/151 - 4s - loss: 0.3069 - accuracy: 0.9072\n",
            "Epoch 182/200\n",
            "151/151 - 4s - loss: 0.3041 - accuracy: 0.9080\n",
            "Epoch 183/200\n",
            "151/151 - 4s - loss: 0.3030 - accuracy: 0.9097\n",
            "Epoch 184/200\n",
            "151/151 - 3s - loss: 0.3030 - accuracy: 0.9072\n",
            "Epoch 185/200\n",
            "151/151 - 4s - loss: 0.3012 - accuracy: 0.9089\n",
            "Epoch 186/200\n",
            "151/151 - 4s - loss: 0.2997 - accuracy: 0.9080\n",
            "Epoch 187/200\n",
            "151/151 - 4s - loss: 0.3019 - accuracy: 0.9055\n",
            "Epoch 188/200\n",
            "151/151 - 4s - loss: 0.2977 - accuracy: 0.9068\n",
            "Epoch 189/200\n",
            "151/151 - 4s - loss: 0.2954 - accuracy: 0.9087\n",
            "Epoch 190/200\n",
            "151/151 - 4s - loss: 0.2937 - accuracy: 0.9060\n",
            "Epoch 191/200\n",
            "151/151 - 4s - loss: 0.2930 - accuracy: 0.9076\n",
            "Epoch 192/200\n",
            "151/151 - 4s - loss: 0.2917 - accuracy: 0.9091\n",
            "Epoch 193/200\n",
            "151/151 - 4s - loss: 0.2916 - accuracy: 0.9087\n",
            "Epoch 194/200\n",
            "151/151 - 3s - loss: 0.2959 - accuracy: 0.9097\n",
            "Epoch 195/200\n",
            "151/151 - 4s - loss: 0.3010 - accuracy: 0.9049\n",
            "Epoch 196/200\n",
            "151/151 - 3s - loss: 0.2991 - accuracy: 0.9084\n",
            "Epoch 197/200\n",
            "151/151 - 4s - loss: 0.2884 - accuracy: 0.9091\n",
            "Epoch 198/200\n",
            "151/151 - 3s - loss: 0.2853 - accuracy: 0.9084\n",
            "Epoch 199/200\n",
            "151/151 - 4s - loss: 0.2853 - accuracy: 0.9101\n",
            "Epoch 200/200\n",
            "151/151 - 4s - loss: 0.2842 - accuracy: 0.9095\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f4373556890>"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DlJEXXshYYFj"
      },
      "source": [
        "# 문장 생성\n",
        "def sentence_generation(model, tokenizer, current_word, n): # 모델, 토크나이저, 현재 단어, 반복할 횟수\n",
        "    init_word = current_word\n",
        "    sentence = ''\n",
        "\n",
        "    # n번 반복\n",
        "    for _ in range(n):\n",
        "        encoded = tokenizer.texts_to_sequences([current_word])[0]\n",
        "        encoded = pad_sequences([encoded], maxlen=max_len-1, padding='pre')\n",
        "\n",
        "        # 입력한 X(현재 단어)에 대해서 y를 예측하고 y(예측한 단어)를 result에 저장.\n",
        "        result = model.predict(encoded, verbose=0)\n",
        "        result = np.argmax(result, axis=1)\n",
        "\n",
        "        for word, index in tokenizer.word_index.items(): \n",
        "            # 만약 예측한 단어와 인덱스와 동일한 단어가 있다면\n",
        "            if index == result:\n",
        "                break\n",
        "\n",
        "        # 현재 단어 + ' ' + 예측 단어를 현재 단어로 변경\n",
        "        current_word = current_word + ' '  + word\n",
        "\n",
        "        # 예측 단어를 문장에 저장\n",
        "        sentence = sentence + ' ' + word\n",
        "\n",
        "    sentence = init_word + sentence\n",
        "    return sentence"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w9zcH3kLYc6X",
        "outputId": "3e8074f6-612b-4100-f5a8-95b53dece854"
      },
      "source": [
        "print(sentence_generation(model, tokenizer, 'i', 10))"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i love my sublet and i want to buy it how\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bfiVELhmYggr",
        "outputId": "9c943d31-b97a-4d46-8194-f7fb34fd0d07"
      },
      "source": [
        "print(sentence_generation(model, tokenizer, 'how', 10))"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "how to be mindful while gardening and a next thinks bill\n"
          ]
        }
      ]
    }
  ]
}