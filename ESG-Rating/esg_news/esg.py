# -*- coding: utf-8 -*-
"""1669063_esg_2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1B-v0FfcP5dVT0hIKAkoGJEkgD9eE1dZh

## 네이버에서 ESG 데이터를 수집한 후 worldcloud 그리기
### 1. ESG 뉴스 스크래핑하기
### 2. csv 파일 열어서 데이터 분석하고 wordcloud 그리기
### (1) ESG 제목 데이터로 wordcloud 그리기
### (2) ESG 기사 내용 데이터로 wordcloud 그리기
"""

!pip install konlpy

!sudo apt-get install -y fonts-nanum
!sudo fc-cache -fv
!rm ~/.cache/matplotlib -rf

# 패키지 불러오기
import numpy as np                    
import pandas as pd                   
import matplotlib.pyplot as plt       

## Text 데이터 처리
from konlpy.tag import Kkma

from collections import Counter
from wordcloud import WordCloud

from bs4 import BeautifulSoup
import requests

import warnings
warnings.filterwarnings('ignore')

"""## 1. ESG 뉴스 스크래핑하기"""

# 해드라인 검색 및 추출 함수를 정의
t_list = []

def web_scraping(keyword, end, start = 1): 
    # 네이버 뉴스 url의 특성을 고려하여 추가된 수식
    end = (end-1)*10 + 1
        
    while 1:
        
        if start > end :    # 스타트 페이지가 마지막 페이지보다 크면 while 문을 빠져 나감 
            break        
        
        url ='https://search.naver.com/search.naver?where=news&sm=tab_pge&query={0}&start={1}'.format(keyword,start)

        req = requests.get(url)    # 해당 페이지를 가져옴

        page = BeautifulSoup(req.text, 'html.parser')
        
        news_titles = page.find_all(attrs='news_tit')
        medias = page.find_all(attrs='info press') 
        texts = page.find_all('a', attrs='api_txt_lines dsc_txt_wrap')

        # 검색 결과 중에서 큰 제목 & 해당 사이트 주소 추출 
        ablist=[] 
        for one in news_titles :
            a = one.attrs['title']
            b = one.attrs['href']
            ablist.append([a, b])
        
        # 언론사 추출
        i = 0
        for one in medias :
            c = one.text
            ablist[i].insert(0, c)
            i += 1

        # 기사 본문 내용 일부 추출
        j = 0
        for one in texts :
            d = one.text
            ablist[j].insert(2, d)
            j += 1
        
        t_list.append(ablist)
        
        start += 10

# 검색어를 ESG로 설정 

#keyword = input('검색어를 입력하세요 : ')
keyword = 'ESG'
print()

# 마지막 페이지를 1000으로 설정
#end_page = int(input('마지막 페이지 번호를 입력하세요 : '))
end_page = 1000
web_scraping(keyword, end_page)       # end_page 페이지 까지 스크래핑

# 총 1000 페이지 스크래핑함
len(t_list)

# 한 페이지 당 10개의 기사가 있음. 이를 t_list 안에 리스트로 넣어놓음
t_list[0]

# 리스트 안 리스트를 꺼내서 각 페이지별로 리스트에 있는 것이 아니라 
#하나의 리스트 안에 모든 페이지 정보가 들어가도록 구성함 
t_list_final=[]

for k in t_list:
    for m in k:
        t_list_final.append(m)
print(len(t_list_final))

# csv 파일로 만들었음 ['media', 'title', 'text', 'url']

df = pd.DataFrame(t_list_final)
df.columns = ['media', 'title', 'text', 'url']
df.head()

# 데이터프레임을 csv로 변환하여 저장함
df.to_csv('C:\\Users\\user\\Documents\\web_text_mining_ewha\\esg_news.csv')

"""## 2. csv 파일 열어서 데이터 분석하고 wordcloud 그리기
### (1) ESG 제목 데이터로 wordcloud 그리기
"""

# esg_news 데이터 업로드하기
from google.colab import files
file_uploaded = files.upload()

# csv 파일 dataframe으로 열기
import io
import pandas as pd

df2 = pd.read_csv(io.BytesIO(file_uploaded['esg_news.csv']))

df2.head()

# 데이터 분석을 위하여 데이터프레임에서 제목만 추출함
title_li = df2['title'].to_list()
title_li

# 형태소 분석

kkma = Kkma()

sentences_tag = []

for sentence in title_li:
    word_tag = kkma.pos(sentence)
    sentences_tag.append(word_tag)

print(sentences_tag)

# 형태소 분석 후 명사만 추출

noun_list = []

for sentence in sentences_tag:
    for word, tag in sentence: #word와 tag 중에서 word 단어만
        if tag in ['NNG']:               
            noun_list.append(word)
            
print(noun_list)

# 명사 중에서 두음절 단어만 추출 

print('전체 명사의 수: ', len(noun_list))
print() 

noun_list = [word for word in noun_list if len(word) > 1]    # 명사중에서 두음절 이상의 단어  추출

print('두음절 이상의 명사의 수: ', len(noun_list))
print() 

print(noun_list[:100])   # 처음부터 나오는 순서대로 100개 단어 출력

# 단어의 출현 횟수 카운트

counts = Counter(noun_list)
words = counts.most_common(50)     # 가장 많이 출현한 횟수 순으로 50개 단어만 추출
print(words)

# 실루엣을 이용한 워드 클라우드를 위해 이미지 업로드
from google.colab import files
file_uploaded = files.upload()

# 워드 클라우드

import numpy as np                    
import matplotlib as mpl
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
from wordcloud import WordCloud
from wordcloud  import ImageColorGenerator

# 그래프에서 마이너스 폰트 깨지는 문제에 대한 대처
mpl.rcParams['axes.unicode_minus'] = False

# 실루엣 이미지 업로드
cloud_img = plt.imread('leaf.jpg')

# 워드 클라우드 세팅
wordcloud = WordCloud(font_path='NanumBarunGothic',
                      background_color='white',
                      random_state = 1,
                      color_func = ImageColorGenerator(cloud_img),   # img의 color 를 이용
                      mask = cloud_img)      # 그림 이미지에 맞게 그림 

print(dict(words))

# 워드클라우드 이미지 print
cloud = wordcloud.generate_from_frequencies(dict(words))  

fig = plt.figure(figsize=(12, 12))
plt.axis('off')
plt.imshow(cloud)
plt.show()

"""### (2) ESG 기사 내용 데이터로 wordcloud 그리기"""

# 데이터 분석을 위하여 데이터프레임에서 '기사 내용 일부'만 추출함
title_li2 = df2['text'].to_list()
title_li2

# Twitter에서 형태소 분석을 하기 위해 리스트 변형
'''
'제목' 분석 시에는 형태소 분석 전에 별다른 변형/조작이 필요 없었는데, 
각 요소가 2개 이상의 문장으로 이루어진 경우, 형태소 분석기에서 인식을 하지 못하고 TypeError 오류가 남.
그래서 '기사 내용' 분석을 위해서는 문장 별로 잘라준 뒤에 형태소 분석을 해야됨.
'''

real_li = []
for j in range(10000):
  a=str(title_li2[j]).split('.')
  #print(a)
  sample_list = list(filter(None, a))
  real_li.append(sample_list)

whole_li = []
for i in real_li:
  for j in i:
    whole_li.append(j)

print(whole_li[:5])

# Kkma가 시간이 오래걸려서 '제목'보다 데이터 크기가 더 큰 '기사 내용 일부'를 분석할 때는 Twitter를 사용해봄 
# 형태소 분석

from konlpy.tag import Twitter
 
twitter = Twitter()
sentences_tag2 = []

for sentence2 in whole_li:
    word_tag2 = twitter.pos(sentence2)
    sentences_tag2.append(word_tag2)

sentences_tag2

# 형태소 분석 후 명사만 추출

noun_list2 = []

for sentence2 in sentences_tag2:
    for word2, tag2 in sentence2: #word와 tag 중에서 word만 추출
        if tag2 in ['Noun']:               
            noun_list2.append(word2)
            
print(noun_list2)

# 명사 중에서 두음절 단어만 추출 

print('전체 명사의 수: ', len(noun_list2))
print() 

noun_list2 = [word for word in noun_list2 if len(word) > 1]    # 명사중에서 두음절 이상의 단어  추출

print('두음절 이상의 명사의 수: ', len(noun_list2))
print() 

print(noun_list2[:100])   # 처음부터 나오는 순서대로 100개 단어 출력

# 단어의 출현 횟수 카운트

counts2 = Counter(noun_list2)
words2 = counts2.most_common(50)     # 가장 많이 출현한 횟수 순으로 50개 단어만 추출
print(words2)

# 워드 클라우드 (colormap style을 BrBG로 변경)

from wordcloud import WordCloud

wordcloud = WordCloud(font_path='NanumBarunGothic',
                      colormap = 'BrBG',
                      width=600,
                      height=600)

print(dict(words))

# 워드 클라우드 print
# '제목' 분석 시 실루엣 방법을 사용하여 '기사 내용' 분석 시에는 일반적인 워드 클라우드를 사용하고, colormap 색을 ESG를 표현할 수 있는 색으로 선택함

cloud = wordcloud.generate_from_frequencies(dict(words))

plt.figure(figsize=(10, 10))
plt.axis('off')   
plt.imshow(cloud)
plt.show()

